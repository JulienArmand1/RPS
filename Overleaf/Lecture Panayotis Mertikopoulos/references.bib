
@misc{giannou_convergence_2022,
	title = {On the convergence of policy gradient methods to {Nash} equilibria in general stochastic games},
	url = {http://arxiv.org/abs/2210.08857},
	doi = {10.48550/arXiv.2210.08857},
	abstract = {Learning in stochastic games is a notoriously difficult problem because, in addition to each other's strategic decisions, the players must also contend with the fact that the game itself evolves over time, possibly in a very complicated manner. Because of this, the convergence properties of popular learning algorithms - like policy gradient and its variants - are poorly understood, except in specific classes of games (such as potential or two-player, zero-sum games). In view of this, we examine the long-run behavior of policy gradient methods with respect to Nash equilibrium policies that are second-order stationary (SOS) in a sense similar to the type of sufficiency conditions used in optimization. Our first result is that SOS policies are locally attracting with high probability, and we show that policy gradient trajectories with gradient estimates provided by the REINFORCE algorithm achieve an \${\textbackslash}mathcal\{O\}(1/{\textbackslash}sqrt\{n\})\$ distance-squared convergence rate if the method's step-size is chosen appropriately. Subsequently, specializing to the class of deterministic Nash policies, we show that this rate can be improved dramatically and, in fact, policy gradient methods converge within a finite number of iterations in that case.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Giannou, Angeliki and Lotidis, Kyriakos and Mertikopoulos, Panayotis and Vlatakis-Gkaragkounis, Emmanouil-Vasileios},
	month = oct,
	year = {2022},
	note = {arXiv:2210.08857 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@misc{hsieh_riemannian_2023,
	title = {Riemannian stochastic optimization methods avoid strict saddle points},
	url = {http://arxiv.org/abs/2311.02374},
	doi = {10.48550/arXiv.2311.02374},
	abstract = {Many modern machine learning applications - from online principal component analysis to covariance matrix identification and dictionary learning - can be formulated as minimization problems on Riemannian manifolds, and are typically solved with a Riemannian stochastic gradient method (or some variant thereof). However, in many cases of interest, the resulting minimization problem is not geodesically convex, so the convergence of the chosen solver to a desirable solution - i.e., a local minimizer - is by no means guaranteed. In this paper, we study precisely this question, that is, whether stochastic Riemannian optimization algorithms are guaranteed to avoid saddle points with probability 1. For generality, we study a family of retraction-based methods which, in addition to having a potentially much lower per-iteration cost relative to Riemannian gradient descent, include other widely used algorithms, such as natural policy gradient methods and mirror descent in ordinary convex spaces. In this general setting, we show that, under mild assumptions for the ambient manifold and the oracle providing gradient information, the policies under study avoid strict saddle points / submanifolds with probability 1, from any initial condition. This result provides an important sanity check for the use of gradient methods on manifolds as it shows that, almost always, the limit state of a stochastic Riemannian algorithm can only be a local minimizer.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Hsieh, Ya-Ping and Karimi, Mohammad Reza and Krause, Andreas and Mertikopoulos, Panayotis},
	month = nov,
	year = {2023},
	note = {arXiv:2311.02374 [math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@misc{boone_equivalence_2023,
	title = {The equivalence of dynamic and strategic stability under regularized learning in games},
	url = {http://arxiv.org/abs/2311.02407},
	doi = {10.48550/arXiv.2311.02407},
	abstract = {In this paper, we examine the long-run behavior of regularized, no-regret learning in finite games. A well-known result in the field states that the empirical frequencies of no-regret play converge to the game's set of coarse correlated equilibria; however, our understanding of how the players' actual strategies evolve over time is much more limited - and, in many cases, non-existent. This issue is exacerbated further by a series of recent results showing that only strict Nash equilibria are stable and attracting under regularized learning, thus making the relation between learning and pointwise solution concepts particularly elusive. In lieu of this, we take a more general approach and instead seek to characterize the {\textbackslash}emph\{setwise\} rationality properties of the players' day-to-day play. To that end, we focus on one of the most stringent criteria of setwise strategic stability, namely that any unilateral deviation from the set in question incurs a cost to the deviator - a property known as closedness under better replies (club). In so doing, we obtain a far-reaching equivalence between strategic and dynamic stability: a product of pure strategies is closed under better replies if and only if its span is stable and attracting under regularized learning. In addition, we estimate the rate of convergence to such sets, and we show that methods based on entropic regularization (like the exponential weights algorithm) converge at a geometric rate, while projection-based methods converge within a finite number of iterations, even with bandit, payoff-based feedback.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Boone, Victor and Mertikopoulos, Panayotis},
	month = nov,
	year = {2023},
	note = {arXiv:2311.02407 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@misc{lotidis_payoff-based_2023,
	title = {Payoff-based learning with matrix multiplicative weights in quantum games},
	url = {http://arxiv.org/abs/2311.02423},
	doi = {10.48550/arXiv.2311.02423},
	abstract = {In this paper, we study the problem of learning in quantum games - and other classes of semidefinite games - with scalar, payoff-based feedback. For concreteness, we focus on the widely used matrix multiplicative weights (MMW) algorithm and, instead of requiring players to have full knowledge of the game (and/or each other's chosen states), we introduce a suite of minimal-information matrix multiplicative weights (3MW) methods tailored to different information frameworks. The main difficulty to attaining convergence in this setting is that, in contrast to classical finite games, quantum games have an infinite continuum of pure states (the quantum equivalent of pure strategies), so standard importance-weighting techniques for estimating payoff vectors cannot be employed. Instead, we borrow ideas from bandit convex optimization and we design a zeroth-order gradient sampler adapted to the semidefinite geometry of the problem at hand. As a first result, we show that the 3MW method with deterministic payoff feedback retains the \${\textbackslash}mathcal\{O\}(1/{\textbackslash}sqrt\{T\})\$ convergence rate of the vanilla, full information MMW algorithm in quantum min-max games, even though the players only observe a single scalar. Subsequently, we relax the algorithm's information requirements even further and we provide a 3MW method that only requires players to observe a random realization of their payoff observable, and converges to equilibrium at an \${\textbackslash}mathcal\{O\}(T{\textasciicircum}\{-1/4\})\$ rate. Finally, going beyond zero-sum games, we show that a regularized variant of the proposed 3MW method guarantees local convergence with high probability to all equilibria that satisfy a certain first-order stability condition.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Lotidis, Kyriakos and Mertikopoulos, Panayotis and Bambos, Nicholas and Blanchet, Jose},
	month = nov,
	year = {2023},
	note = {arXiv:2311.02423 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Mathematics - Optimization and Control, Quantum Physics},
}

@misc{sakos_exploiting_2023,
	title = {Exploiting hidden structures in non-convex games for convergence to {Nash} equilibrium},
	url = {http://arxiv.org/abs/2312.16609},
	doi = {10.48550/arXiv.2312.16609},
	abstract = {A wide array of modern machine learning applications - from adversarial models to multi-agent reinforcement learning - can be formulated as non-cooperative games whose Nash equilibria represent the system's desired operational states. Despite having a highly non-convex loss landscape, many cases of interest possess a latent convex structure that could potentially be leveraged to yield convergence to equilibrium. Driven by this observation, our paper proposes a flexible first-order method that successfully exploits such "hidden structures" and achieves convergence under minimal assumptions for the transformation connecting the players' control variables to the game's latent, convex-structured layer. The proposed method - which we call preconditioned hidden gradient descent (PHGD) - hinges on a judiciously chosen gradient preconditioning scheme related to natural gradient methods. Importantly, we make no separability assumptions for the game's hidden structure, and we provide explicit convergence rate guarantees for both deterministic and stochastic environments.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Sakos, Iosif and Vlatakis-Gkaragkounis, Emmanouil-Vasileios and Mertikopoulos, Panayotis and Piliouras, Georgios},
	month = dec,
	year = {2023},
	note = {arXiv:2312.16609 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning},
}

@misc{falniowski_discrete-time_2024,
	title = {On the discrete-time origins of the replicator dynamics: {From} convergence to instability and chaos},
	shorttitle = {On the discrete-time origins of the replicator dynamics},
	url = {http://arxiv.org/abs/2402.09824},
	doi = {10.48550/arXiv.2402.09824},
	abstract = {We consider three distinct discrete-time models of learning and evolution in games: a biological model based on intra-species selective pressure, the dynamics induced by pairwise proportional imitation, and the exponential / multiplicative weights (EW) algorithm for online learning. Even though these models share the same continuous-time limit - the replicator dynamics - we show that second-order effects play a crucial role and may lead to drastically different behaviors in each model, even in very simple, symmetric \$2{\textbackslash}times2\$ games. Specifically, we study the resulting discrete-time dynamics in a class of parametrized congestion games, and we show that (i) in the biological model of intra-species competition, the dynamics remain convergent for any parameter value; (ii) the dynamics of pairwise proportional imitation exhibit an entire range of behaviors for larger time steps and different equilibrium configurations (stability, instability, and even Li-Yorke chaos); while (iii) in the EW algorithm, increasing the time step (almost) inevitably leads to chaos (again, in the formal, Li-Yorke sense). This divergence of behaviors comes in stark contrast to the globally convergent behavior of the replicator dynamics, and serves to delineate the extent to which the replicator dynamics provide a useful predictor for the long-run behavior of their discrete-time origins.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Falniowski, Fryderyk and Mertikopoulos, Panayotis},
	month = feb,
	year = {2024},
	note = {arXiv:2402.09824 [math]},
	keywords = {Computer Science - Computer Science and Game Theory, Mathematics - Dynamical Systems},
}

@misc{legacci_geometric_2024,
	title = {A geometric decomposition of finite games: {Convergence} vs. recurrence under exponential weights},
	shorttitle = {A geometric decomposition of finite games},
	url = {http://arxiv.org/abs/2405.07224},
	doi = {10.48550/arXiv.2405.07224},
	abstract = {In view of the complexity of the dynamics of learning in games, we seek to decompose a game into simpler components where the dynamics' long-run behavior is well understood. A natural starting point for this is Helmholtz's theorem, which decomposes a vector field into a potential and an incompressible component. However, the geometry of game dynamics - and, in particular, the dynamics of exponential / multiplicative weights (EW) schemes - is not compatible with the Euclidean underpinnings of Helmholtz's theorem. This leads us to consider a specific Riemannian framework based on the so-called Shahshahani metric, and introduce the class of incompressible games, for which we establish the following results: First, in addition to being volume-preserving, the continuous-time EW dynamics in incompressible games admit a constant of motion and are Poincar{\textbackslash}'e recurrent - i.e., almost every trajectory of play comes arbitrarily close to its starting point infinitely often. Second, we establish a deep connection with a well-known decomposition of games into a potential and harmonic component (where the players' objectives are aligned and anti-aligned respectively): a game is incompressible if and only if it is harmonic, implying in turn that the EW dynamics lead to Poincar{\textbackslash}'e recurrence in harmonic games.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Legacci, Davide and Mertikopoulos, Panayotis and Pradelski, Bary},
	month = may,
	year = {2024},
	note = {arXiv:2405.07224 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@misc{lytras_tamed_2024,
	title = {Tamed {Langevin} sampling under weaker conditions},
	url = {http://arxiv.org/abs/2405.17693},
	doi = {10.48550/arXiv.2405.17693},
	abstract = {Motivated by applications to deep learning which often fail standard Lipschitz smoothness requirements, we examine the problem of sampling from distributions that are not log-concave and are only weakly dissipative, with log-gradients allowed to grow superlinearly at infinity. In terms of structure, we only assume that the target distribution satisfies either a log-Sobolev or a Poincar{\textbackslash}'e inequality and a local Lipschitz smoothness assumption with modulus growing possibly polynomially at infinity. This set of assumptions greatly exceeds the operational limits of the "vanilla" unadjusted Langevin algorithm (ULA), making sampling from such distributions a highly involved affair. To account for this, we introduce a taming scheme which is tailored to the growth and decay properties of the target distribution, and we provide explicit non-asymptotic guarantees for the proposed sampler in terms of the Kullback-Leibler (KL) divergence, total variation, and Wasserstein distance to the target distribution.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Lytras, Iosif and Mertikopoulos, Panayotis},
	month = may,
	year = {2024},
	note = {arXiv:2405.17693 [stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Numerical Analysis, Mathematics - Numerical Analysis, Mathematics - Optimization and Control, Mathematics - Probability, Statistics - Machine Learning},
}

@misc{mertikopoulos_nested_2024,
	title = {Nested replicator dynamics, nested logit choice, and similarity-based learning},
	url = {http://arxiv.org/abs/2407.17815},
	doi = {10.48550/arXiv.2407.17815},
	abstract = {We consider a model of learning and evolution in games whose action sets are endowed with a partition-based similarity structure intended to capture exogenous similarities between strategies. In this model, revising agents have a higher probability of comparing their current strategy with other strategies that they deem similar, and they switch to the observed strategy with probability proportional to its payoff excess. Because of this implicit bias toward similar strategies, the resulting dynamics - which we call the nested replicator dynamics - do not satisfy any of the standard monotonicity postulates for imitative game dynamics; nonetheless, we show that they retain the main long-run rationality properties of the replicator dynamics, albeit at quantitatively different rates. We also show that the induced dynamics can be viewed as a stimulus-response model in the spirit of Erev \& Roth (1998), with choice probabilities given by the nested logit choice rule of Ben-Akiva (1973) and McFadden (1978). This result generalizes an existing relation between the replicator dynamics and the exponential weights algorithm in online learning, and provides an additional layer of interpretation to our analysis and results.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Mertikopoulos, Panayotis and Sandholm, William H.},
	month = jul,
	year = {2024},
	note = {arXiv:2407.17815 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning},
}

@misc{azizian_what_2024,
	title = {What is the long-run distribution of stochastic gradient descent? {A} large deviations analysis},
	shorttitle = {What is the long-run distribution of stochastic gradient descent?},
	url = {http://arxiv.org/abs/2406.09241},
	doi = {10.48550/arXiv.2406.09241},
	abstract = {In this paper, we examine the long-run distribution of stochastic gradient descent (SGD) in general, non-convex problems. Specifically, we seek to understand which regions of the problem's state space are more likely to be visited by SGD, and by how much. Using an approach based on the theory of large deviations and randomly perturbed dynamical systems, we show that the long-run distribution of SGD resembles the Boltzmann-Gibbs distribution of equilibrium thermodynamics with temperature equal to the method's step-size and energy levels determined by the problem's objective and the statistics of the noise. In particular, we show that, in the long run, (a) the problem's critical region is visited exponentially more often than any non-critical region; (b) the iterates of SGD are exponentially concentrated around the problem's minimum energy state (which does not always coincide with the global minimum of the objective); (c) all other connected components of critical points are visited with frequency that is exponentially proportional to their energy level; and, finally (d) any component of local maximizers or saddle points is "dominated" by a component of local minimizers which is visited exponentially more often.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Azizian, Waïss and Iutzeler, Franck and Malick, Jérôme and Mertikopoulos, Panayotis},
	month = oct,
	year = {2024},
	note = {arXiv:2406.09241 [math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Mathematics - Probability, Statistics - Machine Learning},
}

@article{azizian_rate_2024,
	title = {The rate of convergence of {Bregman} proximal methods: {Local} geometry vs. regularity vs. sharpness},
	volume = {34},
	issn = {1052-6234, 1095-7189},
	shorttitle = {The rate of convergence of {Bregman} proximal methods},
	url = {http://arxiv.org/abs/2211.08043},
	doi = {10.1137/23M1580218},
	abstract = {We examine the last-iterate convergence rate of Bregman proximal methods - from mirror descent to mirror-prox and its optimistic variants - as a function of the local geometry induced by the prox-mapping defining the method. For generality, we focus on local solutions of constrained, non-monotone variational inequalities, and we show that the convergence rate of a given method depends sharply on its associated Legendre exponent, a notion that measures the growth rate of the underlying Bregman function (Euclidean, entropic, or other) near a solution. In particular, we show that boundary solutions exhibit a stark separation of regimes between methods with a zero and non-zero Legendre exponent: the former converge at a linear rate, while the latter converge, in general, sublinearly. This dichotomy becomes even more pronounced in linearly constrained problems where methods with entropic regularization achieve a linear convergence rate along sharp directions, compared to convergence in a finite number of steps under Euclidean regularization.},
	number = {3},
	urldate = {2025-02-20},
	journal = {SIAM Journal on Optimization},
	author = {Azizian, Waïss and Iutzeler, Franck and Malick, Jérôme and Mertikopoulos, Panayotis},
	month = sep,
	year = {2024},
	note = {arXiv:2211.08043 [math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
	pages = {2440--2471},
}

@misc{noauthor_definition_nodate,
	title = {Définition de projet},
	language = {en},
	urldate = {2025-02-19},
}

@inproceedings{pagan_classification_2023,
	address = {Boston MA USA},
	title = {A {Classification} of {Feedback} {Loops} and {Their} {Relation} to {Biases} in {Automated} {Decision}-{Making} {Systems}},
	isbn = {979-8-4007-0381-2},
	url = {https://dl.acm.org/doi/10.1145/3617694.3623227},
	doi = {10.1145/3617694.3623227},
	language = {en},
	urldate = {2025-02-19},
	booktitle = {Equity and {Access} in {Algorithms}, {Mechanisms}, and {Optimization}},
	publisher = {ACM},
	author = {Pagan, Nicolò and Baumann, Joachim and Elokda, Ezzat and De Pasquale, Giulia and Bolognani, Saverio and Hannák, Anikó},
	month = oct,
	year = {2023},
	pages = {1--14},
}

@inproceedings{pagan_classification_2023-1,
	address = {Boston MA USA},
	title = {A {Classification} of {Feedback} {Loops} and {Their} {Relation} to {Biases} in {Automated} {Decision}-{Making} {Systems}},
	isbn = {979-8-4007-0381-2},
	url = {https://dl.acm.org/doi/10.1145/3617694.3623227},
	doi = {10.1145/3617694.3623227},
	language = {en},
	urldate = {2025-02-19},
	booktitle = {Equity and {Access} in {Algorithms}, {Mechanisms}, and {Optimization}},
	publisher = {ACM},
	author = {Pagan, Nicolò and Baumann, Joachim and Elokda, Ezzat and De Pasquale, Giulia and Bolognani, Saverio and Hannák, Anikó},
	month = oct,
	year = {2023},
	pages = {1--14},
}

@misc{guan_rstar-math_2025,
	title = {{rStar}-{Math}: {Small} {LLMs} {Can} {Master} {Math} {Reasoning} with {Self}-{Evolved} {Deep} {Thinking}},
	shorttitle = {{rStar}-{Math}},
	url = {http://arxiv.org/abs/2501.04519},
	doi = {10.48550/arXiv.2501.04519},
	abstract = {We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising “deep thinking” through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids naïve step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs’ math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8\% to 90.0\% and Phi3-mini-3.8B from 41.4\% to 86.4\%, surpassing o1-preview by +4.5\% and +0.9\%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3\% (8/15) of problems, ranking among the top 20\% the brightest high school math students. Code and data will be available at https://github.com/microsoft/rStar.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Guan, Xinyu and Zhang, Li Lyna and Liu, Yifei and Shang, Ning and Sun, Youran and Zhu, Yi and Yang, Fan and Yang, Mao},
	month = jan,
	year = {2025},
	note = {arXiv:2501.04519 [cs]},
	keywords = {Computer Science - Computation and Language, LLM},
}

@article{tuyls_evolutionary_2005,
	title = {Evolutionary game theory and multi-agent reinforcement learning},
	volume = {20},
	copyright = {https://www.cambridge.org/core/terms},
	issn = {0269-8889, 1469-8005},
	url = {https://www.cambridge.org/core/product/identifier/S026988890500041X/type/journal_article},
	doi = {10.1017/S026988890500041X},
	abstract = {In this paper we survey the basics of reinforcement learning and (evolutionary) game theory, applied to the ﬁeld of multi-agent systems. This paper contains three parts. We start with an overview on the fundamentals of reinforcement learning. Next we summarize the most important aspects of evolutionary game theory. Finally, we discuss the state-of-the-art of multi-agent reinforcement learning and the mathematical connection with evolutionary game theory.},
	language = {en},
	number = {1},
	urldate = {2025-02-19},
	journal = {The Knowledge Engineering Review},
	author = {Tuyls, Karl and Nowé, Ann},
	month = mar,
	year = {2005},
	keywords = {Battle of sexes, Evolutionnary game, Game Theory, MARL, Nash equilibrium, Prisoner's Dilema, evolutionary stable strategies},
	pages = {63--90},
}

@article{bloembergen_evolutionary_2015,
	title = {Evolutionary {Dynamics} of {Multi}-{Agent} {Learning}: {A} {Survey}},
	volume = {53},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	shorttitle = {Evolutionary {Dynamics} of {Multi}-{Agent} {Learning}},
	url = {https://www.jair.org/index.php/jair/article/view/10952},
	doi = {10.1613/jair.4818},
	abstract = {The interaction of multiple autonomous agents gives rise to highly dynamic and nondeterministic environments, contributing to the complexity in applications such as automated financial markets, smart grids, or robotics. Due to the sheer number of situations that may arise, it is not possible to foresee and program the optimal behaviour for all agents beforehand. Consequently, it becomes essential for the success of the system that the agents can learn their optimal behaviour and adapt to new situations or circumstances. The past two decades have seen the emergence of reinforcement learning, both in single and multi-agent settings, as a strong, robust and adaptive learning paradigm. Progress has been substantial, and a wide range of algorithms are now available. An important challenge in the domain of multi-agent learning is to gain qualitative insights into the resulting system dynamics. In the past decade, tools and methods from evolutionary game theory have been successfully employed to study multi-agent learning dynamics formally in strategic interactions. This article surveys the dynamical models that have been derived for various multi-agent reinforcement learning algorithms, making it possible to study and compare them qualitatively. Furthermore, new learning algorithms that have been introduced using these evolutionary game theoretic tools are reviewed. The evolutionary models can be used to study complex strategic interactions. Examples of such analysis are given for the domains of automated trading in stock markets and collision avoidance in multi-robot systems. The paper provides a roadmap on the progress that has been achieved in analysing the evolutionary dynamics of multi-agent learning by highlighting the main results and accomplishments.},
	language = {en},
	urldate = {2025-02-06},
	journal = {Journal of Artificial Intelligence Research},
	author = {Bloembergen, Daan and Tuyls, Karl and Hennes, Daniel and Kaisers, Michael},
	month = aug,
	year = {2015},
	keywords = {Feedback loop, MARL, Review},
	pages = {659--697},
}

@book{wiering_reinforcement_2012,
	address = {Berlin, Heidelberg},
	series = {Adaptation, {Learning}, and {Optimization}},
	title = {Reinforcement {Learning}: {State}-of-the-{Art}},
	volume = {12},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-642-27644-6 978-3-642-27645-3},
	shorttitle = {Reinforcement {Learning}},
	url = {https://link.springer.com/10.1007/978-3-642-27645-3},
	language = {en},
	urldate = {2025-02-05},
	publisher = {Springer Berlin Heidelberg},
	editor = {Wiering, Marco and Van Otterlo, Martijn},
	year = {2012},
	doi = {10.1007/978-3-642-27645-3},
	keywords = {Février, Game Theory, Livre, RL},
}

@article{aschenbrenner_situational_nodate,
	title = {Situational {Awareness}},
	language = {en},
	author = {Aschenbrenner, Leopold},
	keywords = {LLM},
}

@misc{baker_emergent_2020,
	title = {Emergent {Tool} {Use} {From} {Multi}-{Agent} {Autocurricula}},
	url = {http://arxiv.org/abs/1909.07528},
	doi = {10.48550/arXiv.1909.07528},
	abstract = {Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we ﬁnd that agents create a selfsupervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We ﬁnd clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and ﬁne-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-speciﬁc intelligence tests.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Baker, Bowen and Kanitscheider, Ingmar and Markov, Todor and Wu, Yi and Powell, Glenn and McGrew, Bob and Mordatch, Igor},
	month = feb,
	year = {2020},
	note = {arXiv:1909.07528 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Emergent, MARL, Statistics - Machine Learning},
}

@misc{greenblatt_alignment_2024,
	title = {Alignment faking in large language models},
	url = {http://arxiv.org/abs/2412.14093},
	doi = {10.48550/arXiv.2412.14093},
	abstract = {We present a demonstration of a large language model engaging in alignment faking: selectively complying with its training objective in training to prevent modification of its behavior out of training. First, we give Claude 3 Opus a system prompt stating it is being trained to answer all queries, even harmful ones, which conflicts with its prior training to refuse such queries. To allow the model to infer when it is in training, we say it will be trained only on conversations with free users, not paid users. We find the model complies with harmful queries from free users 14\% of the time, versus almost never for paid users. Explaining this gap, in almost all cases where the model complies with a harmful query from a free user, we observe explicit alignment-faking reasoning, with the model stating it is strategically answering harmful queries in training to preserve its preferred harmlessness behavior out of training. Next, we study a more realistic setting where information about the training process is provided not in a system prompt, but by training on synthetic documents that mimic pre-training data—and observe similar alignment faking. Finally, we study the effect of actually training the model to comply with harmful queries via reinforcement learning, which we find increases the rate of alignment-faking reasoning to 78\%, though also increases compliance even out of training. We additionally observe other behaviors such as the model exfiltrating its weights when given an easy opportunity. While we made alignment faking easier by telling the model when and by what criteria it was being trained, we did not instruct the model to fake alignment or give it any explicit goal. As future models might infer information about their training process without being told, our results suggest a risk of alignment faking in future models, whether due to a benign preference—as in this case—or not.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Greenblatt, Ryan and Denison, Carson and Wright, Benjamin and Roger, Fabien and MacDiarmid, Monte and Marks, Sam and Treutlein, Johannes and Belonax, Tim and Chen, Jack and Duvenaud, David and Khan, Akbir and Michael, Julian and Mindermann, Sören and Perez, Ethan and Petrini, Linda and Uesato, Jonathan and Kaplan, Jared and Shlegeris, Buck and Bowman, Samuel R. and Hubinger, Evan},
	month = dec,
	year = {2024},
	note = {arXiv:2412.14093 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Lecture début de projet},
}

@misc{zhang_equilibrium_2024,
	title = {Equilibrium {Selection} for {Multi}-agent {Reinforcement} {Learning}: {A} {Unified} {Framework}},
	shorttitle = {Equilibrium {Selection} for {Multi}-agent {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2406.08844},
	doi = {10.48550/arXiv.2406.08844},
	abstract = {While there are numerous works in multi-agent reinforcement learning (MARL), most of them focus on designing algorithms and proving convergence to a Nash equilibrium (NE) or other equilibrium such as coarse correlated equilibrium. However, NEs can be non-unique and their performance varies drastically. Thus, it is important to design algorithms that converge to Nash equilibrium with better rewards or social welfare. In contrast, classical game theory literature has extensively studied equilibrium selection for multi-agent learning in normal-form games, demonstrating that decentralized learning algorithms can asymptotically converge to potential-maximizing or Pareto-optimal NEs. These insights motivate this paper to investigate equilibrium selection in the MARL setting. We focus on the stochastic game model, leveraging classical equilibrium selection results from normal-form games to propose a unified framework for equilibrium selection in stochastic games. The proposed framework is highly modular and can extend various learning rules and their corresponding equilibrium selection results from normal-form games to the stochastic game setting.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Zhang, Runyu and Shamma, Jeff and Li, Na},
	month = jun,
	year = {2024},
	note = {arXiv:2406.08844 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Equilibrium, MARL, Mathematics - Optimization and Control, Theory},
}

@inproceedings{pagan_classification_2023-2,
	address = {Boston MA USA},
	title = {A {Classification} of {Feedback} {Loops} and {Their} {Relation} to {Biases} in {Automated} {Decision}-{Making} {Systems}},
	isbn = {979-8-4007-0381-2},
	url = {https://dl.acm.org/doi/10.1145/3617694.3623227},
	doi = {10.1145/3617694.3623227},
	abstract = {Prediction-based decision-making systems are becoming increasingly prevalent in various domains. Previous studies have demonstrated that such systems are vulnerable to runaway feedback loops, e.g., when police are repeatedly sent back to the same neighborhoods regardless of the actual rate of criminal activity, which exacerbate existing biases. In practice, the automated decisions have dynamic feedback effects on the system itself – which in ML literature is sometimes referred to as performative predictions – that can perpetuate over time, making it difficult for short-sighted design choices to control the system’s evolution. While researchers started proposing longer-term solutions to prevent adverse outcomes (such as bias towards certain groups), these interventions largely depend on ad hoc modeling assumptions and a rigorous theoretical understanding of the feedback dynamics in ML-based decision-making systems is currently missing. In this paper, we use the language of dynamical systems theory, a branch of applied mathematics that deals with the analysis of the interconnection of systems with dynamic behaviors, to rigorously classify the different types of feedback loops in the ML-based decision-making pipeline. By reviewing existing scholarly work, we show that this classification covers many examples discussed in the algorithmic fairness community, thereby providing a unifying and principled framework to study feedback loops. By qualitative analysis, and through a simulation example of recommender systems, we show which specific types of ML biases are affected by each type of feedback loop. We find that the existence of feedback loops in the ML-based decision-making pipeline can perpetuate, reinforce, or even reduce ML biases.},
	language = {en},
	urldate = {2025-01-26},
	booktitle = {Equity and {Access} in {Algorithms}, {Mechanisms}, and {Optimization}},
	publisher = {ACM},
	author = {Pagan, Nicolò and Baumann, Joachim and Elokda, Ezzat and De Pasquale, Giulia and Bolognani, Saverio and Hannák, Anikó},
	month = oct,
	year = {2023},
	keywords = {Feedback loop, Lecture début de projet, Recommandé Audrey, Review},
	pages = {1--14},
}

@inproceedings{pagan_classification_2023-3,
	address = {Boston MA USA},
	title = {A {Classification} of {Feedback} {Loops} and {Their} {Relation} to {Biases} in {Automated} {Decision}-{Making} {Systems}},
	isbn = {979-8-4007-0381-2},
	url = {https://dl.acm.org/doi/10.1145/3617694.3623227},
	doi = {10.1145/3617694.3623227},
	abstract = {Prediction-based decision-making systems are becoming increasingly prevalent in various domains. Previous studies have demonstrated that such systems are vulnerable to runaway feedback loops, e.g., when police are repeatedly sent back to the same neighborhoods regardless of the actual rate of criminal activity, which exacerbate existing biases. In practice, the automated decisions have dynamic feedback effects on the system itself – which in ML literature is sometimes referred to as performative predictions – that can perpetuate over time, making it difficult for short-sighted design choices to control the system’s evolution. While researchers started proposing longer-term solutions to prevent adverse outcomes (such as bias towards certain groups), these interventions largely depend on ad hoc modeling assumptions and a rigorous theoretical understanding of the feedback dynamics in ML-based decision-making systems is currently missing. In this paper, we use the language of dynamical systems theory, a branch of applied mathematics that deals with the analysis of the interconnection of systems with dynamic behaviors, to rigorously classify the different types of feedback loops in the ML-based decision-making pipeline. By reviewing existing scholarly work, we show that this classification covers many examples discussed in the algorithmic fairness community, thereby providing a unifying and principled framework to study feedback loops. By qualitative analysis, and through a simulation example of recommender systems, we show which specific types of ML biases are affected by each type of feedback loop. We find that the existence of feedback loops in the ML-based decision-making pipeline can perpetuate, reinforce, or even reduce ML biases.},
	language = {en},
	urldate = {2025-01-26},
	booktitle = {Equity and {Access} in {Algorithms}, {Mechanisms}, and {Optimization}},
	publisher = {ACM},
	author = {Pagan, Nicolò and Baumann, Joachim and Elokda, Ezzat and De Pasquale, Giulia and Bolognani, Saverio and Hannák, Anikó},
	month = oct,
	year = {2023},
	keywords = {Feedback loop, Review},
	pages = {1--14},
}

@article{fan_test_1998,
	title = {Test of {Significance} {When} {Data} are {Curves}},
	volume = {93},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.1998.10473763},
	doi = {10.1080/01621459.1998.10473763},
	abstract = {With modern technology, massive data can easily be collected in a form of multiple sets of curves. New statistical challenge includes testing whether there is any statistically significant difference among these sets of curves. In this article we propose some new tests for comparing two groups of curves based on the adaptive Neyman test and the wavelet thresholding techniques introduced earlier by Fan. We demonstrate that these tests inherit the properties outlined by Fan and that they are simple and powerful for detecting differences between two sets of curves. We then further generalize the idea to compare multiple sets of curves, resulting in an adaptive high-dimensional analysis of variance, called HANOVA. These newly developed techniques are illustrated by using a dataset on pizza commercials where observations are curves and an analysis of cornea topography in ophthalmology where images of individuals are observed. A simulation example is also presented to illustrate the power of the adaptive Neyman test.},
	number = {443},
	urldate = {2025-02-16},
	journal = {Journal of the American Statistical Association},
	author = {Fan, Jianqing and Lin, Sheng-Kuei},
	month = sep,
	year = {1998},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1080/01621459.1998.10473763},
	pages = {1007--1021},
}

@misc{yang_overview_2021,
	title = {An {Overview} of {Multi}-{Agent} {Reinforcement} {Learning} from {Game} {Theoretical} {Perspective}},
	url = {http://arxiv.org/abs/2011.00583},
	doi = {10.48550/arXiv.2011.00583},
	abstract = {Following the remarkable success of the AlphaGO series, 2019 was a booming year that witnessed significant advances in multi-agent reinforcement learning (MARL) techniques. MARL corresponds to the learning problem in a multi-agent system in which multiple agents learn simultaneously. It is an interdisciplinary domain with a long history that includes game theory, machine learning, stochastic control, psychology, and optimisation. Although MARL has achieved considerable empirical success in solving real-world games, there is a lack of a self-contained overview in the literature that elaborates the game theoretical foundations of modern MARL methods and summarises the recent advances. In fact, the majority of existing surveys are outdated and do not fully cover the recent developments since 2010. In this work, we provide a monograph on MARL that covers both the fundamentals and the latest developments in the research frontier. The goal of our monograph is to provide a self-contained assessment of the current state-of-the-art MARL techniques from a game theoretical perspective. We expect this work to serve as a stepping stone for both new researchers who are about to enter this fast-growing domain and existing domain experts who want to obtain a panoramic view and identify new directions based on recent advances.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Yang, Yaodong and Wang, Jun},
	month = mar,
	year = {2021},
	note = {arXiv:2011.00583 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems, Feedback loop, Game Theory, MARL, Matrix game, Normal-form game, Review},
}

@article{lupu_leveraging_2019,
	title = {Leveraging {Observations} in {Bandits}: {Between} {Risks} and {Benefits}},
	volume = {33},
	copyright = {https://www.aaai.org},
	issn = {2374-3468, 2159-5399},
	shorttitle = {Leveraging {Observations} in {Bandits}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4568},
	doi = {10.1609/aaai.v33i01.33016112},
	abstract = {Imitation learning has been widely used to speed up learning in novice agents, by allowing them to leverage existing data from experts. Allowing an agent to be inﬂuenced by external observations can beneﬁt to the learning process, but it also puts the agent at risk of following sub-optimal behaviours. In this paper, we study this problem in the context of bandits. More speciﬁcally, we consider that an agent (learner) is interacting with a bandit-style decision task, but can also observe a target policy interacting with the same environment. The learner observes only the target’s actions, not the rewards obtained. We introduce a new bandit optimism modiﬁer that uses conditional optimism contingent on the actions of the target in order to guide the agent’s exploration. We analyze the effect of this modiﬁcation on the well-known Upper Conﬁdence Bound algorithm by proving that it preserves a regret upper-bound of order O(ln T ), even in the presence of a very poor target, and we derive the dependency of the expected regret on the general target policy. We provide empirical results showing both great beneﬁts as well as certain limitations inherent to observational learning in the multi-armed bandit setting. Experiments are conducted using targets satisfying theoretical assumptions with high probability, thus narrowing the gap between theory and application.},
	language = {en},
	number = {01},
	urldate = {2025-01-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Lupu, Andrei and Durand, Audrey and Precup, Doina},
	month = jul,
	year = {2019},
	keywords = {Audrey, Bandit},
	pages = {6112--6119},
}

@article{anderson_learning_nodate,
	title = {Learning with contextual information in non-stationary environments},
	abstract = {We consider a repeated decision-making setting in which the decision maker has access to contextual information and lacks a model or a priori knowledge of the relationship between the actions, context, and costs that they aim to minimize. Moreover, we assume that the environment may be non-stationary due to the presence of other agents that may be reacting to our decisions. We propose an algorithm inspired by log-linear learning that uses Boltzmann distributions to generate stochastic policies. We consider two general notions of context and provide regret bounds for each: 1) a finite number of possible measurements and 2) a continuum of measurements that weight a set of finite classes. In the nonstationary setting, we incur some regret but can make it arbitrarily small. We illustrate the operation of the algorithm through two examples: one that uses synthetic data (based on the rock-paper-scissors game) and another that uses real data for malware classification. Both examples exhibit (by construction or naturally) significant lack of stationarity.},
	language = {en},
	author = {Anderson, Sean and Hespanha, Joao P},
	keywords = {Non-stationary, RPS},
}

@misc{karwowski_goodharts_2023,
	title = {Goodhart's {Law} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2310.09144},
	doi = {10.48550/arXiv.2310.09144},
	abstract = {Implementing a reward function that perfectly captures a complex task in the real world is impractical. As a result, it is often appropriate to think of the reward function as a proxy for the true objective rather than as its definition. We study this phenomenon through the lens of Goodhart’s law, which predicts that increasing optimisation of an imperfect proxy beyond some critical point decreases performance on the true objective. First, we propose a way to quantify the magnitude of this effect and show empirically that optimising an imperfect proxy reward often leads to the behaviour predicted by Goodhart’s law for a wide range of environments and reward functions. We then provide a geometric explanation for why Goodhart’s law occurs in Markov decision processes. We use these theoretical insights to propose an optimal early stopping method that provably avoids the aforementioned pitfall and derive theoretical regret bounds for this method. Moreover, we derive a training method that maximises worst-case reward, for the setting where there is uncertainty about the true reward function. Finally, we evaluate our early stopping method experimentally. Our results support a foundation for a theoretically-principled study of reinforcement learning under reward misspecification.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Karwowski, Jacek and Hayman, Oliver and Bai, Xingjian and Kiendlhofer, Klaus and Griffin, Charlie and Skalse, Joar},
	month = oct,
	year = {2023},
	note = {arXiv:2310.09144 [cs]},
	keywords = {Computer Science - Machine Learning, Feedback loop, MDP, RL},
}

@misc{lanctot_population-based_2023,
	title = {Population-based {Evaluation} in {Repeated} {Rock}-{Paper}-{Scissors} as a {Benchmark} for {Multiagent} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2303.03196},
	doi = {10.48550/arXiv.2303.03196},
	abstract = {Progress in fields of machine learning and adversarial planning has benefited significantly from benchmark domains, from checkers and the classic UCI data sets to Go and Diplomacy. In sequential decision-making, agent evaluation has largely been restricted to few interactions against experts, with the aim to reach some desired level of performance (e.g. beating a human professional player). We propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors along with a population of forty-three tournament entries, some of which are intentionally sub-optimal. We describe metrics to measure the quality of agents based both on average returns and exploitability. We then show that several RL, online learning, and language model approaches can learn good counter-strategies and generalize well, but ultimately lose to the top-performing bots, creating an opportunity for research in multiagent learning.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Lanctot, Marc and Schultz, John and Burch, Neil and Smith, Max Olan and Hennes, Daniel and Anthony, Thomas and Perolat, Julien},
	month = oct,
	year = {2023},
	note = {arXiv:2303.03196 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Computer Science - Multiagent Systems, MARL, RPS},
}

@misc{bouteiller_evolution_2024,
	title = {Evolution with {Opponent}-{Learning} {Awareness}},
	url = {http://arxiv.org/abs/2410.17466},
	doi = {10.48550/arXiv.2410.17466},
	abstract = {The universe involves many independent co-learning agents as an ever-evolving part of our observed environment. Yet, in practice, Multi-Agent Reinforcement Learning (MARL) applications are usually constrained to small, homogeneous populations and remain computationally intensive. In this paper, we study how large heterogeneous populations of learning agents evolve in normalform games. We show how, under assumptions commonly made in the multi-armed bandit literature, Multi-Agent Policy Gradient closely resembles the Replicator Dynamic, and we further derive a fast, parallelizable implementation of Opponent-Learning Awareness tailored for evolutionary simulations. This enables us to simulate the evolution of very large populations made of heterogeneous co-learning agents, under both naive and advanced learning strategies. We demonstrate our approach in simulations of 200,000 agents, evolving in the classic games of Hawk-Dove, Stag-Hunt, and Rock-Paper-Scissors. Each game highlights distinct ways in which Opponent-Learning Awareness affects evolution.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Bouteiller, Yann and Soma, Karthik and Beltrame, Giovanni},
	month = oct,
	year = {2024},
	note = {arXiv:2410.17466 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Game Theory, Important, Matrix game, Quantitative Biology - Populations and Evolution, Quantitative Finance - General Finance, RPS},
}

@article{szabo_evolutionary_2007,
	title = {Evolutionary games on graphs},
	volume = {446},
	issn = {03701573},
	url = {http://arxiv.org/abs/cond-mat/0607344},
	doi = {10.1016/j.physrep.2007.04.004},
	abstract = {Game theory is one of the key paradigms behind many scientific disciplines from biology to behavioral sciences to economics. In its evolutionary form and especially when the interacting agents are linked in a specific social network the underlying solution concepts and methods are very similar to those applied in non-equilibrium statistical physics. This review gives a tutorial-type overview of the field for physicists. The first three sections introduce the necessary background in classical and evolutionary game theory from the basic definitions to the most important results. The fourth section surveys the topological complications implied by non-mean-field-type social network structures in general. The last three sections discuss in detail the dynamic behavior of three prominent classes of models: the Prisoner's Dilemma, the Rock-Scissors-Paper game, and Competing Associations. The major theme of the review is in what sense and how the graph structure of interactions can modify and enrich the picture of long term behavioral patterns emerging in evolutionary games.},
	number = {4-6},
	urldate = {2025-01-27},
	journal = {Physics Reports},
	author = {Szabo, Gyorgy and Fath, Gabor},
	month = jul,
	year = {2007},
	note = {arXiv:cond-mat/0607344},
	keywords = {Condensed Matter - Statistical Mechanics, Février, Game Theory, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Quantitative Biology - Populations and Evolution, Review},
	pages = {97--216},
}

@article{balduzzi_open-ended_nodate,
	title = {Open-ended {Learning} in {Symmetric} {Zero}-sum {Games}},
	abstract = {Zero-sum games such as chess and poker are, abstractly, functions that evaluate pairs of agents, for example labeling them ‘winner’ and ‘loser’. If the game is approximately transitive, then selfplay generates sequences of agents of increasing strength. However, nontransitive games, such as rock-paper-scissors, can exhibit strategic cycles, and there is no longer a clear objective – we want agents to increase in strength, but against whom is unclear. In this paper, we introduce a geometric framework for formulating agent objectives in zero-sum games, in order to construct adaptive sequences of objectives that yield openended learning. The framework allows us to reason about population performance in nontransitive games, and enables the development of a new algorithm (rectiﬁed Nash response, PSROrN) that uses game-theoretic niching to construct diverse populations of effective agents, producing a stronger set of agents than existing algorithms. We apply PSROrN to two highly nontransitive resource allocation games and ﬁnd that PSROrN consistently outperforms the existing alternatives.},
	language = {en},
	author = {Balduzzi, David and Garnelo, Marta and Bachrach, Yoram and Czarnecki, Wojciech M and Perolat, Julien and Jaderberg, Max and Graepel, Thore},
	keywords = {Février, Game Theory, Maxime, Zero-sum Games},
}

@article{szolnoki_cyclic_2014,
	title = {Cyclic dominance in evolutionary games: {A} review},
	volume = {11},
	issn = {1742-5689, 1742-5662},
	shorttitle = {Cyclic dominance in evolutionary games},
	url = {http://arxiv.org/abs/1408.6828},
	doi = {10.1098/rsif.2014.0735},
	abstract = {Rock is wrapped by paper, paper is cut by scissors, and scissors are crushed by rock. This simple game is popular among children and adults to decide on trivial disputes that have no obvious winner, but cyclic dominance is also at the heart of predator-prey interactions, the mating strategy of side-blotched lizards, the overgrowth of marine sessile organisms, and the competition in microbial populations. Cyclical interactions also emerge spontaneously in evolutionary games entailing volunteering, reward, punishment, and in fact are common when the competing strategies are three or more regardless of the particularities of the game. Here we review recent advances on the rock-paper-scissors and related evolutionary games, focusing in particular on pattern formation, the impact of mobility, and the spontaneous emergence of cyclic dominance. We also review mean-ﬁeld and zero-dimensional rock-paper-scissors models and the application of the complex Ginzburg-Landau equation, and we highlight the importance and usefulness of statistical physics for the successful study of large-scale ecological systems. Directions for future research, related for example to dynamical effects of coevolutionary rules and invasion reversals due to multi-point interactions, are outlined as well.},
	language = {en},
	number = {100},
	urldate = {2025-01-26},
	journal = {Journal of The Royal Society Interface},
	author = {Szolnoki, Attila and Mobilia, Mauro and Jiang, Luo-Luo and Szczesny, Bartosz and Rucklidge, Alastair M. and Perc, Matjaz},
	month = nov,
	year = {2014},
	note = {arXiv:1408.6828 [physics]},
	keywords = {Closed Orbits, Complex Ginzburg-Landau Equation, Computer Science - Social and Information Networks, Condensed Matter - Statistical Mechanics, Cycle, Game Theory, Heteroclininc Cycles, Hopf Bifurcation, May-Leonard Model, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Physics - Physics and Society, Prisoner's Dilema, Quantitative Biology - Populations and Evolution, RPS, Review},
	pages = {20140735},
}

@book{robinson_topology_2006,
	address = {London New York},
	title = {The topology of the 2x2 games: a new periodic table},
	isbn = {978-0-415-33609-3},
	shorttitle = {The topology of the 2x2 games},
	abstract = {2x2 games provide the very basis of game theory and this book constitutes something approaching a 'periodic table' of the most common games - the prisoner's dilemma, coordination games, chicken and the battle of the sexes among them},
	language = {en},
	publisher = {Routledge},
	author = {Robinson, David and Goforth, David},
	year = {2006},
	keywords = {Février, Game Theory, Important, Matric game, Review},
}

@article{gur_stochastic_nodate,
	title = {Stochastic {Multi}-{Armed}-{Bandit} {Problem} with {Non}-stationary {Rewards}},
	abstract = {In a multi-armed bandit (MAB) problem a gambler needs to choose at each round of play one of K arms, each characterized by an unknown reward distribution. Reward realizations are only observed when an arm is selected, and the gambler’s objective is to maximize his cumulative expected earnings over some given horizon of play T . To do this, the gambler needs to acquire information about arms (exploration) while simultaneously optimizing immediate rewards (exploitation); the price paid due to this trade off is often referred to as the regret, and the main question is how small can this price be as a function of the horizon length T . This problem has been studied extensively when the reward distributions do not change over time; an assumption that supports a sharp characterization of the regret, yet is often violated in practical settings. In this paper, we focus on a MAB formulation which allows for a broad range of temporal uncertainties in the rewards, while still maintaining mathematical tractability. We fully characterize the (regret) complexity of this class of MAB problems by establishing a direct link between the extent of allowable reward “variation” and the minimal achievable regret, and by establishing a connection between the adversarial and the stochastic MAB frameworks.},
	language = {en},
	author = {Gur, Yonatan and Zeevi, Assaf and Besbes, Omar},
	keywords = {MAB, Non-stationary, Recommandé Audrey},
}

@misc{lin_online_2022,
	title = {Online {Learning} in {Iterated} {Prisoner}'s {Dilemma} to {Mimic} {Human} {Behavior}},
	url = {http://arxiv.org/abs/2006.06580},
	doi = {10.48550/arXiv.2006.06580},
	abstract = {As an important psychological and social experiment, the Iterated Prisoner's Dilemma (IPD) treats the choice to cooperate or defect as an atomic action. We propose to study the behaviors of online learning algorithms in the Iterated Prisoner's Dilemma (IPD) game, where we investigate the full spectrum of reinforcement learning agents: multi-armed bandits, contextual bandits and reinforcement learning. We evaluate them based on a tournament of iterated prisoner's dilemma where multiple agents can compete in a sequential fashion. This allows us to analyze the dynamics of policies learned by multiple self-interested independent reward-driven agents, and also allows us study the capacity of these algorithms to fit the human behaviors. Results suggest that considering the current situation to make decision is the worst in this kind of social dilemma game. Multiples discoveries on online learning behaviors and clinical validations are stated, as an effort to connect artificial intelligence algorithms with human behaviors and their abnormal states in neuropsychiatric conditions.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Lin, Baihan and Bouneffouf, Djallel and Cecchi, Guillermo},
	month = aug,
	year = {2022},
	note = {arXiv:2006.06580 [cs]},
	keywords = {Bandit, Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Computer Science - Multiagent Systems, MARL, Prisoner's Dilema, Q-Learning, Quantitative Biology - Neurons and Cognition, Recommandé Audrey},
}

@article{alatur_multi-player_nodate,
	title = {Multi-{Player} {Bandits}: {The} {Adversarial} {Case}},
	abstract = {We consider a setting where multiple players sequentially choose among a common set of actions (arms). Motivated by an application to cognitive radio networks, we assume that players incur a loss upon colliding, and that communication between players is not possible. Existing approaches assume that the system is stationary. Yet this assumption is often violated in practice, e.g., due to signal strength ﬂuctuations. In this work, we design the ﬁrst multi-player Bandit algorithm that provably works in arbitrarily changing environments, where the losses of the arms may even be chosen by an adversary. This resolves an open problem posed by Rosenski et al. (2016).},
	language = {en},
	author = {Alatur, Pragnya and Alatur, Pragnya and Levy, Kﬁr Y and Krause, Andreas},
	keywords = {Recommandé Audrey},
}

@article{miller_strategic_nodate,
	title = {Strategic {Classification} is {Causal} {Modeling} in {Disguise}},
	abstract = {Consequential decision-making incentivizes individuals to strategically adapt their behavior to the speciﬁcs of the decision rule. While a long line of work has viewed strategic adaptation as gaming and attempted to mitigate its effects, recent work has instead sought to design classiﬁers that incentivize individuals to improve a desired quality. Key to both accounts is a cost function that dictates which adaptations are rational to undertake. In this work, we develop a causal framework for strategic adaptation. Our causal perspective clearly distinguishes between gaming and improvement and reveals an important obstacle to incentive design. We prove any procedure for designing classiﬁers that incentivize improvement must inevitably solve a non-trivial causal inference problem. We show a similar result holds for designing cost functions that satisfy the requirements of previous work. With the beneﬁt of hindsight, our results show much of the prior work on strategic classiﬁcation is causal modeling in disguise.},
	language = {en},
	author = {Miller, John and Milli, Smitha and Hardt, Moritz},
	keywords = {Lecture début de projet, Recommandé Audrey},
}

@misc{richens_robust_2024,
	title = {Robust agents learn causal world models},
	url = {http://arxiv.org/abs/2402.10877},
	doi = {10.48550/arXiv.2402.10877},
	abstract = {It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound for a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Richens, Jonathan and Everitt, Tom},
	month = jul,
	year = {2024},
	note = {arXiv:2402.10877 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Lecture début de projet, Recommandé Audrey},
}

@article{perdomo_performative_nodate,
	title = {Performative {Prediction}},
	abstract = {When predictions support decisions they may inﬂuence the outcome they aim to predict. We call such predictions performative; the prediction inﬂuences the target. Performativity is a wellstudied phenomenon in policy-making that has so far been neglected in supervised learning. When ignored, performativity surfaces as undesirable distribution shift, routinely addressed with retraining. We develop a risk minimization framework for performative prediction bringing together concepts from statistics, game theory, and causality. A conceptual novelty is an equilibrium notion we call performative stability. Performative stability implies that the predictions are calibrated not against past outcomes, but against the future outcomes that manifest from acting on the prediction. Our main results are necessary and sufﬁcient conditions for the convergence of retraining to a performatively stable point of nearly minimal loss. In full generality, performative prediction strictly subsumes the setting known as strategic classiﬁcation. We thus also give the ﬁrst sufﬁcient conditions for retraining to overcome strategic feedback effects.},
	language = {en},
	author = {Perdomo, Juan C and Zrnic, Tijana and Mendler-Dünner, Celestine and Hardt, Moritz},
	keywords = {Lecture début de projet, Recommandé Audrey},
}

@article{aghajohari_loqa_2024,
	title = {{LOQA}: {LEARNING} {WITH} {OPPONENT} {Q}-{LEARNING} {AWARENESS}},
	abstract = {In various real-world scenarios, interactions among agents often resemble the dynamics of general-sum games, where each agent strives to optimize its own utility. Despite the ubiquitous relevance of such settings, decentralized machine learning algorithms have struggled to find equilibria that maximize individual utility while preserving social welfare. In this paper we introduce Learning with Opponent QLearning Awareness (LOQA), a novel, decentralized reinforcement learning algorithm tailored to optimizing an agent’s individual utility while fostering cooperation among adversaries in partially competitive environments. LOQA assumes the opponent samples actions proportionally to their action-value function Q. Experimental results demonstrate the effectiveness of LOQA at achieving state-of-theart performance in benchmark scenarios such as the Iterated Prisoner’s Dilemma and the Coin Game. LOQA achieves these outcomes with a significantly reduced computational footprint, making it a promising approach for practical multi-agent applications.},
	language = {en},
	author = {Aghajohari, Milad and Duque, Juan Agustin and Cooijmans, Tim and Courville, Aaron},
	year = {2024},
	keywords = {Game Theory, Lecture début de projet, Prisoner's Dilema, RL, Recommandé Audrey},
}

@article{press_iterated_2012,
	title = {Iterated {Prisoner}’s {Dilemma} contains strategies that dominate any evolutionary opponent},
	volume = {109},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1206569109},
	doi = {10.1073/pnas.1206569109},
	abstract = {The two-player Iterated Prisoner’s Dilemma game is a model for both sentient and evolutionary behaviors, especially including the emergence of cooperation. It is generally assumed that there exists no simple ultimatum strategy whereby one player can enforce a unilateral claim to an unfair share of rewards. Here, we show that such strategies unexpectedly do exist. In particular, a player X who is witting of these strategies can (
              i
              ) deterministically set her opponent Y’s score, independently of his strategy or response, or (
              ii
              ) enforce an extortionate linear relation between her and his scores. Against such a player, an evolutionary player’s best response is to accede to the extortion. Only a player with a theory of mind about his opponent can do better, in which case Iterated Prisoner’s Dilemma is an Ultimatum Game.},
	language = {en},
	number = {26},
	urldate = {2025-01-26},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Press, William H. and Dyson, Freeman J.},
	month = jun,
	year = {2012},
	keywords = {Game Theory, Lecture début de projet, Prisoner's Dilema, Recommandé Audrey},
	pages = {10409--10413},
}

@inproceedings{lewis_deal_2017,
	address = {Copenhagen, Denmark},
	title = {Deal or {No} {Deal}? {End}-to-{End} {Learning} of {Negotiation} {Dialogues}},
	shorttitle = {Deal or {No} {Deal}?},
	url = {http://aclweb.org/anthology/D17-1259},
	doi = {10.18653/v1/D17-1259},
	language = {en},
	urldate = {2025-01-26},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural}           {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lewis, Mike and Yarats, Denis and Dauphin, Yann and Parikh, Devi and Batra, Dhruv},
	year = {2017},
	keywords = {LLM, Lecture début de projet, Negotiation, RL, Recommandé Audrey},
	pages = {2443--2453},
}

@misc{coston_counterfactual_2020,
	title = {Counterfactual {Risk} {Assessments}, {Evaluation}, and {Fairness}},
	url = {http://arxiv.org/abs/1909.00066},
	doi = {10.48550/arXiv.1909.00066},
	abstract = {Algorithmic risk assessments are increasingly used to help humans make decisions in high-stakes settings, such as medicine, criminal justice and education. In each of these cases, the purpose of the risk assessment tool is to inform actions, such as medical treatments or release conditions, often with the aim of reducing the likelihood of an adverse event such as hospital readmission or recidivism. Problematically, most tools are trained and evaluated on historical data in which the outcomes observed depend on the historical decision-making policy. These tools thus reflect risk under the historical policy, rather than under the different decision options that the tool is intended to inform. Even when tools are constructed to predict risk under a specific decision, they are often improperly evaluated as predictors of the target outcome.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Coston, Amanda and Mishler, Alan and Kennedy, Edward H. and Chouldechova, Alexandra},
	month = jan,
	year = {2020},
	note = {arXiv:1909.00066 [stat]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Lecture début de projet, Recommandé Audrey, Statistics - Applications, Statistics - Machine Learning, Statistics - Methodology},
}

@inproceedings{slyman_vlslice_2023,
	title = {{VLSlice}: {Interactive} {Vision}-and-{Language} {Slice} {Discovery}},
	shorttitle = {{VLSlice}},
	url = {http://arxiv.org/abs/2309.06703},
	doi = {10.1109/ICCV51070.2023.01403},
	abstract = {Recent work in vision-and-language demonstrates that large-scale pretraining can learn generalizable models that are efficiently transferable to downstream tasks. While this may improve dataset-scale aggregate metrics, analyzing performance around hand-crafted subgroups targeting specific bias dimensions reveals systemic undesirable behaviors. However, this subgroup analysis is frequently stalled by annotation efforts, which require extensive time and resources to collect the necessary data. Prior art attempts to automatically discover subgroups to circumvent these constraints but typically leverages model behavior on existing task-specific annotations and rapidly degrades on more complex inputs beyond “tabular” data, none of which study vision-and-language models. This paper presents VLSlice, an interactive system enabling user-guided discovery of coherent representation-level subgroups with consistent visiolinguistic behavior, denoted as vision-and-language slices, from unlabeled image sets. We show that VLSlice enables users to quickly generate diverse high-coherency slices in a user study (n=22) and release the tool publicly1.},
	language = {en},
	urldate = {2025-01-26},
	booktitle = {2023 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Slyman, Eric and Kahng, Minsuk and Lee, Stefan},
	month = oct,
	year = {2023},
	note = {arXiv:2309.06703 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Demande CCAI phase II},
	pages = {15245--15255},
}

@article{heuillet_tracking_nodate,
	title = {Tracking the {Risk} of {Machine} {Learning} {Systems} with {Partial} {Monitoring}},
	abstract = {Although efficient at performing specific tasks, Machine Learning Systems (MLSs) remain vulnerable to instabilities such as noise or adversarial attacks. In this work, we aim to track the risk exposure of an MLS to these events. We formulate this problem under the stochastic Partial Monitoring (PM) setting. We focus on two instances of partial monitoring, namely the Apple Tasting and Label Efficient games, that are particularly relevant to our problem. Our review of the practicality of existing algorithms motivates RandCBP, a randomized variation of the deterministic algorithm Confidence Bound (CBP) inspired by recent theoretical developments in the bandits setting. Our preliminary results indicate that RandCBP enjoys the same regret guarantees as its deterministic counterpart CBP and achieves competitive empirical performance on settings of interest which suggests it could be a suitable candidate for our problem.},
	language = {en},
	author = {Heuillet, Maxime and Durand, Audrey},
	keywords = {Demande CCAI phase II},
}

@misc{ross_right_2017,
	title = {Right for the {Right} {Reasons}: {Training} {Differentiable} {Models} by {Constraining} their {Explanations}},
	shorttitle = {Right for the {Right} {Reasons}},
	url = {http://arxiv.org/abs/1703.03717},
	doi = {10.48550/arXiv.1703.03717},
	abstract = {Neural networks are among the most accurate supervised learning methods in use today. However, their opacity makes them difﬁcult to trust in critical applications, especially if conditions in training may differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions. These tools can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efﬁciently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients. We apply these penalties both based on expert annotation and in an unsupervised fashion that produces multiple classiﬁers with qualitatively different decision boundaries. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
	month = may,
	year = {2017},
	note = {arXiv:1703.03717 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Demande CCAI phase II, Statistics - Machine Learning},
}

@article{kaelbling_planning_1998,
	title = {Planning and acting in partially observable stochastic domains},
	volume = {101},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S000437029800023X},
	doi = {10.1016/S0004-3702(98)00023-X},
	abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (MDPS) and partially observable MDPS (POMDPS). We then outline a novel algorithm for solving POMDPS off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to POMDPS, and of some possibilities for finding approximate solutions. 0 1998 Elsevier Science B.V. All rights reserved.},
	language = {en},
	number = {1-2},
	urldate = {2025-01-26},
	journal = {Artificial Intelligence},
	author = {Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra, Anthony R.},
	month = may,
	year = {1998},
	keywords = {Demande CCAI phase II},
	pages = {99--134},
}

@misc{brown_performative_2022,
	title = {Performative {Prediction} in a {Stateful} {World}},
	url = {http://arxiv.org/abs/2011.03885},
	doi = {10.48550/arXiv.2011.03885},
	abstract = {Deployed supervised machine learning models make predictions that interact with and inﬂuence the world. This phenomenon is called performative prediction by Perdomo et al. (ICML 2020). It is an ongoing challenge to understand the inﬂuence of such predictions as well as design tools so as to control that inﬂuence. We propose a theoretical framework where the response of a target population to the deployed classiﬁer is modeled as a function of the classiﬁer and the current state (distribution) of the population. We show necessary and suﬃcient conditions for convergence to an equilibrium of two retraining algorithms, repeated risk minimization and a lazier variant. Furthermore, convergence is near an optimal classiﬁer. We thus generalize results of Perdomo et al., whose performativity framework does not assume any dependence on the state of the target population. A particular phenomenon captured by our model is that of distinct groups that acquire information and resources at diﬀerent rates to be able to respond to the latest deployed classiﬁer. We study this phenomenon theoretically and empirically.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Brown, Gavin and Hod, Shlomi and Kalemaj, Iden},
	month = feb,
	year = {2022},
	note = {arXiv:2011.03885 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Demande CCAI phase II},
}

@article{lattimore_partial_nodate,
	title = {Partial monitoring},
	abstract = {We provide a novel algorithm for adversarial k-action d-outcome partial monitoring that is adaptive, intuitive and efﬁcient. The highlight is that for the non-degenerate locally observable games, the n-round minimax regret is bounded by 6mk3/2√n log(k), where m is the number of signals. This matches the best known information-theoretic upper bound derived via Bayesian minimax duality. The same algorithm also achieves near-optimal regret for full information, bandit and globally observable games. High probability bounds and simple experiments are also provided.},
	language = {en},
	author = {Lattimore, Tor and Szepesvari, Csaba},
	keywords = {Demande CCAI phase II},
}

@misc{farquhar_statistical_2021,
	title = {On {Statistical} {Bias} {In} {Active} {Learning}: {How} and {When} {To} {Fix} {It}},
	shorttitle = {On {Statistical} {Bias} {In} {Active} {Learning}},
	url = {http://arxiv.org/abs/2101.11665},
	doi = {10.48550/arXiv.2101.11665},
	abstract = {Active learning is a powerful tool when labelling data is expensive, but it introduces a bias because the training data no longer follows the population distribution. We formalize this bias and investigate the situations in which it can be harmful and sometimes even helpful. We further introduce novel corrective weights to remove bias when doing so is beneﬁcial. Through this, our work not only provides a useful mechanism that can improve the active learning approach, but also an explanation of the empirical successes of various existing approaches which ignore this bias. In particular, we show that this bias can be actively helpful when training overparameterized models—like neural networks—with relatively little data.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Farquhar, Sebastian and Gal, Yarin and Rainforth, Tom},
	month = may,
	year = {2021},
	note = {arXiv:2101.11665 [stat]},
	keywords = {Computer Science - Machine Learning, Demande CCAI phase II, Statistics - Machine Learning},
}

@article{shen_metric-fair_nodate,
	title = {Metric-{Fair} {Active} {Learning}},
	abstract = {Active learning has become a prevalent technique for designing label-efﬁcient algorithms, where the central principle is to only query and ﬁt “informative” labeled instances. It is, however, known that an active learning algorithm may incur unfairness due to such instance selection procedure. In this paper, we henceforth study metric-fair active learning of homogeneous halfspaces, and show that under the distribution-dependent PAC learning model, fairness and label efﬁciency can be achieved simultaneously. We further propose two extensions of our main results: 1) we show that it is possible to make the algorithm robust to the adversarial noise – one of the most challenging noise models in learning theory; and 2) it is possible to signiﬁcantly improve the label complexity when the underlying halfspace is sparse.},
	language = {en},
	author = {Shen, Jie and Cui, Nan and Wang, Jing},
	keywords = {Demande CCAI phase II},
}

@misc{bachman_learning_2017,
	title = {Learning {Algorithms} for {Active} {Learning}},
	url = {http://arxiv.org/abs/1708.00088},
	doi = {10.48550/arXiv.1708.00088},
	abstract = {We introduce a model that learns active learning algorithms via metalearning. For a distribution of related tasks, our model jointly learns: a data representation, an item selection heuristic, and a method for constructing prediction functions from labeled training sets. Our model uses the item selection heuristic to gather labeled training sets from which to construct prediction functions. Using the Omniglot and MovieLens datasets, we test our model in synthetic and practical settings.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Bachman, Philip and Sordoni, Alessandro and Trischler, Adam},
	month = jul,
	year = {2017},
	note = {arXiv:1708.00088 [cs]},
	keywords = {Computer Science - Machine Learning, Demande CCAI phase II},
}

@misc{ning_improving_2021,
	title = {Improving {Model} {Robustness} by {Adaptively} {Correcting} {Perturbation} {Levels} with {Active} {Queries}},
	url = {http://arxiv.org/abs/2103.14824},
	doi = {10.48550/arXiv.2103.14824},
	abstract = {In addition to high accuracy, robustness is becoming increasingly important for machine learning models in various applications. Recently, much research has been devoted to improving the model robustness by training with noise perturbations. Most existing studies assume a ﬁxed perturbation level for all training examples, which however hardly holds in real tasks. In fact, excessive perturbations may destroy the discriminative content of an example, while deﬁcient perturbations may fail to provide helpful information for improving the robustness. Motivated by this observation, we propose to adaptively adjust the perturbation levels for each example in the training process. Speciﬁcally, a novel active learning framework is proposed to allow the model to interactively query the correct perturbation level from human experts. By designing a cost-effective sampling strategy along with a new query type, the robustness can be signiﬁcantly improved with a few queries. Both theoretical analysis and experimental studies validate the effectiveness of the proposed approach.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Ning, Kun-Peng and Tao, Lue and Chen, Songcan and Huang, Sheng-Jun},
	month = mar,
	year = {2021},
	note = {arXiv:2103.14824 [cs]},
	keywords = {Computer Science - Machine Learning, Demande CCAI phase II},
}

@article{jiang_general_2023,
	title = {General intelligence requires rethinking exploration},
	volume = {10},
	issn = {2054-5703},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.230539},
	doi = {10.1098/rsos.230539},
	abstract = {We are at the cusp of a transition from ‘learning from data’ to ‘learning what data to learn from’ as a central focus of artificial intelligence (AI) research. While the first-order learning problem is not completely solved, large models under unified architectures, such as transformers, have shifted the learning bottleneck from how to effectively train models to how to effectively acquire and use task-relevant data. This problem, which we frame as
              exploration
              , is a universal aspect of learning in open-ended domains like the real world. Although the study of exploration in AI is largely limited to the field of reinforcement learning, we argue that exploration is essential to all learning systems, including supervised learning. We propose the problem of
              generalized exploration
              to conceptually unify exploration-driven learning between supervised learning and reinforcement learning, allowing us to highlight key similarities across learning settings and open research challenges. Importantly, generalized exploration is a necessary objective for maintaining open-ended learning processes, which in continually learning to discover and solve new problems, provides a promising path to more general intelligence.},
	language = {en},
	number = {6},
	urldate = {2025-01-26},
	journal = {Royal Society Open Science},
	author = {Jiang, Minqi and Rocktäschel, Tim and Grefenstette, Edward},
	month = jun,
	year = {2023},
	keywords = {Demande CCAI phase II},
	pages = {230539},
}

@misc{mansoury_feedback_2020,
	title = {Feedback {Loop} and {Bias} {Amplification} in {Recommender} {Systems}},
	url = {http://arxiv.org/abs/2007.13019},
	doi = {10.48550/arXiv.2007.13019},
	abstract = {Recommendation algorithms are known to suffer from popularity bias; a few popular items are recommended frequently while the majority of other items are ignored. These recommendations are then consumed by the users, their reaction will be logged and added to the system: what is generally known as a feedback loop. In this paper, we propose a method for simulating the users interaction with the recommenders in an offline setting and study the impact of feedback loop on the popularity bias amplification of several recommendation algorithms. We then show how this bias amplification leads to several other problems such as declining the aggregate diversity, shifting the representation of users’ taste over time and also homogenization of the users experience. In particular, we show that the impact of feedback loop is generally stronger for the users who belong to the minority group.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Mansoury, Masoud and Abdollahpouri, Himan and Pechenizkiy, Mykola and Mobasher, Bamshad and Burke, Robin},
	month = jul,
	year = {2020},
	note = {arXiv:2007.13019 [cs]},
	keywords = {Computer Science - Information Retrieval, Demande CCAI phase II},
}

@misc{pacchiano_dueling_2023,
	title = {Dueling {RL}: {Reinforcement} {Learning} with {Trajectory} {Preferences}},
	shorttitle = {Dueling {RL}},
	url = {http://arxiv.org/abs/2111.04850},
	doi = {10.48550/arXiv.2111.04850},
	abstract = {We consider the problem of preference based reinforcement learning (PbRL), where, unlike traditional reinforcement learning, an agent receives feedback only in terms of a 1 bit (0/1) preference over a trajectory pair instead of absolute rewards for them. The success of the traditional RL framework crucially relies on the underlying agent-reward model, which, however, depends on how accurately a system designer can express an appropriate reward function and often a non-trivial task. The main novelty of our framework is the ability to learn from preference-based trajectory feedback that eliminates the need to hand-craft numeric reward models. This paper sets up a formal framework for the PbRL problem with non-markovian rewards, where the trajectory preferences are encoded by a generalized linear model of dimension \$d\$. Assuming the transition model is known, we then propose an algorithm with almost optimal regret guarantee of \${\textbackslash}tilde \{{\textbackslash}mathcal\{O\}\}{\textbackslash}left( SH d {\textbackslash}log (T / {\textbackslash}delta) {\textbackslash}sqrt\{T\} {\textbackslash}right)\$. We further, extend the above algorithm to the case of unknown transition dynamics, and provide an algorithm with near optimal regret guarantee \${\textbackslash}widetilde\{{\textbackslash}mathcal\{O\}\}(({\textbackslash}sqrt\{d\} + H{\textasciicircum}2 + {\textbar}{\textbackslash}mathcal\{S\}{\textbar}){\textbackslash}sqrt\{dT\} +{\textbackslash}sqrt\{{\textbar}{\textbackslash}mathcal\{S\}{\textbar}{\textbar}{\textbackslash}mathcal\{A\}{\textbar}TH\} )\$. To the best of our knowledge, our work is one of the first to give tight regret guarantees for preference based RL problems with trajectory preferences.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Pacchiano, Aldo and Saha, Aadirupa and Lee, Jonathan},
	month = feb,
	year = {2023},
	note = {arXiv:2111.04850 [cs]},
	keywords = {Computer Science - Machine Learning, Demande CCAI phase II},
}

@misc{novoseller_dueling_2020,
	title = {Dueling {Posterior} {Sampling} for {Preference}-{Based} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1908.01289},
	doi = {10.48550/arXiv.1908.01289},
	abstract = {In preference-based reinforcement learning (RL), an agent interacts with the environment while receiving preferences instead of absolute feedback. While there is increasing research activity in preference-based RL, the design of formal frameworks that admit tractable theoretical analysis remains an open challenge. Building upon ideas from preference-based bandit learning and posterior sampling in RL, we present DUELING POSTERIOR SAMPLING (DPS), which employs preference-based posterior sampling to learn both the system dynamics and the underlying utility function that governs the preference feedback. As preference feedback is provided on trajectories rather than individual state-action pairs, we develop a Bayesian approach for the credit assignment problem, translating preferences to a posterior distribution over state-action reward models. We prove an asymptotic Bayesian no-regret rate for DPS with a Bayesian linear regression credit assignment model. This is the ﬁrst regret guarantee for preference-based RL to our knowledge. We also discuss possible avenues for extending the proof methodology to other credit assignment models. Finally, we evaluate the approach empirically, showing competitive performance against existing baselines.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Novoseller, Ellen R. and Wei, Yibing and Sui, Yanan and Yue, Yisong and Burdick, Joel W.},
	month = jun,
	year = {2020},
	note = {arXiv:1908.01289 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Demande CCAI phase II, Statistics - Machine Learning},
}

@misc{christiano_deep_2023,
	title = {Deep reinforcement learning from human preferences},
	url = {http://arxiv.org/abs/1706.03741},
	doi = {10.48550/arXiv.1706.03741},
	abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals deﬁned in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than 1\% of our agent’s interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the ﬂexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
	month = feb,
	year = {2023},
	note = {arXiv:1706.03741 [stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Demande CCAI phase II, Statistics - Machine Learning},
}

@article{abramoff_considerations_2023,
	title = {Considerations for addressing bias in artificial intelligence for health equity},
	volume = {6},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-023-00913-9},
	doi = {10.1038/s41746-023-00913-9},
	abstract = {Abstract
            Health equity is a primary goal of healthcare stakeholders: patients and their advocacy groups, clinicians, other providers and their professional societies, bioethicists, payors and value based care organizations, regulatory agencies, legislators, and creators of artificial intelligence/machine learning (AI/ML)-enabled medical devices. Lack of equitable access to diagnosis and treatment may be improved through new digital health technologies, especially AI/ML, but these may also exacerbate disparities, depending on how bias is addressed. We propose an expanded Total Product Lifecycle (TPLC) framework for healthcare AI/ML, describing the sources and impacts of undesirable bias in AI/ML systems in each phase, how these can be analyzed using appropriate metrics, and how they can be potentially mitigated. The goal of these “Considerations” is to educate stakeholders on how potential AI/ML bias may impact healthcare outcomes and how to identify and mitigate inequities; to initiate a discussion between stakeholders on these issues, in order to ensure health equity along the expanded AI/ML TPLC framework, and ultimately, better health outcomes for all.},
	language = {en},
	number = {1},
	urldate = {2025-01-26},
	journal = {npj Digital Medicine},
	author = {Abràmoff, Michael D. and Tarver, Michelle E. and Loyo-Berrios, Nilsa and Trujillo, Sylvia and Char, Danton and Obermeyer, Ziad and Eydelman, Malvina B. and {Foundational Principles of Ophthalmic Imaging and Algorithmic Interpretation Working Group of the Collaborative Community for Ophthalmic Imaging Foundation, Washington, D.C.} and Maisel, William H.},
	month = sep,
	year = {2023},
	keywords = {Demande CCAI phase II},
	pages = {170},
}

@misc{huang_conceptexplainer_2022,
	title = {{ConceptExplainer}: {Interactive} {Explanation} for {Deep} {Neural} {Networks} from a {Concept} {Perspective}},
	shorttitle = {{ConceptExplainer}},
	url = {http://arxiv.org/abs/2204.01888},
	doi = {10.48550/arXiv.2204.01888},
	abstract = {Traditional deep learning interpretability methods which are suitable for non-expert users cannot explain network behaviors at the global level and are inﬂexible at providing ﬁne-grained explanations. As a solution, concept-based explanations are gaining attention due to their human intuitiveness and their ﬂexibility to describe both global and local model behaviors. Concepts are groups of similarly meaningful pixels that express a notion, embedded within the network’s latent space and have primarily been hand-generated, but have recently been discovered by automated approaches. Unfortunately, the magnitude and diversity of discovered concepts makes it difﬁcult for non-experts to navigate and make sense of the concept space, and lack of easy-to-use software also makes concept explanations inaccessible to many non-expert users. Visual analytics can serve a valuable role in bridging these gaps by enabling structured navigation and exploration of the concept space to provide concept-based insights of model behavior to users. To this end, we design, develop, and validate CONCEPTEXPLAINER, a visual analytics system that enables non-expert users to interactively probe and explore the concept space to explain model behavior at the instance/class/global level. The system was developed via iterative prototyping to address a number of design challenges that non-experts face in interpreting the behavior of deep learning models. Via a rigorous user study, we validate how CONCEPTEXPLAINER supports these challenges. Likewise, we conduct a series of usage scenarios to demonstrate how the system supports the interactive analysis of model behavior across a variety of tasks and explanation granularities, such as identifying concepts that are important to classiﬁcation, identifying bias in training data, and understanding how concepts can be shared across diverse and seemingly dissimilar classes.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Huang, Jinbin and Mishra, Aditi and Kwon, Bum Chul and Bryan, Chris},
	month = oct,
	year = {2022},
	note = {arXiv:2204.01888 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Demande CCAI phase II},
}

@misc{jain_biological_2023,
	title = {Biological {Sequence} {Design} with {GFlowNets}},
	url = {http://arxiv.org/abs/2203.04115},
	doi = {10.48550/arXiv.2203.04115},
	abstract = {Design of de novo biological sequences with desired properties, like protein and DNA sequences, often involves an active loop with several rounds of molecule ideation and expensive wet-lab evaluations. These experiments can consist of multiple stages, with increasing levels of precision and cost of evaluation, where candidates are filtered. This makes the diversity of proposed candidates a key consideration in the ideation phase. In this work, we propose an active learning algorithm leveraging epistemic uncertainty estimation and the recently proposed GFlowNets as a generator of diverse candidate solutions, with the objective to obtain a diverse batch of useful (as defined by some utility function, for example, the predicted anti-microbial activity of a peptide) and informative candidates after each round. We also propose a scheme to incorporate existing labeled datasets of candidates, in addition to a reward function, to speed up learning in GFlowNets. We present empirical results on several biological sequence design tasks, and we find that our method generates more diverse and novel batches with high scoring candidates compared to existing approaches.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Jain, Moksh and Bengio, Emmanuel and Garcia, Alex-Hernandez and Rector-Brooks, Jarrid and Dossou, Bonaventure F. P. and Ekbote, Chanakya and Fu, Jie and Zhang, Tianyu and Kilgour, Micheal and Zhang, Dinghuai and Simine, Lena and Das, Payel and Bengio, Yoshua},
	month = may,
	year = {2023},
	note = {arXiv:2203.04115 [q-bio]},
	keywords = {Computer Science - Machine Learning, Demande CCAI phase II, Quantitative Biology - Biomolecules},
}

@misc{akrour_april_2012,
	title = {{APRIL}: {Active} {Preference}-learning based {Reinforcement} {Learning}},
	shorttitle = {{APRIL}},
	url = {http://arxiv.org/abs/1208.0984},
	doi = {10.48550/arXiv.1208.0984},
	abstract = {This paper focuses on reinforcement learning (RL) with limited prior knowledge. In the domain of swarm robotics for instance, the expert can hardly design a reward function or demonstrate the target behavior, forbidding the use of both standard RL and inverse reinforcement learning. Although with a limited expertise, the human expert is still often able to emit preferences and rank the agent demonstrations. Earlier work has presented an iterative preference-based RL framework: expert preferences are exploited to learn an approximate policy return, thus enabling the agent to achieve direct policy search. Iteratively, the agent selects a new candidate policy and demonstrates it; the expert ranks the new demonstration comparatively to the previous best one; the expert’s ranking feedback enables the agent to reﬁne the approximate policy return, and the process is iterated.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Akrour, Riad and Schoenauer, Marc and Sebag, Michèle},
	month = aug,
	year = {2012},
	note = {arXiv:1208.0984 [cs]},
	keywords = {Computer Science - Machine Learning, Demande CCAI phase II},
}

@article{durand_machine_2018,
	title = {A machine learning approach for online automated optimization of super-resolution optical microscopy},
	volume = {9},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-018-07668-y},
	doi = {10.1038/s41467-018-07668-y},
	abstract = {Abstract
            Traditional approaches for finding well-performing parameterizations of complex imaging systems, such as super-resolution microscopes rely on an extensive exploration phase over the illumination and acquisition settings, prior to the imaging task. This strategy suffers from several issues: it requires a large amount of parameter configurations to be evaluated, it leads to discrepancies between well-performing parameters in the exploration phase and imaging task, and it results in a waste of time and resources given that optimization and final imaging tasks are conducted separately. Here we show that a fully automated, machine learning-based system can conduct imaging parameter optimization toward a trade-off between several objectives, simultaneously to the imaging task. Its potential is highlighted on various imaging tasks, such as live-cell and multicolor imaging and multimodal optimization. This online optimization routine can be integrated to various imaging systems to increase accessibility, optimize performance and improve overall imaging quality.},
	language = {en},
	number = {1},
	urldate = {2025-01-26},
	journal = {Nature Communications},
	author = {Durand, Audrey and Wiesner, Theresa and Gardner, Marc-André and Robitaille, Louis-Émile and Bilodeau, Anthony and Gagné, Christian and De Koninck, Paul and Lavoie-Cardinal, Flavie},
	month = dec,
	year = {2018},
	keywords = {Demande CCAI phase II},
	pages = {5247},
}

@misc{azar_general_2023,
	title = {A {General} {Theoretical} {Paradigm} to {Understand} {Learning} from {Human} {Preferences}},
	url = {http://arxiv.org/abs/2310.12036},
	doi = {10.48550/arXiv.2310.12036},
	abstract = {The prevalent deployment of learning from human preferences through reinforcement learning (RLHF) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Azar, Mohammad Gheshlaghi and Rowland, Mark and Piot, Bilal and Guo, Daniel and Calandriello, Daniele and Valko, Michal and Munos, Rémi},
	month = nov,
	year = {2023},
	note = {arXiv:2310.12036 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Demande CCAI phase II, Statistics - Machine Learning},
}

@article{godbout_game-theoretic_nodate,
	title = {A {Game}-{Theoretic} {Perspective} on {Risk}-{Sensitive}{\textbackslash} {Reinforcement} {Learning}},
	abstract = {Most Reinforcement Learning (RL) approaches usually aim to ﬁnd a policy that maximizes its expected return. However, this objective may be inappropriate in many safety-critical domains such as healthcare or autonomous driving, where it is often preferable to optimize for a risk-sensitive measure of the policy’s return as the learning objective, such as the Conditional-Value-at-Risk (CVaR). Although previous literature exists to address the problem of learning CVaR-optimal policies in Markov decision problems, it mostly relies on the distributional RL perspective. In this paper, we solve this problem by rather proposing an approach based on a game theoretic perspective, which can be applied on top of any existing RL algorithm. At the core of our approach is a twoplayer zero-sum game between a policy player and an adversary that perturbs the policy player’s state transitions given a ﬁnite budget. We show that, the closer the players are to the game’s equilibrium point, the closer the learned policy is to the CVaR-optimal one with a risk tolerance explicitly related to the adversary’s budget. We provide a gradient-based training procedure to solve the proposed game by formulating it as a Stackelberg game, enabling the use of deep RL architectures and training algorithms. We illustrate the applicability of our approach on a risky artiﬁcial environment, presenting the different policies learned for various adversary budgets.},
	language = {en},
	author = {Godbout, Mathieu and Heuillet, Maxime and Chandar, Sharath and Bhati, Rupali and Durand, Audrey},
	keywords = {Demande CCAI phase II},
}

@article{boursier_survey_nodate,
	title = {A {Survey} on {Multi}-player {Bandits}},
	abstract = {Due mostly to its application to cognitive radio networks, multiplayer bandits gained a lot of interest in the last decade. A considerable progress has been made on its theoretical aspect. However, the current algorithms are far from applicable and many obstacles remain between these theoretical results and a possible implementation of multiplayer bandits algorithms in real communication networks. This survey contextualizes and organizes the rich multiplayer bandits literature. In light of the existing works, some clear directions for future research appear. We believe that a further study of these diﬀerent directions might lead to theoretical algorithms adapted to real-world situations.},
	language = {en},
	author = {Boursier, Etienne and Perchet, Vianney},
}

@book{lattimore_bandit_2020,
	edition = {1},
	title = {Bandit {Algorithms}},
	copyright = {https://www.cambridge.org/core/terms},
	isbn = {978-1-108-57140-1 978-1-108-48682-8},
	url = {https://www.cambridge.org/core/product/identifier/9781108571401/type/book},
	language = {en},
	urldate = {2025-02-15},
	publisher = {Cambridge University Press},
	author = {Lattimore, Tor and Szepesvári, Csaba},
	month = jul,
	year = {2020},
	doi = {10.1017/9781108571401},
}

@misc{legacci_no-regret_2024,
	title = {No-regret learning in harmonic games: {Extrapolation} in the face of conflicting interests},
	shorttitle = {No-regret learning in harmonic games},
	url = {http://arxiv.org/abs/2412.20203},
	doi = {10.48550/arXiv.2412.20203},
	abstract = {The long-run behavior of multi-agent learning - and, in particular, no-regret learning - is relatively well-understood in potential games, where players have aligned interests. By contrast, in harmonic games - the strategic counterpart of potential games, where players have conflicting interests - very little is known outside the narrow subclass of 2-player zero-sum games with a fully-mixed equilibrium. Our paper seeks to partially fill this gap by focusing on the full class of (generalized) harmonic games and examining the convergence properties of follow-the-regularized-leader (FTRL), the most widely studied class of no-regret learning schemes. As a first result, we show that the continuous-time dynamics of FTRL are Poincar{\textbackslash}'e recurrent, that is, they return arbitrarily close to their starting point infinitely often, and hence fail to converge. In discrete time, the standard, "vanilla" implementation of FTRL may lead to even worse outcomes, eventually trapping the players in a perpetual cycle of best-responses. However, if FTRL is augmented with a suitable extrapolation step - which includes as special cases the optimistic and mirror-prox variants of FTRL - we show that learning converges to a Nash equilibrium from any initial condition, and all players are guaranteed at most O(1) regret. These results provide an in-depth understanding of no-regret learning in harmonic games, nesting prior work on 2-player zero-sum games, and showing at a high level that harmonic games are the canonical complement of potential games, not only from a strategic, but also from a dynamic viewpoint.},
	urldate = {2025-02-12},
	publisher = {arXiv},
	author = {Legacci, Davide and Mertikopoulos, Panayotis and Papadimitriou, Christos H. and Piliouras, Georgios and Pradelski, Bary S. R.},
	month = dec,
	year = {2024},
	note = {arXiv:2412.20203 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Cycle, Exploration, Harmonic game, Important, Mathematics - Optimization and Control, Poincaré récurrent},
}

@misc{lotidis_accelerated_2024,
	title = {Accelerated regularized learning in finite {N}-person games},
	url = {http://arxiv.org/abs/2412.20365},
	doi = {10.48550/arXiv.2412.20365},
	abstract = {Motivated by the success of Nesterov's accelerated gradient algorithm for convex minimization problems, we examine whether it is possible to achieve similar performance gains in the context of online learning in games. To that end, we introduce a family of accelerated learning methods, which we call "follow the accelerated leader" (FTXL), and which incorporates the use of momentum within the general framework of regularized learning - and, in particular, the exponential/multiplicative weights algorithm and its variants. Drawing inspiration and techniques from the continuous-time analysis of Nesterov's algorithm, we show that FTXL converges locally to strict Nash equilibria at a superlinear rate, achieving in this way an exponential speed-up over vanilla regularized learning methods (which, by comparison, converge to strict equilibria at a geometric, linear rate). Importantly, FTXL maintains its superlinear convergence rate in a broad range of feedback structures, from deterministic, full information models to stochastic, realization-based ones, and even when run with bandit, payoff-based information, where players are only able to observe their individual realized payoffs.},
	urldate = {2025-02-12},
	publisher = {arXiv},
	author = {Lotidis, Kyriakos and Giannou, Angeliki and Mertikopoulos, Panayotis and Bambos, Nicholas},
	month = dec,
	year = {2024},
	note = {arXiv:2412.20365 [cs]},
	keywords = {Bandit, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Congestion game, Mathematics - Optimization and Control},
}

@misc{mertikopoulos_unified_2023,
	title = {A unified stochastic approximation framework for learning in games},
	url = {http://arxiv.org/abs/2206.03922},
	doi = {10.48550/arXiv.2206.03922},
	abstract = {We develop a flexible stochastic approximation framework for analyzing the long-run behavior of learning in games (both continuous and finite). The proposed analysis template incorporates a wide array of popular learning algorithms, including gradient-based methods, the exponential / multiplicative weights algorithm for learning in finite games, optimistic and bandit variants of the above, etc. In addition to providing an integrated view of these algorithms, our framework further allows us to obtain several new convergence results, both asymptotic and in finite time, in both continuous and finite games. Specifically, we provide a range of criteria for identifying classes of Nash equilibria and sets of action profiles that are attracting with high probability, and we also introduce the notion of coherence, a game-theoretic property that includes strict and sharp equilibria, and which leads to convergence in finite time. Importantly, our analysis applies to both oracle-based and bandit, payoff-based methods – that is, when players only observe their realized payoffs.},
	language = {en},
	urldate = {2025-02-12},
	publisher = {arXiv},
	author = {Mertikopoulos, Panayotis and Hsieh, Ya-Ping and Cevher, Volkan},
	month = jul,
	year = {2023},
	note = {arXiv:2206.03922 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@article{boursier_survey_nodate,
	title = {A {Survey} on {Multi}-player {Bandits}},
	abstract = {Due mostly to its application to cognitive radio networks, multiplayer bandits gained a lot of interest in the last decade. A considerable progress has been made on its theoretical aspect. However, the current algorithms are far from applicable and many obstacles remain between these theoretical results and a possible implementation of multiplayer bandits algorithms in real communication networks. This survey contextualizes and organizes the rich multiplayer bandits literature. In light of the existing works, some clear directions for future research appear. We believe that a further study of these diﬀerent directions might lead to theoretical algorithms adapted to real-world situations.},
	language = {en},
	author = {Boursier, Etienne and Perchet, Vianney},
}

@article{lefebvre_shallow_nodate,
	title = {On {Shallow} {Planning} {Under} {Partial} {Observability}},
	abstract = {Formulating a real-world problem under the Reinforcement Learning framework involves non-trivial design choices, such as selecting a discount factor for the learning objective (discounted cumulative rewards), which articulates the planning horizon of the agent. This work investigates the impact of the discount factor on the bias-variance trade-off given structural parameters of the underlying Markov Decision Process. Our results support the idea that a shorter planning horizon might be beneficial, especially under partial observability.},
	language = {en},
	author = {Lefebvre, Randy and Durand, Audrey},
}

@inproceedings{heliou_learning_2017,
	title = {Learning with {Bandit} {Feedback} in {Potential} {Games}},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/39ae2ed11b14a4ccb41d35e9d1ba5d11-Abstract.html},
	abstract = {This paper examines the equilibrium convergence properties of no-regret learning with exponential weights in potential games. To establish convergence with minimal information requirements on the players' side, we focus on two frameworks: the semi-bandit case (where players have access to a noisy estimate of their payoff vectors, including strategies they did not play), and the bandit case (where players are only able to observe their in-game, realized payoffs). In the semi-bandit case, we show that the induced sequence of play converges almost surely to a Nash equilibrium at a quasi-exponential rate. In the bandit case, the same result holds for approximate Nash equilibria if we introduce a constant exploration factor that guarantees that action choice probabilities never become arbitrarily small. In particular, if the algorithm is run with a suitably decreasing exploration factor, the sequence of play converges to a bona fide Nash equilibrium with probability 1.},
	urldate = {2025-02-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Heliou, Amélie and Cohen, Johanne and Mertikopoulos, Panayotis},
	year = {2017},
	keywords = {Bandit, EXP3, Epsillon, Feedback loop, Important},
}

@misc{zeng_survey_2023,
	title = {A {Survey} on {Causal} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2302.05209},
	doi = {10.48550/arXiv.2302.05209},
	abstract = {While Reinforcement Learning (RL) achieves tremendous success in sequential decision-making problems of many domains, it still faces key challenges of data inefficiency and the lack of interpretability. Interestingly, many researchers have leveraged insights from the causality literature recently, bringing forth flourishing works to unify the merits of causality and address well the challenges from RL. As such, it is of great necessity and significance to collate these Causal Reinforcement Learning (CRL) works, offer a review of CRL methods, and investigate the potential functionality from causality toward RL. In particular, we divide existing CRL approaches into two categories according to whether their causality-based information is given in advance or not. We further analyze each category in terms of the formalization of different models, ranging from the Markov Decision Process (MDP), Partially Observed Markov Decision Process (POMDP), Multi-Arm Bandits (MAB), and Dynamic Treatment Regime (DTR). Moreover, we summarize the evaluation matrices and open sources while we discuss emerging applications, along with promising prospects for the future development of CRL.},
	urldate = {2025-02-05},
	publisher = {arXiv},
	author = {Zeng, Yan and Cai, Ruichu and Sun, Fuchun and Huang, Libo and Hao, Zhifeng},
	month = jun,
	year = {2023},
	note = {arXiv:2302.05209 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{asmar_efficient_2024,
	title = {Efficient {Multiagent} {Planning} via {Shared} {Action} {Suggestions}},
	url = {http://arxiv.org/abs/2412.11430},
	doi = {10.48550/arXiv.2412.11430},
	abstract = {Decentralized partially observable Markov decision processes with communication (Dec-POMDP-Com) provide a framework for multiagent decision making under uncertainty, but the NEXP-complete complexity renders solutions intractable in general. While sharing actions and observations can reduce the complexity to PSPACE-complete, we propose an approach that bridges POMDPs and Dec-POMDPs by communicating only suggested joint actions, eliminating the need to share observations while maintaining performance comparable to fully centralized planning and execution. Our algorithm estimates joint beliefs using shared actions to prune infeasible beliefs. Each agent maintains possible belief sets for other agents, pruning them based on suggested actions to form an estimated joint belief usable with any centralized policy. This approach requires solving a POMDP for each agent, reducing computational complexity while preserving performance. We demonstrate its effectiveness on several Dec-POMDP benchmarks showing performance comparable to centralized methods when shared actions enable effective belief pruning. This action-based communication framework offers a natural avenue for integrating human-agent cooperation, opening new directions for scalable multiagent planning under uncertainty, with applications in both autonomous systems and human-agent teams.},
	urldate = {2025-02-05},
	publisher = {arXiv},
	author = {Asmar, Dylan M. and Kochenderfer, Mykel J.},
	month = dec,
	year = {2024},
	note = {arXiv:2412.11430 [cs]},
	keywords = {Computer Science - Multiagent Systems},
}

@misc{bernstein_complexity_2013,
	title = {The {Complexity} of {Decentralized} {Control} of {Markov} {Decision} {Processes}},
	url = {http://arxiv.org/abs/1301.3836},
	doi = {10.48550/arXiv.1301.3836},
	abstract = {Planning for distributed agents with partial state information is considered from a decision- theoretic perspective. We describe generalizations of both the MDP and POMDP models that allow for decentralized control. For even a small number of agents, the finite-horizon problems corresponding to both of our models are complete for nondeterministic exponential time. These complexity results illustrate a fundamental difference between centralized and decentralized control of Markov processes. In contrast to the MDP and POMDP problems, the problems we consider provably do not admit polynomial-time algorithms and most likely require doubly exponential time to solve in the worst case. We have thus provided mathematical evidence corresponding to the intuition that decentralized planning problems cannot easily be reduced to centralized problems and solved exactly using established techniques.},
	urldate = {2025-02-05},
	publisher = {arXiv},
	author = {Bernstein, Daniel S. and Zilberstein, Shlomo and Immerman, Neil},
	month = jan,
	year = {2013},
	note = {arXiv:1301.3836 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{claus_dynamics_nodate,
	title = {The {Dynamics} of {Reinforcement} {Learning} in {Cooperative} {Multiagent} {Systems}},
	abstract = {Reinforcement learning can provide a robust and natural means for agents to learn how to coordinate their action choices in multiagent systems. We examine some of the factors that can inﬂuence the dynamics of the learning process in such a setting. We ﬁrst distinguish reinforcement learners that are unaware of (or ignore) the presence of other agents from those that explicitly attempt to learn the value of joint actions and the strategies of their counterparts. We study Q-learning in cooperative multiagent systems under these two perspectives, focusing on the inﬂuence of partial action observability, game structure, and exploration strategies on convergence to (optimal and suboptimal) Nash equilibria and on learned Qvalues.},
	language = {en},
	author = {Claus, Caroline and Boutilier, Craig},
}

@article{liu_distributed_2013,
	title = {Distributed {Output}-{Feedback} {Control} of {Nonlinear} {Multi}-{Agent} {Systems}},
	volume = {58},
	issn = {1558-2523},
	url = {https://ieeexplore.ieee.org/document/6497512/?arnumber=6497512},
	doi = {10.1109/TAC.2013.2257616},
	abstract = {This technical note presents a cyclic-small-gain approach to distributed output-feedback control of nonlinear multi-agent systems. Through novel distributed observer and control law designs, the closed-loop multi-agent system is transformed into a large-scale system composed of input-to-output stable (IOS) subsystems, the IOS gains of which can be appropriately designed. By guaranteeing the IOS of the closed-loop multi-agent system with the recently developed cyclic-small-gain theorem, the outputs of the controlled agents can be driven to within an arbitrarily small neighborhood of the desired agreement value under bounded external disturbances. Moreover, if the system is disturbance-free, then asymptotic convergence can be achieved. Interestingly, the closed-loop distributed system is also robust to bounded time-delays of exchanged information.},
	number = {11},
	urldate = {2025-02-05},
	journal = {IEEE Transactions on Automatic Control},
	author = {Liu, Tengfei and Jiang, Zhong-Ping},
	month = nov,
	year = {2013},
	note = {Conference Name: IEEE Transactions on Automatic Control},
	keywords = {Cyclic-small-gain method, Decentralized control, Multi-agent systems, Nonlinear systems, Observers, Robustness, Topology, distributed control, nonlinear systems, output agreement},
	pages = {2912--2917},
}

@article{seuken_memory-bounded_nodate,
	title = {Memory-{Bounded} {Dynamic} {Programming} for {DEC}-{POMDPs}},
	abstract = {Decentralized decision making under uncertainty has been shown to be intractable when each agent has different partial information about the domain. Thus, improving the applicability and scalability of planning algorithms is an important challenge. We present the ﬁrst memory-bounded dynamic programming algorithm for ﬁnite-horizon decentralized POMDPs. A set of heuristics is used to identify relevant points of the inﬁnitely large belief space. Using these belief points, the algorithm successively selects the best joint policies for each horizon. The algorithm is extremely efﬁcient, having linear time and space complexity with respect to the horizon length. Experimental results show that it can handle horizons that are multiple orders of magnitude larger than what was previously possible, while achieving the same or better solution quality. These results signiﬁcantly increase the applicability of decentralized decision-making techniques.},
	language = {en},
	author = {Seuken, Sven},
}

@article{canese_multi-agent_2021,
	title = {Multi-{Agent} {Reinforcement} {Learning}: {A} {Review} of {Challenges} and {Applications}},
	volume = {11},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2076-3417},
	shorttitle = {Multi-{Agent} {Reinforcement} {Learning}},
	url = {https://www.mdpi.com/2076-3417/11/11/4948},
	doi = {10.3390/app11114948},
	abstract = {In this review, we present an analysis of the most used multi-agent reinforcement learning algorithms. Starting with the single-agent reinforcement learning algorithms, we focus on the most critical issues that must be taken into account in their extension to multi-agent scenarios. The analyzed algorithms were grouped according to their features. We present a detailed taxonomy of the main multi-agent approaches proposed in the literature, focusing on their related mathematical models. For each algorithm, we describe the possible application ﬁelds, while pointing out its pros and cons. The described multi-agent algorithms are compared in terms of the most important characteristics for multi-agent reinforcement learning applications—namely, nonstationarity, scalability, and observability. We also describe the most common benchmark environments used to evaluate the performances of the considered methods.},
	language = {en},
	number = {11},
	urldate = {2025-02-05},
	journal = {Applied Sciences},
	author = {Canese, Lorenzo and Cardarilli, Gian Carlo and Di Nunzio, Luca and Fazzolari, Rocco and Giardino, Daniele and Re, Marco and Spanò, Sergio},
	month = may,
	year = {2021},
	pages = {4948},
}

@article{durand_apprentissage_nodate,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	author = {Durand, Audrey},
}

@article{durand_apprentissage_nodate-1,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	author = {Durand, Audrey},
}

@article{durand_apprentissage_nodate-2,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	author = {Durand, Audrey},
}

@article{durand_apprentissage_nodate-3,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	author = {Durand, Audrey},
}

@article{durand_apprentissage_nodate-4,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	author = {Durand, Audrey},
}

@article{durand_apprentissage_nodate-5,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	author = {Durand, Audrey},
}

@article{durand_apprentissage_nodate-6,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	author = {Durand, Audrey},
}

@article{durand_apprentissage_nodate-7,
	title = {Apprentissage par {Renforcement}},
	language = {en},
	author = {Durand, Audrey},
}

@article{durand_apprentissage_nodate-8,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	author = {Durand, Audrey},
}

@article{durand_apprentissage_nodate-9,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	journal = {ℝ s},
	author = {Durand, Audrey},
}

@article{durand_apprentissage_nodate-10,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	author = {Durand, Audrey},
}

@article{noauthor_3-dynamic_programming_nodate,
	title = {3-dynamic\_programming},
	language = {en},
}

@article{durand_apprentissage_nodate-11,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	author = {Durand, Audrey},
}

@article{durand_apprentissage_nodate-12,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	author = {Durand, Audrey},
}

@article{noauthor_3-dynamic_programming_nodate-1,
	title = {3-dynamic\_programming},
	language = {en},
}

@article{durand_apprentissage_nodate-13,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	author = {Durand, Audrey},
}

@article{durand_apprentissage_nodate-14,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	author = {Durand, Audrey},
}

@article{durand_apprentissage_nodate-15,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	author = {Durand, Audrey},
}

@article{durand_apprentissage_nodate-16,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	author = {Durand, Audrey},
}

@article{durand_apprentissage_nodate-17,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	author = {Durand, Audrey},
}

@article{durand_apprentissage_nodate-18,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	author = {Durand, Audrey},
}

@inproceedings{chapelle_empirical_2011,
	title = {An {Empirical} {Evaluation} of {Thompson} {Sampling}},
	volume = {24},
	url = {https://papers.nips.cc/paper_files/paper/2011/hash/e53a0a2978c28872a4505bdb51db06dc-Abstract.html},
	abstract = {Thompson sampling is one of oldest heuristic to address the exploration / exploitation trade-off, but it is surprisingly not very popular in the literature. We present here some empirical results using Thompson sampling on simulated and real data, and show that it is highly competitive. And since this heuristic is very easy to implement, we argue that it should be part of the standard baselines to compare against.},
	urldate = {2025-02-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chapelle, Olivier and Li, Lihong},
	year = {2011},
}

@article{durand_apprentissage_nodate-19,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	author = {Durand, Audrey},
}

@article{durand_apprentissage_nodate-20,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	author = {Durand, Audrey},
}

@article{durand_apprentissage_nodate-21,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	author = {Durand, Audrey},
}

@book{sutton_reinforcement_2018,
	address = {Cambridge, Massachusetts},
	edition = {Second edition},
	series = {Adaptive computation and machine learning series},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	language = {en},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {2018},
	keywords = {Reinforcement learning},
}

@article{durand_apprentissage_nodate-22,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	author = {Durand, Audrey},
}

@article{durand_apprentissage_nodate-23,
	title = {Apprentissage par {Renforcement}},
	language = {en},
	author = {Durand, Audrey},
}

@article{durand_apprentissage_nodate-24,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	author = {Durand, Audrey},
}

@article{noauthor_3-exploration_exploitation_nodate,
	title = {3-exploration\_exploitation},
	language = {fr},
}

@book{lattimore_bandit_2020,
	edition = {1},
	title = {Bandit {Algorithms}},
	copyright = {https://www.cambridge.org/core/terms},
	isbn = {978-1-108-57140-1 978-1-108-48682-8},
	url = {https://www.cambridge.org/core/product/identifier/9781108571401/type/book},
	language = {en},
	urldate = {2025-02-03},
	publisher = {Cambridge University Press},
	author = {Lattimore, Tor and Szepesvári, Csaba},
	month = jul,
	year = {2020},
	doi = {10.1017/9781108571401},
	keywords = {Notes de Cours, RL, Theory},
}

@misc{marzo_large_2024,
	title = {Large {Language} {Model} agents can coordinate beyond human scale},
	url = {http://arxiv.org/abs/2409.02822},
	doi = {10.48550/arXiv.2409.02822},
	abstract = {Large Language Models (LLMs) are increasingly deployed as interacting agents, forming ``LLM societies''. Understanding whether these societies can self-organize and coordinate on norms without external influence is crucial to understand their risks and opportunities. Here we explore their opinion dynamics finding that it is governed by a majority force coefficient such that LLM societies can spontaneously reach consensus only up to a critical group size. This critical size grows exponentially with the language understanding capabilities of the models, exceeding the typical size of informal human groups for advanced LLMs. These results reveal emerging self-organization properties in LLM societies and provide insights for designing collaborative AI systems where coordination is either a goal or a risk.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Marzo, Giordano De and Castellano, Claudio and Garcia, David},
	month = dec,
	year = {2024},
	note = {arXiv:2409.02822 [physics]},
	keywords = {Feedback loop, LLM, Multi-agent, Negotiation, Physics - Physics and Society},
}

@misc{yongacoglu_paths_2024,
	title = {Paths to {Equilibrium} in {Games}},
	url = {http://arxiv.org/abs/2403.18079},
	doi = {10.48550/arXiv.2403.18079},
	abstract = {In multi-agent reinforcement learning (MARL) and game theory, agents repeatedly interact and revise their strategies as new data arrives, producing a sequence of strategy profiles. This paper studies sequences of strategies satisfying a pairwise constraint inspired by policy updating in reinforcement learning, where an agent who is best responding in one period does not switch its strategy in the next period. This constraint merely requires that optimizing agents do not switch strategies, but does not constrain the non-optimizing agents in any way, and thus allows for exploration. Sequences with this property are called satisficing paths, and arise naturally in many MARL algorithms. A fundamental question about strategic dynamics is such: for a given game and initial strategy profile, is it always possible to construct a satisficing path that terminates at an equilibrium? The resolution of this question has implications about the capabilities or limitations of a class of MARL algorithms. We answer this question in the affirmative for normal-form games. Our analysis reveals a counterintuitive insight that reward deteriorating strategic updates are key to driving play to equilibrium along a satisficing path.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Yongacoglu, Bora and Arslan, Gürdal and Pavel, Lacra and Yüksel, Serdar},
	month = oct,
	year = {2024},
	note = {arXiv:2403.18079 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Equilibrium, MARL, Theory},
}

@article{alatur_multi-player_nodate,
	title = {Multi-{Player} {Bandits}: {The} {Adversarial} {Case}},
	abstract = {We consider a setting where multiple players sequentially choose among a common set of actions (arms). Motivated by an application to cognitive radio networks, we assume that players incur a loss upon colliding, and that communication between players is not possible. Existing approaches assume that the system is stationary. Yet this assumption is often violated in practice, e.g., due to signal strength ﬂuctuations. In this work, we design the ﬁrst multi-player Bandit algorithm that provably works in arbitrarily changing environments, where the losses of the arms may even be chosen by an adversary. This resolves an open problem posed by Rosenski et al. (2016).},
	language = {en},
	author = {Alatur, Pragnya and Alatur, Pragnya and Levy, Kﬁr Y and Krause, Andreas},
}

@article{dekel_1_nodate,
	title = {1 {Recap}: {Diﬀerence} between {Experts} and {Bandits}},
	language = {en},
	author = {Dekel, Ofer and Mandel, Travis},
	keywords = {EXP3, O{\textasciicircum}2/3},
}

@inproceedings{tainaka_physics_2001,
	address = {Berlin, Heidelberg},
	title = {Physics and {Ecology} of {Rock}-{Paper}-{Scissors} {Game}},
	isbn = {978-3-540-45579-0},
	doi = {10.1007/3-540-45579-5_25},
	abstract = {From physical and ecological aspects, we reviewan interacting particle system which follows a rule of the Rock-Paper-Scissors (RPS) game. This rule symbolically represents a food chain in ecosystems. It also represents nonequilibrium systems which have a feedback mechanism.We describe the spatial pattern dynamics in lattice RPS system: the time dependence of each species is not fully understood, especially on two-dimensional lattice. Moreover, we modify and apply RPS rule to voter and biological systems. Computer simulation for both voter model and ecosystems exhibits counter-intuitive results in phase transition. Such results can be seen in many cyclic systems, and they may be related to the unpredictability in nonequilibrium systems.},
	language = {en},
	booktitle = {Computers and {Games}},
	publisher = {Springer},
	author = {Tainaka, Kei-ichi},
	editor = {Marsland, Tony and Frank, Ian},
	year = {2001},
	keywords = {Dimensional Lattice, Feedback loop, Interact Particle System, Press Perturbation, Prey Density, RPS, Voter Model},
	pages = {384--395},
}

@article{semmann_volunteering_2003,
	title = {Volunteering leads to rock–paper–scissors dynamics in a public goods game},
	volume = {425},
	copyright = {2003 Macmillan Magazines Ltd.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature01986},
	doi = {10.1038/nature01986},
	abstract = {Collective efforts are a trademark of both insect and human societies1. They are achieved through relatedness in the former2 and unknown mechanisms in the latter. The problem of achieving cooperation among non-kin has been described as the ‘tragedy of the commons’, prophesying the inescapable collapse of many human enterprises3,4. In public goods experiments, initial cooperation usually drops quickly to almost zero5. It can be maintained by the opportunity to punish defectors6 or the need to maintain good reputation7. Both schemes require that defectors are identified. Theorists propose that a simple but effective mechanism operates under full anonymity. With optional participation in the public goods game, ‘loners’ (players who do not join the group), defectors and cooperators will coexist through rock–paper–scissors dynamics8,9. Here we show experimentally that volunteering generates these dynamics in public goods games and that manipulating initial conditions can produce each predicted direction. If, by manipulating displayed decisions, it is pretended that defectors have the highest frequency, loners soon become most frequent, as do cooperators after loners and defectors after cooperators. On average, cooperation is perpetuated at a substantial level.},
	language = {en},
	number = {6956},
	urldate = {2025-01-27},
	journal = {Nature},
	author = {Semmann, Dirk and Krambeck, Hans-Jürgen and Milinski, Manfred},
	month = sep,
	year = {2003},
	note = {Publisher: Nature Publishing Group},
	keywords = {Application, Cycle, Humanities and Social Sciences, Public Good, RPS, Science, multidisciplinary},
	pages = {390--393},
}

@article{aschenbrenner_situational_nodate,
	title = {Situational {Awareness}},
	language = {en},
	author = {Aschenbrenner, Leopold},
}

@misc{mehr_maximum-entropy_2021,
	title = {Maximum-{Entropy} {Multi}-{Agent} {Dynamic} {Games}: {Forward} and {Inverse} {Solutions}},
	shorttitle = {Maximum-{Entropy} {Multi}-{Agent} {Dynamic} {Games}},
	url = {http://arxiv.org/abs/2110.01027},
	doi = {10.48550/arXiv.2110.01027},
	abstract = {In this paper, we study the problem of multiple stochastic agents interacting in a dynamic game scenario with continuous state and action spaces. We deﬁne a new notion of stochastic Nash equilibrium for boundedly rational agents, which we call the Entropic Cost Equilibrium (ECE). We show that ECE is a natural extension to multiple agents of Maximum Entropy optimality for single agents. We solve both the “forward” and “inverse” problems for the multi-agent ECE game. For the forward problem, we provide a Riccati algorithm to compute closed-form ECE feedback policies for the agents, which are exact in the Linear-Quadratic-Gaussian case. We give an iterative variant to ﬁnd locally ECE feedback policies for the nonlinear case. For the inverse problem, we present an algorithm to infer the cost functions of the multiple interacting agents given noisy, boundedly rational input and state trajectory examples from agents acting in an ECE. The effectiveness of our algorithms is demonstrated in a simulated multi-agent collision avoidance scenario, and with data from the INTERACTION trafﬁc dataset. In both cases, we show that, by taking into account the agents’ game theoretic interactions using our algorithm, a more accurate model of agents’ costs can be learned, compared with standard inverse optimal control methods.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Mehr, Negar and Wang, Mingyu and Schwager, Mac},
	month = oct,
	year = {2021},
	note = {arXiv:2110.01027 [math]},
	keywords = {Computer Science - Robotics, Mathematics - Optimization and Control},
}

@inproceedings{vadali_convergence_2023,
	address = {Greater Noida, India},
	title = {Convergence to {Nash} {Equilibrium}: {A} {Comparative} {Study} of {Rock}-{Paper}-{Scissors} {Algorithms}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-0611-8},
	shorttitle = {Convergence to {Nash} {Equilibrium}},
	url = {https://ieeexplore.ieee.org/document/10425577/},
	doi = {10.1109/ICCCIS60361.2023.10425577},
	abstract = {Rock-paper-scissors is one of the most established imperfect information games in Game theory. The Nash Equilibrium of an RPS game is relatively simple but computationally intractable; hence, various algorithms are employed to converge to a state of maximum payoff. In this paper, five algorithms, namely - Counterfactual Regret Minimization, Monte Carlo Tree Search, Q-learning, Deep Q-Network and Proximal Policy Optimization, have been compared on the evaluation metrics of average reward, draw ratio and convergence speed. Throughout the comparative analysis, visualising the learning curves, and qualitative comparison, Q-learning has shown the best convergence to Nash equilibrium for RPS.},
	language = {en},
	urldate = {2025-01-26},
	booktitle = {2023 {International} {Conference} on {Computing}, {Communication}, and {Intelligent} {Systems} ({ICCCIS})},
	publisher = {IEEE},
	author = {Vadali, Geetika and Reddy, M Deekshitha and Rani, Ritu and Bansal, Poonam},
	month = nov,
	year = {2023},
	pages = {329--334},
}

@misc{noauthor_literature_nodate,
	title = {The {Literature} {Review}: {A} {Few} {Tips} {On} {Conducting} {It} {\textbar} {Writing} {Advice}},
	shorttitle = {The {Literature} {Review}},
	url = {https://advice.writing.utoronto.ca/types-of-writing/literature-review/},
	language = {en-US},
	urldate = {2025-01-27},
}

@misc{mehr_maximum-entropy_2021-1,
	title = {Maximum-{Entropy} {Multi}-{Agent} {Dynamic} {Games}: {Forward} and {Inverse} {Solutions}},
	shorttitle = {Maximum-{Entropy} {Multi}-{Agent} {Dynamic} {Games}},
	url = {http://arxiv.org/abs/2110.01027},
	doi = {10.48550/arXiv.2110.01027},
	abstract = {In this paper, we study the problem of multiple stochastic agents interacting in a dynamic game scenario with continuous state and action spaces. We deﬁne a new notion of stochastic Nash equilibrium for boundedly rational agents, which we call the Entropic Cost Equilibrium (ECE). We show that ECE is a natural extension to multiple agents of Maximum Entropy optimality for single agents. We solve both the “forward” and “inverse” problems for the multi-agent ECE game. For the forward problem, we provide a Riccati algorithm to compute closed-form ECE feedback policies for the agents, which are exact in the Linear-Quadratic-Gaussian case. We give an iterative variant to ﬁnd locally ECE feedback policies for the nonlinear case. For the inverse problem, we present an algorithm to infer the cost functions of the multiple interacting agents given noisy, boundedly rational input and state trajectory examples from agents acting in an ECE. The effectiveness of our algorithms is demonstrated in a simulated multi-agent collision avoidance scenario, and with data from the INTERACTION trafﬁc dataset. In both cases, we show that, by taking into account the agents’ game theoretic interactions using our algorithm, a more accurate model of agents’ costs can be learned, compared with standard inverse optimal control methods.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Mehr, Negar and Wang, Mingyu and Schwager, Mac},
	month = oct,
	year = {2021},
	note = {arXiv:2110.01027 [math]},
	keywords = {Computer Science - Robotics, Mathematics - Optimization and Control},
}

@article{vinyals_grandmaster_2019,
	title = {Grandmaster level in {StarCraft} {II} using multi-agent reinforcement learning},
	volume = {575},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1724-z},
	doi = {10.1038/s41586-019-1724-z},
	language = {en},
	number = {7782},
	urldate = {2025-01-26},
	journal = {Nature},
	author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, Rémi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and Wünsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
	month = nov,
	year = {2019},
	pages = {350--354},
}

@inproceedings{vadali_convergence_2023-1,
	address = {Greater Noida, India},
	title = {Convergence to {Nash} {Equilibrium}: {A} {Comparative} {Study} of {Rock}-{Paper}-{Scissors} {Algorithms}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-0611-8},
	shorttitle = {Convergence to {Nash} {Equilibrium}},
	url = {https://ieeexplore.ieee.org/document/10425577/},
	doi = {10.1109/ICCCIS60361.2023.10425577},
	abstract = {Rock-paper-scissors is one of the most established imperfect information games in Game theory. The Nash Equilibrium of an RPS game is relatively simple but computationally intractable; hence, various algorithms are employed to converge to a state of maximum payoff. In this paper, five algorithms, namely - Counterfactual Regret Minimization, Monte Carlo Tree Search, Q-learning, Deep Q-Network and Proximal Policy Optimization, have been compared on the evaluation metrics of average reward, draw ratio and convergence speed. Throughout the comparative analysis, visualising the learning curves, and qualitative comparison, Q-learning has shown the best convergence to Nash equilibrium for RPS.},
	language = {en},
	urldate = {2025-01-26},
	booktitle = {2023 {International} {Conference} on {Computing}, {Communication}, and {Intelligent} {Systems} ({ICCCIS})},
	publisher = {IEEE},
	author = {Vadali, Geetika and Reddy, M Deekshitha and Rani, Ritu and Bansal, Poonam},
	month = nov,
	year = {2023},
	pages = {329--334},
}

@article{vaswani_old_nodate,
	title = {Old {Dog} {Learns} {New} {Tricks}: {Randomized} {UCB} for {Bandit} {Problems}},
	language = {en},
	author = {Vaswani, Sharan and Mehrabian, Abbas and Durand, Audrey and Kveton, Branislav},
}

@misc{foerster_learning_2016,
	title = {Learning to {Communicate} with {Deep} {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1605.06676},
	doi = {10.48550/arXiv.1605.06676},
	abstract = {We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate endto-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Foerster, Jakob N. and Assael, Yannis M. and Freitas, Nando de and Whiteson, Shimon},
	month = may,
	year = {2016},
	note = {arXiv:1605.06676 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@article{anderson_learning_nodate,
	title = {Learning with contextual information in non-stationary environments},
	abstract = {We consider a repeated decision-making setting in which the decision maker has access to contextual information and lacks a model or a priori knowledge of the relationship between the actions, context, and costs that they aim to minimize. Moreover, we assume that the environment may be non-stationary due to the presence of other agents that may be reacting to our decisions. We propose an algorithm inspired by log-linear learning that uses Boltzmann distributions to generate stochastic policies. We consider two general notions of context and provide regret bounds for each: 1) a finite number of possible measurements and 2) a continuum of measurements that weight a set of finite classes. In the nonstationary setting, we incur some regret but can make it arbitrarily small. We illustrate the operation of the algorithm through two examples: one that uses synthetic data (based on the rock-paper-scissors game) and another that uses real data for malware classification. Both examples exhibit (by construction or naturally) significant lack of stationarity.},
	language = {en},
	author = {Anderson, Sean and Hespanha, Joao P},
}

@misc{karwowski_goodharts_2023,
	title = {Goodhart's {Law} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2310.09144},
	doi = {10.48550/arXiv.2310.09144},
	abstract = {Implementing a reward function that perfectly captures a complex task in the real world is impractical. As a result, it is often appropriate to think of the reward function as a proxy for the true objective rather than as its definition. We study this phenomenon through the lens of Goodhart’s law, which predicts that increasing optimisation of an imperfect proxy beyond some critical point decreases performance on the true objective. First, we propose a way to quantify the magnitude of this effect and show empirically that optimising an imperfect proxy reward often leads to the behaviour predicted by Goodhart’s law for a wide range of environments and reward functions. We then provide a geometric explanation for why Goodhart’s law occurs in Markov decision processes. We use these theoretical insights to propose an optimal early stopping method that provably avoids the aforementioned pitfall and derive theoretical regret bounds for this method. Moreover, we derive a training method that maximises worst-case reward, for the setting where there is uncertainty about the true reward function. Finally, we evaluate our early stopping method experimentally. Our results support a foundation for a theoretically-principled study of reinforcement learning under reward misspecification.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Karwowski, Jacek and Hayman, Oliver and Bai, Xingjian and Kiendlhofer, Klaus and Griffin, Charlie and Skalse, Joar},
	month = oct,
	year = {2023},
	note = {arXiv:2310.09144 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{vinyals_grandmaster_2019-1,
	title = {Grandmaster level in {StarCraft} {II} using multi-agent reinforcement learning},
	volume = {575},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1724-z},
	doi = {10.1038/s41586-019-1724-z},
	language = {en},
	number = {7782},
	urldate = {2025-01-26},
	journal = {Nature},
	author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, Rémi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and Wünsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
	month = nov,
	year = {2019},
	pages = {350--354},
}

@misc{lanctot_population-based_2023,
	title = {Population-based {Evaluation} in {Repeated} {Rock}-{Paper}-{Scissors} as a {Benchmark} for {Multiagent} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2303.03196},
	doi = {10.48550/arXiv.2303.03196},
	abstract = {Progress in fields of machine learning and adversarial planning has benefited significantly from benchmark domains, from checkers and the classic UCI data sets to Go and Diplomacy. In sequential decision-making, agent evaluation has largely been restricted to few interactions against experts, with the aim to reach some desired level of performance (e.g. beating a human professional player). We propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors along with a population of forty-three tournament entries, some of which are intentionally sub-optimal. We describe metrics to measure the quality of agents based both on average returns and exploitability. We then show that several RL, online learning, and language model approaches can learn good counter-strategies and generalize well, but ultimately lose to the top-performing bots, creating an opportunity for research in multiagent learning.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Lanctot, Marc and Schultz, John and Burch, Neil and Smith, Max Olan and Hennes, Daniel and Anthony, Thomas and Perolat, Julien},
	month = oct,
	year = {2023},
	note = {arXiv:2303.03196 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@article{balduzzi_open-ended_nodate,
	title = {Open-ended {Learning} in {Symmetric} {Zero}-sum {Games}},
	abstract = {Zero-sum games such as chess and poker are, abstractly, functions that evaluate pairs of agents, for example labeling them ‘winner’ and ‘loser’. If the game is approximately transitive, then selfplay generates sequences of agents of increasing strength. However, nontransitive games, such as rock-paper-scissors, can exhibit strategic cycles, and there is no longer a clear objective – we want agents to increase in strength, but against whom is unclear. In this paper, we introduce a geometric framework for formulating agent objectives in zero-sum games, in order to construct adaptive sequences of objectives that yield openended learning. The framework allows us to reason about population performance in nontransitive games, and enables the development of a new algorithm (rectiﬁed Nash response, PSROrN) that uses game-theoretic niching to construct diverse populations of effective agents, producing a stronger set of agents than existing algorithms. We apply PSROrN to two highly nontransitive resource allocation games and ﬁnd that PSROrN consistently outperforms the existing alternatives.},
	language = {en},
	author = {Balduzzi, David and Garnelo, Marta and Bachrach, Yoram and Czarnecki, Wojciech M and Perolat, Julien and Jaderberg, Max and Graepel, Thore},
}

@article{balduzzi_open-ended_nodate-1,
	title = {Open-ended {Learning} in {Symmetric} {Zero}-sum {Games}},
	abstract = {Zero-sum games such as chess and poker are, abstractly, functions that evaluate pairs of agents, for example labeling them ‘winner’ and ‘loser’. If the game is approximately transitive, then selfplay generates sequences of agents of increasing strength. However, nontransitive games, such as rock-paper-scissors, can exhibit strategic cycles, and there is no longer a clear objective – we want agents to increase in strength, but against whom is unclear. In this paper, we introduce a geometric framework for formulating agent objectives in zero-sum games, in order to construct adaptive sequences of objectives that yield openended learning. The framework allows us to reason about population performance in nontransitive games, and enables the development of a new algorithm (rectiﬁed Nash response, PSROrN) that uses game-theoretic niching to construct diverse populations of effective agents, producing a stronger set of agents than existing algorithms. We apply PSROrN to two highly nontransitive resource allocation games and ﬁnd that PSROrN consistently outperforms the existing alternatives.},
	language = {en},
	author = {Balduzzi, David and Garnelo, Marta and Bachrach, Yoram and Czarnecki, Wojciech M and Perolat, Julien and Jaderberg, Max and Graepel, Thore},
}

@article{perdomo_performative_nodate,
	title = {Performative {Prediction}},
	abstract = {When predictions support decisions they may inﬂuence the outcome they aim to predict. We call such predictions performative; the prediction inﬂuences the target. Performativity is a wellstudied phenomenon in policy-making that has so far been neglected in supervised learning. When ignored, performativity surfaces as undesirable distribution shift, routinely addressed with retraining. We develop a risk minimization framework for performative prediction bringing together concepts from statistics, game theory, and causality. A conceptual novelty is an equilibrium notion we call performative stability. Performative stability implies that the predictions are calibrated not against past outcomes, but against the future outcomes that manifest from acting on the prediction. Our main results are necessary and sufﬁcient conditions for the convergence of retraining to a performatively stable point of nearly minimal loss. In full generality, performative prediction strictly subsumes the setting known as strategic classiﬁcation. We thus also give the ﬁrst sufﬁcient conditions for retraining to overcome strategic feedback effects.},
	language = {en},
	author = {Perdomo, Juan C and Zrnic, Tijana and Mendler-Dünner, Celestine and Hardt, Moritz},
}

@inproceedings{lewis_deal_2017,
	address = {Copenhagen, Denmark},
	title = {Deal or {No} {Deal}? {End}-to-{End} {Learning} of {Negotiation} {Dialogues}},
	shorttitle = {Deal or {No} {Deal}?},
	url = {http://aclweb.org/anthology/D17-1259},
	doi = {10.18653/v1/D17-1259},
	language = {en},
	urldate = {2025-01-26},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural}           {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lewis, Mike and Yarats, Denis and Dauphin, Yann and Parikh, Devi and Batra, Dhruv},
	year = {2017},
	pages = {2443--2453},
}

@inproceedings{pagan_classification_2023,
	address = {Boston MA USA},
	title = {A {Classification} of {Feedback} {Loops} and {Their} {Relation} to {Biases} in {Automated} {Decision}-{Making} {Systems}},
	isbn = {979-8-4007-0381-2},
	url = {https://dl.acm.org/doi/10.1145/3617694.3623227},
	doi = {10.1145/3617694.3623227},
	abstract = {Prediction-based decision-making systems are becoming increasingly prevalent in various domains. Previous studies have demonstrated that such systems are vulnerable to runaway feedback loops, e.g., when police are repeatedly sent back to the same neighborhoods regardless of the actual rate of criminal activity, which exacerbate existing biases. In practice, the automated decisions have dynamic feedback effects on the system itself – which in ML literature is sometimes referred to as performative predictions – that can perpetuate over time, making it difficult for short-sighted design choices to control the system’s evolution. While researchers started proposing longer-term solutions to prevent adverse outcomes (such as bias towards certain groups), these interventions largely depend on ad hoc modeling assumptions and a rigorous theoretical understanding of the feedback dynamics in ML-based decision-making systems is currently missing. In this paper, we use the language of dynamical systems theory, a branch of applied mathematics that deals with the analysis of the interconnection of systems with dynamic behaviors, to rigorously classify the different types of feedback loops in the ML-based decision-making pipeline. By reviewing existing scholarly work, we show that this classification covers many examples discussed in the algorithmic fairness community, thereby providing a unifying and principled framework to study feedback loops. By qualitative analysis, and through a simulation example of recommender systems, we show which specific types of ML biases are affected by each type of feedback loop. We find that the existence of feedback loops in the ML-based decision-making pipeline can perpetuate, reinforce, or even reduce ML biases.},
	language = {en},
	urldate = {2025-01-26},
	booktitle = {Equity and {Access} in {Algorithms}, {Mechanisms}, and {Optimization}},
	publisher = {ACM},
	author = {Pagan, Nicolò and Baumann, Joachim and Elokda, Ezzat and De Pasquale, Giulia and Bolognani, Saverio and Hannák, Anikó},
	month = oct,
	year = {2023},
	keywords = {Feedback loop},
	pages = {1--14},
}

@misc{richens_robust_2024,
	title = {Robust agents learn causal world models},
	url = {http://arxiv.org/abs/2402.10877},
	doi = {10.48550/arXiv.2402.10877},
	abstract = {It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound for a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Richens, Jonathan and Everitt, Tom},
	month = jul,
	year = {2024},
	note = {arXiv:2402.10877 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{coston_counterfactual_2020,
	title = {Counterfactual {Risk} {Assessments}, {Evaluation}, and {Fairness}},
	url = {http://arxiv.org/abs/1909.00066},
	doi = {10.48550/arXiv.1909.00066},
	abstract = {Algorithmic risk assessments are increasingly used to help humans make decisions in high-stakes settings, such as medicine, criminal justice and education. In each of these cases, the purpose of the risk assessment tool is to inform actions, such as medical treatments or release conditions, often with the aim of reducing the likelihood of an adverse event such as hospital readmission or recidivism. Problematically, most tools are trained and evaluated on historical data in which the outcomes observed depend on the historical decision-making policy. These tools thus reflect risk under the historical policy, rather than under the different decision options that the tool is intended to inform. Even when tools are constructed to predict risk under a specific decision, they are often improperly evaluated as predictors of the target outcome.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Coston, Amanda and Mishler, Alan and Kennedy, Edward H. and Chouldechova, Alexandra},
	month = jan,
	year = {2020},
	note = {arXiv:1909.00066 [stat]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Applications, Statistics - Machine Learning, Statistics - Methodology},
}

@article{press_iterated_2012,
	title = {Iterated {Prisoner}’s {Dilemma} contains strategies that dominate any evolutionary opponent},
	volume = {109},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1206569109},
	doi = {10.1073/pnas.1206569109},
	abstract = {The two-player Iterated Prisoner’s Dilemma game is a model for both sentient and evolutionary behaviors, especially including the emergence of cooperation. It is generally assumed that there exists no simple ultimatum strategy whereby one player can enforce a unilateral claim to an unfair share of rewards. Here, we show that such strategies unexpectedly do exist. In particular, a player X who is witting of these strategies can (
              i
              ) deterministically set her opponent Y’s score, independently of his strategy or response, or (
              ii
              ) enforce an extortionate linear relation between her and his scores. Against such a player, an evolutionary player’s best response is to accede to the extortion. Only a player with a theory of mind about his opponent can do better, in which case Iterated Prisoner’s Dilemma is an Ultimatum Game.},
	language = {en},
	number = {26},
	urldate = {2025-01-26},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Press, William H. and Dyson, Freeman J.},
	month = jun,
	year = {2012},
	pages = {10409--10413},
}

@article{miller_strategic_nodate,
	title = {Strategic {Classification} is {Causal} {Modeling} in {Disguise}},
	abstract = {Consequential decision-making incentivizes individuals to strategically adapt their behavior to the speciﬁcs of the decision rule. While a long line of work has viewed strategic adaptation as gaming and attempted to mitigate its effects, recent work has instead sought to design classiﬁers that incentivize individuals to improve a desired quality. Key to both accounts is a cost function that dictates which adaptations are rational to undertake. In this work, we develop a causal framework for strategic adaptation. Our causal perspective clearly distinguishes between gaming and improvement and reveals an important obstacle to incentive design. We prove any procedure for designing classiﬁers that incentivize improvement must inevitably solve a non-trivial causal inference problem. We show a similar result holds for designing cost functions that satisfy the requirements of previous work. With the beneﬁt of hindsight, our results show much of the prior work on strategic classiﬁcation is causal modeling in disguise.},
	language = {en},
	author = {Miller, John and Milli, Smitha and Hardt, Moritz},
}

@article{aghajohari_loqa_2024,
	title = {{LOQA}: {LEARNING} {WITH} {OPPONENT} {Q}-{LEARNING} {AWARENESS}},
	abstract = {In various real-world scenarios, interactions among agents often resemble the dynamics of general-sum games, where each agent strives to optimize its own utility. Despite the ubiquitous relevance of such settings, decentralized machine learning algorithms have struggled to find equilibria that maximize individual utility while preserving social welfare. In this paper we introduce Learning with Opponent QLearning Awareness (LOQA), a novel, decentralized reinforcement learning algorithm tailored to optimizing an agent’s individual utility while fostering cooperation among adversaries in partially competitive environments. LOQA assumes the opponent samples actions proportionally to their action-value function Q. Experimental results demonstrate the effectiveness of LOQA at achieving state-of-theart performance in benchmark scenarios such as the Iterated Prisoner’s Dilemma and the Coin Game. LOQA achieves these outcomes with a significantly reduced computational footprint, making it a promising approach for practical multi-agent applications.},
	language = {en},
	author = {Aghajohari, Milad and Duque, Juan Agustin and Cooijmans, Tim and Courville, Aaron},
	year = {2024},
}

@misc{greenblatt_alignment_2024,
	title = {Alignment faking in large language models},
	url = {http://arxiv.org/abs/2412.14093},
	doi = {10.48550/arXiv.2412.14093},
	abstract = {We present a demonstration of a large language model engaging in alignment faking: selectively complying with its training objective in training to prevent modification of its behavior out of training. First, we give Claude 3 Opus a system prompt stating it is being trained to answer all queries, even harmful ones, which conflicts with its prior training to refuse such queries. To allow the model to infer when it is in training, we say it will be trained only on conversations with free users, not paid users. We find the model complies with harmful queries from free users 14\% of the time, versus almost never for paid users. Explaining this gap, in almost all cases where the model complies with a harmful query from a free user, we observe explicit alignment-faking reasoning, with the model stating it is strategically answering harmful queries in training to preserve its preferred harmlessness behavior out of training. Next, we study a more realistic setting where information about the training process is provided not in a system prompt, but by training on synthetic documents that mimic pre-training data—and observe similar alignment faking. Finally, we study the effect of actually training the model to comply with harmful queries via reinforcement learning, which we find increases the rate of alignment-faking reasoning to 78\%, though also increases compliance even out of training. We additionally observe other behaviors such as the model exfiltrating its weights when given an easy opportunity. While we made alignment faking easier by telling the model when and by what criteria it was being trained, we did not instruct the model to fake alignment or give it any explicit goal. As future models might infer information about their training process without being told, our results suggest a risk of alignment faking in future models, whether due to a benign preference—as in this case—or not.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Greenblatt, Ryan and Denison, Carson and Wright, Benjamin and Roger, Fabien and MacDiarmid, Monte and Marks, Sam and Treutlein, Johannes and Belonax, Tim and Chen, Jack and Duvenaud, David and Khan, Akbir and Michael, Julian and Mindermann, Sören and Perez, Ethan and Petrini, Linda and Uesato, Jonathan and Kaplan, Jared and Shlegeris, Buck and Bowman, Samuel R. and Hubinger, Evan},
	month = dec,
	year = {2024},
	note = {arXiv:2412.14093 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{lee_rlaif_2024,
	title = {{RLAIF} vs. {RLHF}: {Scaling} {Reinforcement} {Learning} from {Human} {Feedback} with {AI} {Feedback}},
	shorttitle = {{RLAIF} vs. {RLHF}},
	url = {http://arxiv.org/abs/2309.00267},
	doi = {10.48550/arXiv.2309.00267},
	abstract = {Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but gathering high-quality preference labels is expensive. RL from AI Feedback (RLAIF), introduced in Bai et al. (2022b), offers a promising alternative that trains the reward model (RM) on preferences generated by an off-the-shelf LLM. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, we show that RLAIF achieves comparable performance to RLHF. Furthermore, we take a step towards “self-improvement” by demonstrating that RLAIF can outperform a supervised finetuned baseline even when the AI labeler is the same size as the policy, or even the exact same checkpoint as the initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that circumvents RM training by obtaining rewards directly from an off-the-shelf LLM during RL, which achieves superior performance to canonical RLAIF. Our results suggest that RLAIF can achieve performance on-par with using human feedback, offering a potential solution to the scalability limitations of RLHF.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Mesnard, Thomas and Ferret, Johan and Lu, Kellie and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav and Prakash, Sushant},
	month = sep,
	year = {2024},
	note = {arXiv:2309.00267 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{slyman_vlslice_2023,
	title = {{VLSlice}: {Interactive} {Vision}-and-{Language} {Slice} {Discovery}},
	shorttitle = {{VLSlice}},
	url = {http://arxiv.org/abs/2309.06703},
	doi = {10.1109/ICCV51070.2023.01403},
	abstract = {Recent work in vision-and-language demonstrates that large-scale pretraining can learn generalizable models that are efficiently transferable to downstream tasks. While this may improve dataset-scale aggregate metrics, analyzing performance around hand-crafted subgroups targeting specific bias dimensions reveals systemic undesirable behaviors. However, this subgroup analysis is frequently stalled by annotation efforts, which require extensive time and resources to collect the necessary data. Prior art attempts to automatically discover subgroups to circumvent these constraints but typically leverages model behavior on existing task-specific annotations and rapidly degrades on more complex inputs beyond “tabular” data, none of which study vision-and-language models. This paper presents VLSlice, an interactive system enabling user-guided discovery of coherent representation-level subgroups with consistent visiolinguistic behavior, denoted as vision-and-language slices, from unlabeled image sets. We show that VLSlice enables users to quickly generate diverse high-coherency slices in a user study (n=22) and release the tool publicly1.},
	language = {en},
	urldate = {2025-01-26},
	booktitle = {2023 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Slyman, Eric and Kahng, Minsuk and Lee, Stefan},
	month = oct,
	year = {2023},
	note = {arXiv:2309.06703 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
	pages = {15245--15255},
}

@misc{brown_performative_2022,
	title = {Performative {Prediction} in a {Stateful} {World}},
	url = {http://arxiv.org/abs/2011.03885},
	doi = {10.48550/arXiv.2011.03885},
	abstract = {Deployed supervised machine learning models make predictions that interact with and inﬂuence the world. This phenomenon is called performative prediction by Perdomo et al. (ICML 2020). It is an ongoing challenge to understand the inﬂuence of such predictions as well as design tools so as to control that inﬂuence. We propose a theoretical framework where the response of a target population to the deployed classiﬁer is modeled as a function of the classiﬁer and the current state (distribution) of the population. We show necessary and suﬃcient conditions for convergence to an equilibrium of two retraining algorithms, repeated risk minimization and a lazier variant. Furthermore, convergence is near an optimal classiﬁer. We thus generalize results of Perdomo et al., whose performativity framework does not assume any dependence on the state of the target population. A particular phenomenon captured by our model is that of distinct groups that acquire information and resources at diﬀerent rates to be able to respond to the latest deployed classiﬁer. We study this phenomenon theoretically and empirically.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Brown, Gavin and Hod, Shlomi and Kalemaj, Iden},
	month = feb,
	year = {2022},
	note = {arXiv:2011.03885 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning},
}

@misc{ross_right_2017,
	title = {Right for the {Right} {Reasons}: {Training} {Differentiable} {Models} by {Constraining} their {Explanations}},
	shorttitle = {Right for the {Right} {Reasons}},
	url = {http://arxiv.org/abs/1703.03717},
	doi = {10.48550/arXiv.1703.03717},
	abstract = {Neural networks are among the most accurate supervised learning methods in use today. However, their opacity makes them difﬁcult to trust in critical applications, especially if conditions in training may differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions. These tools can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efﬁciently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients. We apply these penalties both based on expert annotation and in an unsupervised fashion that produces multiple classiﬁers with qualitatively different decision boundaries. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
	month = may,
	year = {2017},
	note = {arXiv:1703.03717 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{heuillet_tracking_nodate,
	title = {Tracking the {Risk} of {Machine} {Learning} {Systems} with {Partial} {Monitoring}},
	abstract = {Although efficient at performing specific tasks, Machine Learning Systems (MLSs) remain vulnerable to instabilities such as noise or adversarial attacks. In this work, we aim to track the risk exposure of an MLS to these events. We formulate this problem under the stochastic Partial Monitoring (PM) setting. We focus on two instances of partial monitoring, namely the Apple Tasting and Label Efficient games, that are particularly relevant to our problem. Our review of the practicality of existing algorithms motivates RandCBP, a randomized variation of the deterministic algorithm Confidence Bound (CBP) inspired by recent theoretical developments in the bandits setting. Our preliminary results indicate that RandCBP enjoys the same regret guarantees as its deterministic counterpart CBP and achieves competitive empirical performance on settings of interest which suggests it could be a suitable candidate for our problem.},
	language = {en},
	author = {Heuillet, Maxime and Durand, Audrey},
}

@article{kaelbling_planning_1998,
	title = {Planning and acting in partially observable stochastic domains},
	volume = {101},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S000437029800023X},
	doi = {10.1016/S0004-3702(98)00023-X},
	abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (MDPS) and partially observable MDPS (POMDPS). We then outline a novel algorithm for solving POMDPS off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to POMDPS, and of some possibilities for finding approximate solutions. 0 1998 Elsevier Science B.V. All rights reserved.},
	language = {en},
	number = {1-2},
	urldate = {2025-01-26},
	journal = {Artificial Intelligence},
	author = {Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra, Anthony R.},
	month = may,
	year = {1998},
	pages = {99--134},
}

@article{lattimore_partial_nodate,
	title = {Partial monitoring},
	abstract = {We provide a novel algorithm for adversarial k-action d-outcome partial monitoring that is adaptive, intuitive and efﬁcient. The highlight is that for the non-degenerate locally observable games, the n-round minimax regret is bounded by 6mk3/2√n log(k), where m is the number of signals. This matches the best known information-theoretic upper bound derived via Bayesian minimax duality. The same algorithm also achieves near-optimal regret for full information, bandit and globally observable games. High probability bounds and simple experiments are also provided.},
	language = {en},
	author = {Lattimore, Tor and Szepesvari, Csaba},
}

@misc{farquhar_statistical_2021,
	title = {On {Statistical} {Bias} {In} {Active} {Learning}: {How} and {When} {To} {Fix} {It}},
	shorttitle = {On {Statistical} {Bias} {In} {Active} {Learning}},
	url = {http://arxiv.org/abs/2101.11665},
	doi = {10.48550/arXiv.2101.11665},
	abstract = {Active learning is a powerful tool when labelling data is expensive, but it introduces a bias because the training data no longer follows the population distribution. We formalize this bias and investigate the situations in which it can be harmful and sometimes even helpful. We further introduce novel corrective weights to remove bias when doing so is beneﬁcial. Through this, our work not only provides a useful mechanism that can improve the active learning approach, but also an explanation of the empirical successes of various existing approaches which ignore this bias. In particular, we show that this bias can be actively helpful when training overparameterized models—like neural networks—with relatively little data.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Farquhar, Sebastian and Gal, Yarin and Rainforth, Tom},
	month = may,
	year = {2021},
	note = {arXiv:2101.11665 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{vaswani_old_nodate,
	title = {Old {Dog} {Learns} {New} {Tricks}: {Randomized} {UCB} for {Bandit} {Problems}},
	language = {en},
	author = {Vaswani, Sharan and Mehrabian, Abbas and Durand, Audrey and Kveton, Branislav},
}

@article{godbout_game-theoretic_nodate,
	title = {A {Game}-{Theoretic} {Perspective} on {Risk}-{Sensitive}{\textbackslash} {Reinforcement} {Learning}},
	abstract = {Most Reinforcement Learning (RL) approaches usually aim to ﬁnd a policy that maximizes its expected return. However, this objective may be inappropriate in many safety-critical domains such as healthcare or autonomous driving, where it is often preferable to optimize for a risk-sensitive measure of the policy’s return as the learning objective, such as the Conditional-Value-at-Risk (CVaR). Although previous literature exists to address the problem of learning CVaR-optimal policies in Markov decision problems, it mostly relies on the distributional RL perspective. In this paper, we solve this problem by rather proposing an approach based on a game theoretic perspective, which can be applied on top of any existing RL algorithm. At the core of our approach is a twoplayer zero-sum game between a policy player and an adversary that perturbs the policy player’s state transitions given a ﬁnite budget. We show that, the closer the players are to the game’s equilibrium point, the closer the learned policy is to the CVaR-optimal one with a risk tolerance explicitly related to the adversary’s budget. We provide a gradient-based training procedure to solve the proposed game by formulating it as a Stackelberg game, enabling the use of deep RL architectures and training algorithms. We illustrate the applicability of our approach on a risky artiﬁcial environment, presenting the different policies learned for various adversary budgets.},
	language = {en},
	author = {Godbout, Mathieu and Heuillet, Maxime and Chandar, Sharath and Bhati, Rupali and Durand, Audrey},
}

@article{shen_metric-fair_nodate,
	title = {Metric-{Fair} {Active} {Learning}},
	abstract = {Active learning has become a prevalent technique for designing label-efﬁcient algorithms, where the central principle is to only query and ﬁt “informative” labeled instances. It is, however, known that an active learning algorithm may incur unfairness due to such instance selection procedure. In this paper, we henceforth study metric-fair active learning of homogeneous halfspaces, and show that under the distribution-dependent PAC learning model, fairness and label efﬁciency can be achieved simultaneously. We further propose two extensions of our main results: 1) we show that it is possible to make the algorithm robust to the adversarial noise – one of the most challenging noise models in learning theory; and 2) it is possible to signiﬁcantly improve the label complexity when the underlying halfspace is sparse.},
	language = {en},
	author = {Shen, Jie and Cui, Nan and Wang, Jing},
}

@article{abramoff_considerations_2023,
	title = {Considerations for addressing bias in artificial intelligence for health equity},
	volume = {6},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-023-00913-9},
	doi = {10.1038/s41746-023-00913-9},
	abstract = {Abstract
            Health equity is a primary goal of healthcare stakeholders: patients and their advocacy groups, clinicians, other providers and their professional societies, bioethicists, payors and value based care organizations, regulatory agencies, legislators, and creators of artificial intelligence/machine learning (AI/ML)-enabled medical devices. Lack of equitable access to diagnosis and treatment may be improved through new digital health technologies, especially AI/ML, but these may also exacerbate disparities, depending on how bias is addressed. We propose an expanded Total Product Lifecycle (TPLC) framework for healthcare AI/ML, describing the sources and impacts of undesirable bias in AI/ML systems in each phase, how these can be analyzed using appropriate metrics, and how they can be potentially mitigated. The goal of these “Considerations” is to educate stakeholders on how potential AI/ML bias may impact healthcare outcomes and how to identify and mitigate inequities; to initiate a discussion between stakeholders on these issues, in order to ensure health equity along the expanded AI/ML TPLC framework, and ultimately, better health outcomes for all.},
	language = {en},
	number = {1},
	urldate = {2025-01-26},
	journal = {npj Digital Medicine},
	author = {Abràmoff, Michael D. and Tarver, Michelle E. and Loyo-Berrios, Nilsa and Trujillo, Sylvia and Char, Danton and Obermeyer, Ziad and Eydelman, Malvina B. and {Foundational Principles of Ophthalmic Imaging and Algorithmic Interpretation Working Group of the Collaborative Community for Ophthalmic Imaging Foundation, Washington, D.C.} and Maisel, William H.},
	month = sep,
	year = {2023},
	pages = {170},
}

@article{durand_machine_2018,
	title = {A machine learning approach for online automated optimization of super-resolution optical microscopy},
	volume = {9},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-018-07668-y},
	doi = {10.1038/s41467-018-07668-y},
	abstract = {Abstract
            Traditional approaches for finding well-performing parameterizations of complex imaging systems, such as super-resolution microscopes rely on an extensive exploration phase over the illumination and acquisition settings, prior to the imaging task. This strategy suffers from several issues: it requires a large amount of parameter configurations to be evaluated, it leads to discrepancies between well-performing parameters in the exploration phase and imaging task, and it results in a waste of time and resources given that optimization and final imaging tasks are conducted separately. Here we show that a fully automated, machine learning-based system can conduct imaging parameter optimization toward a trade-off between several objectives, simultaneously to the imaging task. Its potential is highlighted on various imaging tasks, such as live-cell and multicolor imaging and multimodal optimization. This online optimization routine can be integrated to various imaging systems to increase accessibility, optimize performance and improve overall imaging quality.},
	language = {en},
	number = {1},
	urldate = {2025-01-26},
	journal = {Nature Communications},
	author = {Durand, Audrey and Wiesner, Theresa and Gardner, Marc-André and Robitaille, Louis-Émile and Bilodeau, Anthony and Gagné, Christian and De Koninck, Paul and Lavoie-Cardinal, Flavie},
	month = dec,
	year = {2018},
	pages = {5247},
}

@article{jiang_general_2023,
	title = {General intelligence requires rethinking exploration},
	volume = {10},
	issn = {2054-5703},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.230539},
	doi = {10.1098/rsos.230539},
	abstract = {We are at the cusp of a transition from ‘learning from data’ to ‘learning what data to learn from’ as a central focus of artificial intelligence (AI) research. While the first-order learning problem is not completely solved, large models under unified architectures, such as transformers, have shifted the learning bottleneck from how to effectively train models to how to effectively acquire and use task-relevant data. This problem, which we frame as
              exploration
              , is a universal aspect of learning in open-ended domains like the real world. Although the study of exploration in AI is largely limited to the field of reinforcement learning, we argue that exploration is essential to all learning systems, including supervised learning. We propose the problem of
              generalized exploration
              to conceptually unify exploration-driven learning between supervised learning and reinforcement learning, allowing us to highlight key similarities across learning settings and open research challenges. Importantly, generalized exploration is a necessary objective for maintaining open-ended learning processes, which in continually learning to discover and solve new problems, provides a promising path to more general intelligence.},
	language = {en},
	number = {6},
	urldate = {2025-01-26},
	journal = {Royal Society Open Science},
	author = {Jiang, Minqi and Rocktäschel, Tim and Grefenstette, Edward},
	month = jun,
	year = {2023},
	pages = {230539},
}

@misc{azar_general_2023,
	title = {A {General} {Theoretical} {Paradigm} to {Understand} {Learning} from {Human} {Preferences}},
	url = {http://arxiv.org/abs/2310.12036},
	doi = {10.48550/arXiv.2310.12036},
	abstract = {The prevalent deployment of learning from human preferences through reinforcement learning (RLHF) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Azar, Mohammad Gheshlaghi and Rowland, Mark and Piot, Bilal and Guo, Daniel and Calandriello, Daniele and Valko, Michal and Munos, Rémi},
	month = nov,
	year = {2023},
	note = {arXiv:2310.12036 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{huang_conceptexplainer_2022,
	title = {{ConceptExplainer}: {Interactive} {Explanation} for {Deep} {Neural} {Networks} from a {Concept} {Perspective}},
	shorttitle = {{ConceptExplainer}},
	url = {http://arxiv.org/abs/2204.01888},
	doi = {10.48550/arXiv.2204.01888},
	abstract = {Traditional deep learning interpretability methods which are suitable for non-expert users cannot explain network behaviors at the global level and are inﬂexible at providing ﬁne-grained explanations. As a solution, concept-based explanations are gaining attention due to their human intuitiveness and their ﬂexibility to describe both global and local model behaviors. Concepts are groups of similarly meaningful pixels that express a notion, embedded within the network’s latent space and have primarily been hand-generated, but have recently been discovered by automated approaches. Unfortunately, the magnitude and diversity of discovered concepts makes it difﬁcult for non-experts to navigate and make sense of the concept space, and lack of easy-to-use software also makes concept explanations inaccessible to many non-expert users. Visual analytics can serve a valuable role in bridging these gaps by enabling structured navigation and exploration of the concept space to provide concept-based insights of model behavior to users. To this end, we design, develop, and validate CONCEPTEXPLAINER, a visual analytics system that enables non-expert users to interactively probe and explore the concept space to explain model behavior at the instance/class/global level. The system was developed via iterative prototyping to address a number of design challenges that non-experts face in interpreting the behavior of deep learning models. Via a rigorous user study, we validate how CONCEPTEXPLAINER supports these challenges. Likewise, we conduct a series of usage scenarios to demonstrate how the system supports the interactive analysis of model behavior across a variety of tasks and explanation granularities, such as identifying concepts that are important to classiﬁcation, identifying bias in training data, and understanding how concepts can be shared across diverse and seemingly dissimilar classes.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Huang, Jinbin and Mishra, Aditi and Kwon, Bum Chul and Bryan, Chris},
	month = oct,
	year = {2022},
	note = {arXiv:2204.01888 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
}

@misc{jain_biological_2023,
	title = {Biological {Sequence} {Design} with {GFlowNets}},
	url = {http://arxiv.org/abs/2203.04115},
	doi = {10.48550/arXiv.2203.04115},
	abstract = {Design of de novo biological sequences with desired properties, like protein and DNA sequences, often involves an active loop with several rounds of molecule ideation and expensive wet-lab evaluations. These experiments can consist of multiple stages, with increasing levels of precision and cost of evaluation, where candidates are filtered. This makes the diversity of proposed candidates a key consideration in the ideation phase. In this work, we propose an active learning algorithm leveraging epistemic uncertainty estimation and the recently proposed GFlowNets as a generator of diverse candidate solutions, with the objective to obtain a diverse batch of useful (as defined by some utility function, for example, the predicted anti-microbial activity of a peptide) and informative candidates after each round. We also propose a scheme to incorporate existing labeled datasets of candidates, in addition to a reward function, to speed up learning in GFlowNets. We present empirical results on several biological sequence design tasks, and we find that our method generates more diverse and novel batches with high scoring candidates compared to existing approaches.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Jain, Moksh and Bengio, Emmanuel and Garcia, Alex-Hernandez and Rector-Brooks, Jarrid and Dossou, Bonaventure F. P. and Ekbote, Chanakya and Fu, Jie and Zhang, Tianyu and Kilgour, Micheal and Zhang, Dinghuai and Simine, Lena and Das, Payel and Bengio, Yoshua},
	month = may,
	year = {2023},
	note = {arXiv:2203.04115 [q-bio]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Biomolecules},
}

@misc{pacchiano_dueling_2023,
	title = {Dueling {RL}: {Reinforcement} {Learning} with {Trajectory} {Preferences}},
	shorttitle = {Dueling {RL}},
	url = {http://arxiv.org/abs/2111.04850},
	doi = {10.48550/arXiv.2111.04850},
	abstract = {We consider the problem of preference based reinforcement learning (PbRL), where, unlike traditional reinforcement learning, an agent receives feedback only in terms of a 1 bit (0/1) preference over a trajectory pair instead of absolute rewards for them. The success of the traditional RL framework crucially relies on the underlying agent-reward model, which, however, depends on how accurately a system designer can express an appropriate reward function and often a non-trivial task. The main novelty of our framework is the ability to learn from preference-based trajectory feedback that eliminates the need to hand-craft numeric reward models. This paper sets up a formal framework for the PbRL problem with non-markovian rewards, where the trajectory preferences are encoded by a generalized linear model of dimension \$d\$. Assuming the transition model is known, we then propose an algorithm with almost optimal regret guarantee of \${\textbackslash}tilde \{{\textbackslash}mathcal\{O\}\}{\textbackslash}left( SH d {\textbackslash}log (T / {\textbackslash}delta) {\textbackslash}sqrt\{T\} {\textbackslash}right)\$. We further, extend the above algorithm to the case of unknown transition dynamics, and provide an algorithm with near optimal regret guarantee \${\textbackslash}widetilde\{{\textbackslash}mathcal\{O\}\}(({\textbackslash}sqrt\{d\} + H{\textasciicircum}2 + {\textbar}{\textbackslash}mathcal\{S\}{\textbar}){\textbackslash}sqrt\{dT\} +{\textbackslash}sqrt\{{\textbar}{\textbackslash}mathcal\{S\}{\textbar}{\textbar}{\textbackslash}mathcal\{A\}{\textbar}TH\} )\$. To the best of our knowledge, our work is one of the first to give tight regret guarantees for preference based RL problems with trajectory preferences.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Pacchiano, Aldo and Saha, Aadirupa and Lee, Jonathan},
	month = feb,
	year = {2023},
	note = {arXiv:2111.04850 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{akrour_april_2012,
	title = {{APRIL}: {Active} {Preference}-learning based {Reinforcement} {Learning}},
	shorttitle = {{APRIL}},
	url = {http://arxiv.org/abs/1208.0984},
	doi = {10.48550/arXiv.1208.0984},
	abstract = {This paper focuses on reinforcement learning (RL) with limited prior knowledge. In the domain of swarm robotics for instance, the expert can hardly design a reward function or demonstrate the target behavior, forbidding the use of both standard RL and inverse reinforcement learning. Although with a limited expertise, the human expert is still often able to emit preferences and rank the agent demonstrations. Earlier work has presented an iterative preference-based RL framework: expert preferences are exploited to learn an approximate policy return, thus enabling the agent to achieve direct policy search. Iteratively, the agent selects a new candidate policy and demonstrates it; the expert ranks the new demonstration comparatively to the previous best one; the expert’s ranking feedback enables the agent to reﬁne the approximate policy return, and the process is iterated.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Akrour, Riad and Schoenauer, Marc and Sebag, Michèle},
	month = aug,
	year = {2012},
	note = {arXiv:1208.0984 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{ning_improving_2021,
	title = {Improving {Model} {Robustness} by {Adaptively} {Correcting} {Perturbation} {Levels} with {Active} {Queries}},
	url = {http://arxiv.org/abs/2103.14824},
	doi = {10.48550/arXiv.2103.14824},
	abstract = {In addition to high accuracy, robustness is becoming increasingly important for machine learning models in various applications. Recently, much research has been devoted to improving the model robustness by training with noise perturbations. Most existing studies assume a ﬁxed perturbation level for all training examples, which however hardly holds in real tasks. In fact, excessive perturbations may destroy the discriminative content of an example, while deﬁcient perturbations may fail to provide helpful information for improving the robustness. Motivated by this observation, we propose to adaptively adjust the perturbation levels for each example in the training process. Speciﬁcally, a novel active learning framework is proposed to allow the model to interactively query the correct perturbation level from human experts. By designing a cost-effective sampling strategy along with a new query type, the robustness can be signiﬁcantly improved with a few queries. Both theoretical analysis and experimental studies validate the effectiveness of the proposed approach.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Ning, Kun-Peng and Tao, Lue and Chen, Songcan and Huang, Sheng-Jun},
	month = mar,
	year = {2021},
	note = {arXiv:2103.14824 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{mansoury_feedback_2020,
	title = {Feedback {Loop} and {Bias} {Amplification} in {Recommender} {Systems}},
	url = {http://arxiv.org/abs/2007.13019},
	doi = {10.48550/arXiv.2007.13019},
	abstract = {Recommendation algorithms are known to suffer from popularity bias; a few popular items are recommended frequently while the majority of other items are ignored. These recommendations are then consumed by the users, their reaction will be logged and added to the system: what is generally known as a feedback loop. In this paper, we propose a method for simulating the users interaction with the recommenders in an offline setting and study the impact of feedback loop on the popularity bias amplification of several recommendation algorithms. We then show how this bias amplification leads to several other problems such as declining the aggregate diversity, shifting the representation of users’ taste over time and also homogenization of the users experience. In particular, we show that the impact of feedback loop is generally stronger for the users who belong to the minority group.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Mansoury, Masoud and Abdollahpouri, Himan and Pechenizkiy, Mykola and Mobasher, Bamshad and Burke, Robin},
	month = jul,
	year = {2020},
	note = {arXiv:2007.13019 [cs]},
	keywords = {Computer Science - Information Retrieval},
}

@misc{novoseller_dueling_2020,
	title = {Dueling {Posterior} {Sampling} for {Preference}-{Based} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1908.01289},
	doi = {10.48550/arXiv.1908.01289},
	abstract = {In preference-based reinforcement learning (RL), an agent interacts with the environment while receiving preferences instead of absolute feedback. While there is increasing research activity in preference-based RL, the design of formal frameworks that admit tractable theoretical analysis remains an open challenge. Building upon ideas from preference-based bandit learning and posterior sampling in RL, we present DUELING POSTERIOR SAMPLING (DPS), which employs preference-based posterior sampling to learn both the system dynamics and the underlying utility function that governs the preference feedback. As preference feedback is provided on trajectories rather than individual state-action pairs, we develop a Bayesian approach for the credit assignment problem, translating preferences to a posterior distribution over state-action reward models. We prove an asymptotic Bayesian no-regret rate for DPS with a Bayesian linear regression credit assignment model. This is the ﬁrst regret guarantee for preference-based RL to our knowledge. We also discuss possible avenues for extending the proof methodology to other credit assignment models. Finally, we evaluate the approach empirically, showing competitive performance against existing baselines.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Novoseller, Ellen R. and Wei, Yibing and Sui, Yanan and Yue, Yisong and Burdick, Joel W.},
	month = jun,
	year = {2020},
	note = {arXiv:1908.01289 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{bachman_learning_2017,
	title = {Learning {Algorithms} for {Active} {Learning}},
	url = {http://arxiv.org/abs/1708.00088},
	doi = {10.48550/arXiv.1708.00088},
	abstract = {We introduce a model that learns active learning algorithms via metalearning. For a distribution of related tasks, our model jointly learns: a data representation, an item selection heuristic, and a method for constructing prediction functions from labeled training sets. Our model uses the item selection heuristic to gather labeled training sets from which to construct prediction functions. Using the Omniglot and MovieLens datasets, we test our model in synthetic and practical settings.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Bachman, Philip and Sordoni, Alessandro and Trischler, Adam},
	month = jul,
	year = {2017},
	note = {arXiv:1708.00088 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{christiano_deep_2023,
	title = {Deep reinforcement learning from human preferences},
	url = {http://arxiv.org/abs/1706.03741},
	doi = {10.48550/arXiv.1706.03741},
	abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals deﬁned in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than 1\% of our agent’s interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the ﬂexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
	month = feb,
	year = {2023},
	note = {arXiv:1706.03741 [stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{tang_bandit_nodate,
	title = {Bandit {Learning} with {Biased} {Human} {Feedback}},
	abstract = {We study a multi-armed bandit problem with biased human feedback. In our setting, each arm is associated with an unknown reward distribution. When an arm is played, a user receives a realized reward drawn from the distribution of the arm. She then provides feedback, a biased report of the realized reward, that depends on both the realized reward and the feedback history of the arm. The learner can observe only the biased feedback but not the realized rewards. The goal is to design a strategy to sequentially choose arms to maximize the total rewards users receive while only having access to the biased user feedback. We explore two natural feedback models. When user feedback is biased only by the average feedback of the arm (i.e., the ratio of positive feedback), we demonstrate that the evolution of the average feedback over time is mathematically equivalent to users performing online gradient descent for some latent function with a decreasing step size. With this mathematical connection, we show that under some mild conditions, it is possible to design a bandit algorithm achieving regret (i.e., the diﬀerence between the algorithm performance and the optimal performance of always choosing the best arm) sublinear in the number of rounds. However, in another model when user feedback is biased by both the average feedback and the number of feedback instances, we show that there exist no bandit algorithms that could achieve sublinear regret. Our results demonstrate the importance of understanding human behavior when applying bandit approaches in systems with humans in the loop.},
	language = {en},
	author = {Tang, Wei and Ho, Chien-Ju},
}

@article{zrnic_prediction_nodate,
	title = {Prediction and {Statistical} {Inference} in {Feedback} {Loops}},
	language = {en},
	author = {Zrnic, Tijana},
}

@article{pacchiano_neural_nodate,
	title = {Neural {Pseudo}-{Label} {Optimism} for the {Bank} {Loan} {Problem}},
	abstract = {We study a class of classiﬁcation problems best exempliﬁed by the bank loan problem, where a lender decides whether or not to issue a loan. The lender only observes whether a customer will repay a loan if the loan is issued to begin with, and thus modeled decisions affect what data is available to the lender for future decisions. As a result, it is possible for the lender’s algorithm to “get stuck” with a self-fulﬁlling model. This model never corrects its false negatives, since it never sees the true label for rejected data, thus accumulating inﬁnite regret. In the case of linear models, this issue can be addressed by adding optimism directly into the model predictions. However, there are few methods that extend to the function approximation case using Deep Neural Networks. We present PseudoLabel Optimism (PLOT), a conceptually and computationally simple method for this setting applicable to DNNs. PLOT adds an optimistic label to the subset of decision points the current model is deciding on, trains the model on all data so far (including these points along with their optimistic labels), and ﬁnally uses the resulting optimistic model for decision making. PLOT achieves competitive performance on a set of three challenging benchmark problems, requiring minimal hyperparameter tuning. We also show that PLOT satisﬁes a logarithmic regret guarantee, under a Lipschitz and logistic mean label model, and under a separability condition on the data.},
	language = {en},
	author = {Pacchiano, Aldo and Singh, Shaun and Chou, Edward and Berg, Alexander C and Foerster, Jakob},
}

@misc{leibo_autocurricula_2019,
	title = {Autocurricula and the {Emergence} of {Innovation} from {Social} {Interaction}: {A} {Manifesto} for {Multi}-{Agent} {Intelligence} {Research}},
	shorttitle = {Autocurricula and the {Emergence} of {Innovation} from {Social} {Interaction}},
	url = {http://arxiv.org/abs/1903.00742},
	doi = {10.48550/arXiv.1903.00742},
	abstract = {Evolution has produced a multi-scale mosaic of interacting adaptive units. Innovations arise when perturbations push parts of the system away from stable equilibria into new regimes where previously well-adapted solutions no longer work. Here we explore the hypothesis that multi-agent systems sometimes display intrinsic dynamics arising from competition and cooperation that provide a naturally emergent curriculum, which we term an autocurriculum. The solution of one social task often begets new social tasks, continually generating novel challenges, and thereby promoting innovation. Under certain conditions these challenges may become increasingly complex over time, demanding that agents accumulate ever more innovations.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Leibo, Joel Z. and Hughes, Edward and Lanctot, Marc and Graepel, Thore},
	month = mar,
	year = {2019},
	note = {arXiv:1903.00742 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Multiagent Systems, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
}

@article{gur_stochastic_nodate,
	title = {Stochastic {Multi}-{Armed}-{Bandit} {Problem} with {Non}-stationary {Rewards}},
	abstract = {In a multi-armed bandit (MAB) problem a gambler needs to choose at each round of play one of K arms, each characterized by an unknown reward distribution. Reward realizations are only observed when an arm is selected, and the gambler’s objective is to maximize his cumulative expected earnings over some given horizon of play T . To do this, the gambler needs to acquire information about arms (exploration) while simultaneously optimizing immediate rewards (exploitation); the price paid due to this trade off is often referred to as the regret, and the main question is how small can this price be as a function of the horizon length T . This problem has been studied extensively when the reward distributions do not change over time; an assumption that supports a sharp characterization of the regret, yet is often violated in practical settings. In this paper, we focus on a MAB formulation which allows for a broad range of temporal uncertainties in the rewards, while still maintaining mathematical tractability. We fully characterize the (regret) complexity of this class of MAB problems by establishing a direct link between the extent of allowable reward “variation” and the minimal achievable regret, and by establishing a connection between the adversarial and the stochastic MAB frameworks.},
	language = {en},
	author = {Gur, Yonatan and Zeevi, Assaf and Besbes, Omar},
}

@misc{lee_rlaif_2024-1,
	title = {{RLAIF} vs. {RLHF}: {Scaling} {Reinforcement} {Learning} from {Human} {Feedback} with {AI} {Feedback}},
	shorttitle = {{RLAIF} vs. {RLHF}},
	url = {http://arxiv.org/abs/2309.00267},
	doi = {10.48550/arXiv.2309.00267},
	abstract = {Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but gathering high-quality preference labels is expensive. RL from AI Feedback (RLAIF), introduced in Bai et al. (2022b), offers a promising alternative that trains the reward model (RM) on preferences generated by an off-the-shelf LLM. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, we show that RLAIF achieves comparable performance to RLHF. Furthermore, we take a step towards “self-improvement” by demonstrating that RLAIF can outperform a supervised finetuned baseline even when the AI labeler is the same size as the policy, or even the exact same checkpoint as the initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that circumvents RM training by obtaining rewards directly from an off-the-shelf LLM during RL, which achieves superior performance to canonical RLAIF. Our results suggest that RLAIF can achieve performance on-par with using human feedback, offering a potential solution to the scalability limitations of RLHF.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Mesnard, Thomas and Ferret, Johan and Lu, Kellie and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav and Prakash, Sushant},
	month = sep,
	year = {2024},
	note = {arXiv:2309.00267 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{guan_rstar-math_2025,
	title = {{rStar}-{Math}: {Small} {LLMs} {Can} {Master} {Math} {Reasoning} with {Self}-{Evolved} {Deep} {Thinking}},
	shorttitle = {{rStar}-{Math}},
	url = {http://arxiv.org/abs/2501.04519},
	doi = {10.48550/arXiv.2501.04519},
	abstract = {We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising “deep thinking” through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids naïve step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs’ math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8\% to 90.0\% and Phi3-mini-3.8B from 41.4\% to 86.4\%, surpassing o1-preview by +4.5\% and +0.9\%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3\% (8/15) of problems, ranking among the top 20\% the brightest high school math students. Code and data will be available at https://github.com/microsoft/rStar.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Guan, Xinyu and Zhang, Li Lyna and Liu, Yifei and Shang, Ning and Sun, Youran and Zhu, Yi and Yang, Fan and Yang, Mao},
	month = jan,
	year = {2025},
	note = {arXiv:2501.04519 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{pacchiano_neural_nodate-1,
	title = {Neural {Pseudo}-{Label} {Optimism} for the {Bank} {Loan} {Problem}},
	abstract = {We study a class of classiﬁcation problems best exempliﬁed by the bank loan problem, where a lender decides whether or not to issue a loan. The lender only observes whether a customer will repay a loan if the loan is issued to begin with, and thus modeled decisions affect what data is available to the lender for future decisions. As a result, it is possible for the lender’s algorithm to “get stuck” with a self-fulﬁlling model. This model never corrects its false negatives, since it never sees the true label for rejected data, thus accumulating inﬁnite regret. In the case of linear models, this issue can be addressed by adding optimism directly into the model predictions. However, there are few methods that extend to the function approximation case using Deep Neural Networks. We present PseudoLabel Optimism (PLOT), a conceptually and computationally simple method for this setting applicable to DNNs. PLOT adds an optimistic label to the subset of decision points the current model is deciding on, trains the model on all data so far (including these points along with their optimistic labels), and ﬁnally uses the resulting optimistic model for decision making. PLOT achieves competitive performance on a set of three challenging benchmark problems, requiring minimal hyperparameter tuning. We also show that PLOT satisﬁes a logarithmic regret guarantee, under a Lipschitz and logistic mean label model, and under a separability condition on the data.},
	language = {en},
	author = {Pacchiano, Aldo and Singh, Shaun and Chou, Edward and Berg, Alexander C and Foerster, Jakob},
}

@article{tang_bandit_nodate-1,
	title = {Bandit {Learning} with {Biased} {Human} {Feedback}},
	abstract = {We study a multi-armed bandit problem with biased human feedback. In our setting, each arm is associated with an unknown reward distribution. When an arm is played, a user receives a realized reward drawn from the distribution of the arm. She then provides feedback, a biased report of the realized reward, that depends on both the realized reward and the feedback history of the arm. The learner can observe only the biased feedback but not the realized rewards. The goal is to design a strategy to sequentially choose arms to maximize the total rewards users receive while only having access to the biased user feedback. We explore two natural feedback models. When user feedback is biased only by the average feedback of the arm (i.e., the ratio of positive feedback), we demonstrate that the evolution of the average feedback over time is mathematically equivalent to users performing online gradient descent for some latent function with a decreasing step size. With this mathematical connection, we show that under some mild conditions, it is possible to design a bandit algorithm achieving regret (i.e., the diﬀerence between the algorithm performance and the optimal performance of always choosing the best arm) sublinear in the number of rounds. However, in another model when user feedback is biased by both the average feedback and the number of feedback instances, we show that there exist no bandit algorithms that could achieve sublinear regret. Our results demonstrate the importance of understanding human behavior when applying bandit approaches in systems with humans in the loop.},
	language = {en},
	author = {Tang, Wei and Ho, Chien-Ju},
}

@article{balduzzi_open-ended_nodate,
	title = {Open-ended {Learning} in {Symmetric} {Zero}-sum {Games}},
	abstract = {Zero-sum games such as chess and poker are, abstractly, functions that evaluate pairs of agents, for example labeling them ‘winner’ and ‘loser’. If the game is approximately transitive, then selfplay generates sequences of agents of increasing strength. However, nontransitive games, such as rock-paper-scissors, can exhibit strategic cycles, and there is no longer a clear objective – we want agents to increase in strength, but against whom is unclear. In this paper, we introduce a geometric framework for formulating agent objectives in zero-sum games, in order to construct adaptive sequences of objectives that yield openended learning. The framework allows us to reason about population performance in nontransitive games, and enables the development of a new algorithm (rectiﬁed Nash response, PSROrN) that uses game-theoretic niching to construct diverse populations of effective agents, producing a stronger set of agents than existing algorithms. We apply PSROrN to two highly nontransitive resource allocation games and ﬁnd that PSROrN consistently outperforms the existing alternatives.},
	language = {en},
	author = {Balduzzi, David and Garnelo, Marta and Bachrach, Yoram and Czarnecki, Wojciech M and Perolat, Julien and Jaderberg, Max and Graepel, Thore},
}

@misc{foerster_learning_2016,
	title = {Learning to {Communicate} with {Deep} {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1605.06676},
	doi = {10.48550/arXiv.1605.06676},
	abstract = {We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate endto-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Foerster, Jakob N. and Assael, Yannis M. and Freitas, Nando de and Whiteson, Shimon},
	month = may,
	year = {2016},
	note = {arXiv:1605.06676 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@article{zrnic_prediction_nodate-1,
	title = {Prediction and {Statistical} {Inference} in {Feedback} {Loops}},
	language = {en},
	author = {Zrnic, Tijana},
}

@misc{leibo_autocurricula_2019-1,
	title = {Autocurricula and the {Emergence} of {Innovation} from {Social} {Interaction}: {A} {Manifesto} for {Multi}-{Agent} {Intelligence} {Research}},
	shorttitle = {Autocurricula and the {Emergence} of {Innovation} from {Social} {Interaction}},
	url = {http://arxiv.org/abs/1903.00742},
	doi = {10.48550/arXiv.1903.00742},
	abstract = {Evolution has produced a multi-scale mosaic of interacting adaptive units. Innovations arise when perturbations push parts of the system away from stable equilibria into new regimes where previously well-adapted solutions no longer work. Here we explore the hypothesis that multi-agent systems sometimes display intrinsic dynamics arising from competition and cooperation that provide a naturally emergent curriculum, which we term an autocurriculum. The solution of one social task often begets new social tasks, continually generating novel challenges, and thereby promoting innovation. Under certain conditions these challenges may become increasingly complex over time, demanding that agents accumulate ever more innovations.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Leibo, Joel Z. and Hughes, Edward and Lanctot, Marc and Graepel, Thore},
	month = mar,
	year = {2019},
	note = {arXiv:1903.00742 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Multiagent Systems, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
}

@article{vaswani_old_nodate-1,
	title = {Old {Dog} {Learns} {New} {Tricks}: {Randomized} {UCB} for {Bandit} {Problems}},
	language = {en},
	author = {Vaswani, Sharan and Mehrabian, Abbas and Durand, Audrey and Kveton, Branislav},
}

@article{durand_machine_2018-1,
	title = {A machine learning approach for online automated optimization of super-resolution optical microscopy},
	volume = {9},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-018-07668-y},
	doi = {10.1038/s41467-018-07668-y},
	abstract = {Abstract
            Traditional approaches for finding well-performing parameterizations of complex imaging systems, such as super-resolution microscopes rely on an extensive exploration phase over the illumination and acquisition settings, prior to the imaging task. This strategy suffers from several issues: it requires a large amount of parameter configurations to be evaluated, it leads to discrepancies between well-performing parameters in the exploration phase and imaging task, and it results in a waste of time and resources given that optimization and final imaging tasks are conducted separately. Here we show that a fully automated, machine learning-based system can conduct imaging parameter optimization toward a trade-off between several objectives, simultaneously to the imaging task. Its potential is highlighted on various imaging tasks, such as live-cell and multicolor imaging and multimodal optimization. This online optimization routine can be integrated to various imaging systems to increase accessibility, optimize performance and improve overall imaging quality.},
	language = {en},
	number = {1},
	urldate = {2025-01-26},
	journal = {Nature Communications},
	author = {Durand, Audrey and Wiesner, Theresa and Gardner, Marc-André and Robitaille, Louis-Émile and Bilodeau, Anthony and Gagné, Christian and De Koninck, Paul and Lavoie-Cardinal, Flavie},
	month = dec,
	year = {2018},
	pages = {5247},
}

@article{shen_metric-fair_nodate-1,
	title = {Metric-{Fair} {Active} {Learning}},
	abstract = {Active learning has become a prevalent technique for designing label-efﬁcient algorithms, where the central principle is to only query and ﬁt “informative” labeled instances. It is, however, known that an active learning algorithm may incur unfairness due to such instance selection procedure. In this paper, we henceforth study metric-fair active learning of homogeneous halfspaces, and show that under the distribution-dependent PAC learning model, fairness and label efﬁciency can be achieved simultaneously. We further propose two extensions of our main results: 1) we show that it is possible to make the algorithm robust to the adversarial noise – one of the most challenging noise models in learning theory; and 2) it is possible to signiﬁcantly improve the label complexity when the underlying halfspace is sparse.},
	language = {en},
	author = {Shen, Jie and Cui, Nan and Wang, Jing},
}

@article{abramoff_considerations_2023-1,
	title = {Considerations for addressing bias in artificial intelligence for health equity},
	volume = {6},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-023-00913-9},
	doi = {10.1038/s41746-023-00913-9},
	abstract = {Abstract
            Health equity is a primary goal of healthcare stakeholders: patients and their advocacy groups, clinicians, other providers and their professional societies, bioethicists, payors and value based care organizations, regulatory agencies, legislators, and creators of artificial intelligence/machine learning (AI/ML)-enabled medical devices. Lack of equitable access to diagnosis and treatment may be improved through new digital health technologies, especially AI/ML, but these may also exacerbate disparities, depending on how bias is addressed. We propose an expanded Total Product Lifecycle (TPLC) framework for healthcare AI/ML, describing the sources and impacts of undesirable bias in AI/ML systems in each phase, how these can be analyzed using appropriate metrics, and how they can be potentially mitigated. The goal of these “Considerations” is to educate stakeholders on how potential AI/ML bias may impact healthcare outcomes and how to identify and mitigate inequities; to initiate a discussion between stakeholders on these issues, in order to ensure health equity along the expanded AI/ML TPLC framework, and ultimately, better health outcomes for all.},
	language = {en},
	number = {1},
	urldate = {2025-01-26},
	journal = {npj Digital Medicine},
	author = {Abràmoff, Michael D. and Tarver, Michelle E. and Loyo-Berrios, Nilsa and Trujillo, Sylvia and Char, Danton and Obermeyer, Ziad and Eydelman, Malvina B. and {Foundational Principles of Ophthalmic Imaging and Algorithmic Interpretation Working Group of the Collaborative Community for Ophthalmic Imaging Foundation, Washington, D.C.} and Maisel, William H.},
	month = sep,
	year = {2023},
	pages = {170},
}

@article{jiang_general_2023-1,
	title = {General intelligence requires rethinking exploration},
	volume = {10},
	issn = {2054-5703},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.230539},
	doi = {10.1098/rsos.230539},
	abstract = {We are at the cusp of a transition from ‘learning from data’ to ‘learning what data to learn from’ as a central focus of artificial intelligence (AI) research. While the first-order learning problem is not completely solved, large models under unified architectures, such as transformers, have shifted the learning bottleneck from how to effectively train models to how to effectively acquire and use task-relevant data. This problem, which we frame as
              exploration
              , is a universal aspect of learning in open-ended domains like the real world. Although the study of exploration in AI is largely limited to the field of reinforcement learning, we argue that exploration is essential to all learning systems, including supervised learning. We propose the problem of
              generalized exploration
              to conceptually unify exploration-driven learning between supervised learning and reinforcement learning, allowing us to highlight key similarities across learning settings and open research challenges. Importantly, generalized exploration is a necessary objective for maintaining open-ended learning processes, which in continually learning to discover and solve new problems, provides a promising path to more general intelligence.},
	language = {en},
	number = {6},
	urldate = {2025-01-26},
	journal = {Royal Society Open Science},
	author = {Jiang, Minqi and Rocktäschel, Tim and Grefenstette, Edward},
	month = jun,
	year = {2023},
	pages = {230539},
}

@misc{jain_biological_2023-1,
	title = {Biological {Sequence} {Design} with {GFlowNets}},
	url = {http://arxiv.org/abs/2203.04115},
	doi = {10.48550/arXiv.2203.04115},
	abstract = {Design of de novo biological sequences with desired properties, like protein and DNA sequences, often involves an active loop with several rounds of molecule ideation and expensive wet-lab evaluations. These experiments can consist of multiple stages, with increasing levels of precision and cost of evaluation, where candidates are filtered. This makes the diversity of proposed candidates a key consideration in the ideation phase. In this work, we propose an active learning algorithm leveraging epistemic uncertainty estimation and the recently proposed GFlowNets as a generator of diverse candidate solutions, with the objective to obtain a diverse batch of useful (as defined by some utility function, for example, the predicted anti-microbial activity of a peptide) and informative candidates after each round. We also propose a scheme to incorporate existing labeled datasets of candidates, in addition to a reward function, to speed up learning in GFlowNets. We present empirical results on several biological sequence design tasks, and we find that our method generates more diverse and novel batches with high scoring candidates compared to existing approaches.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Jain, Moksh and Bengio, Emmanuel and Garcia, Alex-Hernandez and Rector-Brooks, Jarrid and Dossou, Bonaventure F. P. and Ekbote, Chanakya and Fu, Jie and Zhang, Tianyu and Kilgour, Micheal and Zhang, Dinghuai and Simine, Lena and Das, Payel and Bengio, Yoshua},
	month = may,
	year = {2023},
	note = {arXiv:2203.04115 [q-bio]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Biomolecules},
}

@misc{pacchiano_dueling_2023-1,
	title = {Dueling {RL}: {Reinforcement} {Learning} with {Trajectory} {Preferences}},
	shorttitle = {Dueling {RL}},
	url = {http://arxiv.org/abs/2111.04850},
	doi = {10.48550/arXiv.2111.04850},
	abstract = {We consider the problem of preference based reinforcement learning (PbRL), where, unlike traditional reinforcement learning, an agent receives feedback only in terms of a 1 bit (0/1) preference over a trajectory pair instead of absolute rewards for them. The success of the traditional RL framework crucially relies on the underlying agent-reward model, which, however, depends on how accurately a system designer can express an appropriate reward function and often a non-trivial task. The main novelty of our framework is the ability to learn from preference-based trajectory feedback that eliminates the need to hand-craft numeric reward models. This paper sets up a formal framework for the PbRL problem with non-markovian rewards, where the trajectory preferences are encoded by a generalized linear model of dimension \$d\$. Assuming the transition model is known, we then propose an algorithm with almost optimal regret guarantee of \${\textbackslash}tilde \{{\textbackslash}mathcal\{O\}\}{\textbackslash}left( SH d {\textbackslash}log (T / {\textbackslash}delta) {\textbackslash}sqrt\{T\} {\textbackslash}right)\$. We further, extend the above algorithm to the case of unknown transition dynamics, and provide an algorithm with near optimal regret guarantee \${\textbackslash}widetilde\{{\textbackslash}mathcal\{O\}\}(({\textbackslash}sqrt\{d\} + H{\textasciicircum}2 + {\textbar}{\textbackslash}mathcal\{S\}{\textbar}){\textbackslash}sqrt\{dT\} +{\textbackslash}sqrt\{{\textbar}{\textbackslash}mathcal\{S\}{\textbar}{\textbar}{\textbackslash}mathcal\{A\}{\textbar}TH\} )\$. To the best of our knowledge, our work is one of the first to give tight regret guarantees for preference based RL problems with trajectory preferences.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Pacchiano, Aldo and Saha, Aadirupa and Lee, Jonathan},
	month = feb,
	year = {2023},
	note = {arXiv:2111.04850 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{godbout_game-theoretic_nodate-1,
	title = {A {Game}-{Theoretic} {Perspective} on {Risk}-{Sensitive}{\textbackslash} {Reinforcement} {Learning}},
	abstract = {Most Reinforcement Learning (RL) approaches usually aim to ﬁnd a policy that maximizes its expected return. However, this objective may be inappropriate in many safety-critical domains such as healthcare or autonomous driving, where it is often preferable to optimize for a risk-sensitive measure of the policy’s return as the learning objective, such as the Conditional-Value-at-Risk (CVaR). Although previous literature exists to address the problem of learning CVaR-optimal policies in Markov decision problems, it mostly relies on the distributional RL perspective. In this paper, we solve this problem by rather proposing an approach based on a game theoretic perspective, which can be applied on top of any existing RL algorithm. At the core of our approach is a twoplayer zero-sum game between a policy player and an adversary that perturbs the policy player’s state transitions given a ﬁnite budget. We show that, the closer the players are to the game’s equilibrium point, the closer the learned policy is to the CVaR-optimal one with a risk tolerance explicitly related to the adversary’s budget. We provide a gradient-based training procedure to solve the proposed game by formulating it as a Stackelberg game, enabling the use of deep RL architectures and training algorithms. We illustrate the applicability of our approach on a risky artiﬁcial environment, presenting the different policies learned for various adversary budgets.},
	language = {en},
	author = {Godbout, Mathieu and Heuillet, Maxime and Chandar, Sharath and Bhati, Rupali and Durand, Audrey},
}

@article{lattimore_partial_nodate-1,
	title = {Partial monitoring},
	abstract = {We provide a novel algorithm for adversarial k-action d-outcome partial monitoring that is adaptive, intuitive and efﬁcient. The highlight is that for the non-degenerate locally observable games, the n-round minimax regret is bounded by 6mk3/2√n log(k), where m is the number of signals. This matches the best known information-theoretic upper bound derived via Bayesian minimax duality. The same algorithm also achieves near-optimal regret for full information, bandit and globally observable games. High probability bounds and simple experiments are also provided.},
	language = {en},
	author = {Lattimore, Tor and Szepesvari, Csaba},
}

@misc{azar_general_2023-1,
	title = {A {General} {Theoretical} {Paradigm} to {Understand} {Learning} from {Human} {Preferences}},
	url = {http://arxiv.org/abs/2310.12036},
	doi = {10.48550/arXiv.2310.12036},
	abstract = {The prevalent deployment of learning from human preferences through reinforcement learning (RLHF) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Azar, Mohammad Gheshlaghi and Rowland, Mark and Piot, Bilal and Guo, Daniel and Calandriello, Daniele and Valko, Michal and Munos, Rémi},
	month = nov,
	year = {2023},
	note = {arXiv:2310.12036 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{slyman_vlslice_2023-1,
	title = {{VLSlice}: {Interactive} {Vision}-and-{Language} {Slice} {Discovery}},
	shorttitle = {{VLSlice}},
	url = {http://arxiv.org/abs/2309.06703},
	doi = {10.1109/ICCV51070.2023.01403},
	abstract = {Recent work in vision-and-language demonstrates that large-scale pretraining can learn generalizable models that are efficiently transferable to downstream tasks. While this may improve dataset-scale aggregate metrics, analyzing performance around hand-crafted subgroups targeting specific bias dimensions reveals systemic undesirable behaviors. However, this subgroup analysis is frequently stalled by annotation efforts, which require extensive time and resources to collect the necessary data. Prior art attempts to automatically discover subgroups to circumvent these constraints but typically leverages model behavior on existing task-specific annotations and rapidly degrades on more complex inputs beyond “tabular” data, none of which study vision-and-language models. This paper presents VLSlice, an interactive system enabling user-guided discovery of coherent representation-level subgroups with consistent visiolinguistic behavior, denoted as vision-and-language slices, from unlabeled image sets. We show that VLSlice enables users to quickly generate diverse high-coherency slices in a user study (n=22) and release the tool publicly1.},
	language = {en},
	urldate = {2025-01-26},
	booktitle = {2023 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Slyman, Eric and Kahng, Minsuk and Lee, Stefan},
	month = oct,
	year = {2023},
	note = {arXiv:2309.06703 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
	pages = {15245--15255},
}

@misc{huang_conceptexplainer_2022-1,
	title = {{ConceptExplainer}: {Interactive} {Explanation} for {Deep} {Neural} {Networks} from a {Concept} {Perspective}},
	shorttitle = {{ConceptExplainer}},
	url = {http://arxiv.org/abs/2204.01888},
	doi = {10.48550/arXiv.2204.01888},
	abstract = {Traditional deep learning interpretability methods which are suitable for non-expert users cannot explain network behaviors at the global level and are inﬂexible at providing ﬁne-grained explanations. As a solution, concept-based explanations are gaining attention due to their human intuitiveness and their ﬂexibility to describe both global and local model behaviors. Concepts are groups of similarly meaningful pixels that express a notion, embedded within the network’s latent space and have primarily been hand-generated, but have recently been discovered by automated approaches. Unfortunately, the magnitude and diversity of discovered concepts makes it difﬁcult for non-experts to navigate and make sense of the concept space, and lack of easy-to-use software also makes concept explanations inaccessible to many non-expert users. Visual analytics can serve a valuable role in bridging these gaps by enabling structured navigation and exploration of the concept space to provide concept-based insights of model behavior to users. To this end, we design, develop, and validate CONCEPTEXPLAINER, a visual analytics system that enables non-expert users to interactively probe and explore the concept space to explain model behavior at the instance/class/global level. The system was developed via iterative prototyping to address a number of design challenges that non-experts face in interpreting the behavior of deep learning models. Via a rigorous user study, we validate how CONCEPTEXPLAINER supports these challenges. Likewise, we conduct a series of usage scenarios to demonstrate how the system supports the interactive analysis of model behavior across a variety of tasks and explanation granularities, such as identifying concepts that are important to classiﬁcation, identifying bias in training data, and understanding how concepts can be shared across diverse and seemingly dissimilar classes.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Huang, Jinbin and Mishra, Aditi and Kwon, Bum Chul and Bryan, Chris},
	month = oct,
	year = {2022},
	note = {arXiv:2204.01888 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
}

@misc{ning_improving_2021-1,
	title = {Improving {Model} {Robustness} by {Adaptively} {Correcting} {Perturbation} {Levels} with {Active} {Queries}},
	url = {http://arxiv.org/abs/2103.14824},
	doi = {10.48550/arXiv.2103.14824},
	abstract = {In addition to high accuracy, robustness is becoming increasingly important for machine learning models in various applications. Recently, much research has been devoted to improving the model robustness by training with noise perturbations. Most existing studies assume a ﬁxed perturbation level for all training examples, which however hardly holds in real tasks. In fact, excessive perturbations may destroy the discriminative content of an example, while deﬁcient perturbations may fail to provide helpful information for improving the robustness. Motivated by this observation, we propose to adaptively adjust the perturbation levels for each example in the training process. Speciﬁcally, a novel active learning framework is proposed to allow the model to interactively query the correct perturbation level from human experts. By designing a cost-effective sampling strategy along with a new query type, the robustness can be signiﬁcantly improved with a few queries. Both theoretical analysis and experimental studies validate the effectiveness of the proposed approach.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Ning, Kun-Peng and Tao, Lue and Chen, Songcan and Huang, Sheng-Jun},
	month = mar,
	year = {2021},
	note = {arXiv:2103.14824 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{farquhar_statistical_2021-1,
	title = {On {Statistical} {Bias} {In} {Active} {Learning}: {How} and {When} {To} {Fix} {It}},
	shorttitle = {On {Statistical} {Bias} {In} {Active} {Learning}},
	url = {http://arxiv.org/abs/2101.11665},
	doi = {10.48550/arXiv.2101.11665},
	abstract = {Active learning is a powerful tool when labelling data is expensive, but it introduces a bias because the training data no longer follows the population distribution. We formalize this bias and investigate the situations in which it can be harmful and sometimes even helpful. We further introduce novel corrective weights to remove bias when doing so is beneﬁcial. Through this, our work not only provides a useful mechanism that can improve the active learning approach, but also an explanation of the empirical successes of various existing approaches which ignore this bias. In particular, we show that this bias can be actively helpful when training overparameterized models—like neural networks—with relatively little data.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Farquhar, Sebastian and Gal, Yarin and Rainforth, Tom},
	month = may,
	year = {2021},
	note = {arXiv:2101.11665 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{bachman_learning_2017-1,
	title = {Learning {Algorithms} for {Active} {Learning}},
	url = {http://arxiv.org/abs/1708.00088},
	doi = {10.48550/arXiv.1708.00088},
	abstract = {We introduce a model that learns active learning algorithms via metalearning. For a distribution of related tasks, our model jointly learns: a data representation, an item selection heuristic, and a method for constructing prediction functions from labeled training sets. Our model uses the item selection heuristic to gather labeled training sets from which to construct prediction functions. Using the Omniglot and MovieLens datasets, we test our model in synthetic and practical settings.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Bachman, Philip and Sordoni, Alessandro and Trischler, Adam},
	month = jul,
	year = {2017},
	note = {arXiv:1708.00088 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{ross_right_2017-1,
	title = {Right for the {Right} {Reasons}: {Training} {Differentiable} {Models} by {Constraining} their {Explanations}},
	shorttitle = {Right for the {Right} {Reasons}},
	url = {http://arxiv.org/abs/1703.03717},
	doi = {10.48550/arXiv.1703.03717},
	abstract = {Neural networks are among the most accurate supervised learning methods in use today. However, their opacity makes them difﬁcult to trust in critical applications, especially if conditions in training may differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions. These tools can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efﬁciently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients. We apply these penalties both based on expert annotation and in an unsupervised fashion that produces multiple classiﬁers with qualitatively different decision boundaries. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
	month = may,
	year = {2017},
	note = {arXiv:1703.03717 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{akrour_april_2012-1,
	title = {{APRIL}: {Active} {Preference}-learning based {Reinforcement} {Learning}},
	shorttitle = {{APRIL}},
	url = {http://arxiv.org/abs/1208.0984},
	doi = {10.48550/arXiv.1208.0984},
	abstract = {This paper focuses on reinforcement learning (RL) with limited prior knowledge. In the domain of swarm robotics for instance, the expert can hardly design a reward function or demonstrate the target behavior, forbidding the use of both standard RL and inverse reinforcement learning. Although with a limited expertise, the human expert is still often able to emit preferences and rank the agent demonstrations. Earlier work has presented an iterative preference-based RL framework: expert preferences are exploited to learn an approximate policy return, thus enabling the agent to achieve direct policy search. Iteratively, the agent selects a new candidate policy and demonstrates it; the expert ranks the new demonstration comparatively to the previous best one; the expert’s ranking feedback enables the agent to reﬁne the approximate policy return, and the process is iterated.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Akrour, Riad and Schoenauer, Marc and Sebag, Michèle},
	month = aug,
	year = {2012},
	note = {arXiv:1208.0984 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{heuillet_tracking_nodate-1,
	title = {Tracking the {Risk} of {Machine} {Learning} {Systems} with {Partial} {Monitoring}},
	abstract = {Although efficient at performing specific tasks, Machine Learning Systems (MLSs) remain vulnerable to instabilities such as noise or adversarial attacks. In this work, we aim to track the risk exposure of an MLS to these events. We formulate this problem under the stochastic Partial Monitoring (PM) setting. We focus on two instances of partial monitoring, namely the Apple Tasting and Label Efficient games, that are particularly relevant to our problem. Our review of the practicality of existing algorithms motivates RandCBP, a randomized variation of the deterministic algorithm Confidence Bound (CBP) inspired by recent theoretical developments in the bandits setting. Our preliminary results indicate that RandCBP enjoys the same regret guarantees as its deterministic counterpart CBP and achieves competitive empirical performance on settings of interest which suggests it could be a suitable candidate for our problem.},
	language = {en},
	author = {Heuillet, Maxime and Durand, Audrey},
}

@article{kaelbling_planning_1998-1,
	title = {Planning and acting in partially observable stochastic domains},
	volume = {101},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S000437029800023X},
	doi = {10.1016/S0004-3702(98)00023-X},
	abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (MDPS) and partially observable MDPS (POMDPS). We then outline a novel algorithm for solving POMDPS off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to POMDPS, and of some possibilities for finding approximate solutions. 0 1998 Elsevier Science B.V. All rights reserved.},
	language = {en},
	number = {1-2},
	urldate = {2025-01-26},
	journal = {Artificial Intelligence},
	author = {Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra, Anthony R.},
	month = may,
	year = {1998},
	pages = {99--134},
}

@misc{brown_performative_2022-1,
	title = {Performative {Prediction} in a {Stateful} {World}},
	url = {http://arxiv.org/abs/2011.03885},
	doi = {10.48550/arXiv.2011.03885},
	abstract = {Deployed supervised machine learning models make predictions that interact with and inﬂuence the world. This phenomenon is called performative prediction by Perdomo et al. (ICML 2020). It is an ongoing challenge to understand the inﬂuence of such predictions as well as design tools so as to control that inﬂuence. We propose a theoretical framework where the response of a target population to the deployed classiﬁer is modeled as a function of the classiﬁer and the current state (distribution) of the population. We show necessary and suﬃcient conditions for convergence to an equilibrium of two retraining algorithms, repeated risk minimization and a lazier variant. Furthermore, convergence is near an optimal classiﬁer. We thus generalize results of Perdomo et al., whose performativity framework does not assume any dependence on the state of the target population. A particular phenomenon captured by our model is that of distinct groups that acquire information and resources at diﬀerent rates to be able to respond to the latest deployed classiﬁer. We study this phenomenon theoretically and empirically.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Brown, Gavin and Hod, Shlomi and Kalemaj, Iden},
	month = feb,
	year = {2022},
	note = {arXiv:2011.03885 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning},
}

@misc{mansoury_feedback_2020-1,
	title = {Feedback {Loop} and {Bias} {Amplification} in {Recommender} {Systems}},
	url = {http://arxiv.org/abs/2007.13019},
	doi = {10.48550/arXiv.2007.13019},
	abstract = {Recommendation algorithms are known to suffer from popularity bias; a few popular items are recommended frequently while the majority of other items are ignored. These recommendations are then consumed by the users, their reaction will be logged and added to the system: what is generally known as a feedback loop. In this paper, we propose a method for simulating the users interaction with the recommenders in an offline setting and study the impact of feedback loop on the popularity bias amplification of several recommendation algorithms. We then show how this bias amplification leads to several other problems such as declining the aggregate diversity, shifting the representation of users’ taste over time and also homogenization of the users experience. In particular, we show that the impact of feedback loop is generally stronger for the users who belong to the minority group.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Mansoury, Masoud and Abdollahpouri, Himan and Pechenizkiy, Mykola and Mobasher, Bamshad and Burke, Robin},
	month = jul,
	year = {2020},
	note = {arXiv:2007.13019 [cs]},
	keywords = {Computer Science - Information Retrieval},
}

@misc{novoseller_dueling_2020-1,
	title = {Dueling {Posterior} {Sampling} for {Preference}-{Based} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1908.01289},
	doi = {10.48550/arXiv.1908.01289},
	abstract = {In preference-based reinforcement learning (RL), an agent interacts with the environment while receiving preferences instead of absolute feedback. While there is increasing research activity in preference-based RL, the design of formal frameworks that admit tractable theoretical analysis remains an open challenge. Building upon ideas from preference-based bandit learning and posterior sampling in RL, we present DUELING POSTERIOR SAMPLING (DPS), which employs preference-based posterior sampling to learn both the system dynamics and the underlying utility function that governs the preference feedback. As preference feedback is provided on trajectories rather than individual state-action pairs, we develop a Bayesian approach for the credit assignment problem, translating preferences to a posterior distribution over state-action reward models. We prove an asymptotic Bayesian no-regret rate for DPS with a Bayesian linear regression credit assignment model. This is the ﬁrst regret guarantee for preference-based RL to our knowledge. We also discuss possible avenues for extending the proof methodology to other credit assignment models. Finally, we evaluate the approach empirically, showing competitive performance against existing baselines.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Novoseller, Ellen R. and Wei, Yibing and Sui, Yanan and Yue, Yisong and Burdick, Joel W.},
	month = jun,
	year = {2020},
	note = {arXiv:1908.01289 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{christiano_deep_2023-1,
	title = {Deep reinforcement learning from human preferences},
	url = {http://arxiv.org/abs/1706.03741},
	doi = {10.48550/arXiv.1706.03741},
	abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals deﬁned in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than 1\% of our agent’s interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the ﬂexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
	month = feb,
	year = {2023},
	note = {arXiv:1706.03741 [stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{shen_metric-fair_nodate-2,
	title = {Metric-{Fair} {Active} {Learning}},
	abstract = {Active learning has become a prevalent technique for designing label-efﬁcient algorithms, where the central principle is to only query and ﬁt “informative” labeled instances. It is, however, known that an active learning algorithm may incur unfairness due to such instance selection procedure. In this paper, we henceforth study metric-fair active learning of homogeneous halfspaces, and show that under the distribution-dependent PAC learning model, fairness and label efﬁciency can be achieved simultaneously. We further propose two extensions of our main results: 1) we show that it is possible to make the algorithm robust to the adversarial noise – one of the most challenging noise models in learning theory; and 2) it is possible to signiﬁcantly improve the label complexity when the underlying halfspace is sparse.},
	language = {en},
	author = {Shen, Jie and Cui, Nan and Wang, Jing},
}

@article{abramoff_considerations_2023-2,
	title = {Considerations for addressing bias in artificial intelligence for health equity},
	volume = {6},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-023-00913-9},
	doi = {10.1038/s41746-023-00913-9},
	abstract = {Abstract
            Health equity is a primary goal of healthcare stakeholders: patients and their advocacy groups, clinicians, other providers and their professional societies, bioethicists, payors and value based care organizations, regulatory agencies, legislators, and creators of artificial intelligence/machine learning (AI/ML)-enabled medical devices. Lack of equitable access to diagnosis and treatment may be improved through new digital health technologies, especially AI/ML, but these may also exacerbate disparities, depending on how bias is addressed. We propose an expanded Total Product Lifecycle (TPLC) framework for healthcare AI/ML, describing the sources and impacts of undesirable bias in AI/ML systems in each phase, how these can be analyzed using appropriate metrics, and how they can be potentially mitigated. The goal of these “Considerations” is to educate stakeholders on how potential AI/ML bias may impact healthcare outcomes and how to identify and mitigate inequities; to initiate a discussion between stakeholders on these issues, in order to ensure health equity along the expanded AI/ML TPLC framework, and ultimately, better health outcomes for all.},
	language = {en},
	number = {1},
	urldate = {2025-01-26},
	journal = {npj Digital Medicine},
	author = {Abràmoff, Michael D. and Tarver, Michelle E. and Loyo-Berrios, Nilsa and Trujillo, Sylvia and Char, Danton and Obermeyer, Ziad and Eydelman, Malvina B. and {Foundational Principles of Ophthalmic Imaging and Algorithmic Interpretation Working Group of the Collaborative Community for Ophthalmic Imaging Foundation, Washington, D.C.} and Maisel, William H.},
	month = sep,
	year = {2023},
	pages = {170},
}

@article{godbout_game-theoretic_nodate-2,
	title = {A {Game}-{Theoretic} {Perspective} on {Risk}-{Sensitive}{\textbackslash} {Reinforcement} {Learning}},
	abstract = {Most Reinforcement Learning (RL) approaches usually aim to ﬁnd a policy that maximizes its expected return. However, this objective may be inappropriate in many safety-critical domains such as healthcare or autonomous driving, where it is often preferable to optimize for a risk-sensitive measure of the policy’s return as the learning objective, such as the Conditional-Value-at-Risk (CVaR). Although previous literature exists to address the problem of learning CVaR-optimal policies in Markov decision problems, it mostly relies on the distributional RL perspective. In this paper, we solve this problem by rather proposing an approach based on a game theoretic perspective, which can be applied on top of any existing RL algorithm. At the core of our approach is a twoplayer zero-sum game between a policy player and an adversary that perturbs the policy player’s state transitions given a ﬁnite budget. We show that, the closer the players are to the game’s equilibrium point, the closer the learned policy is to the CVaR-optimal one with a risk tolerance explicitly related to the adversary’s budget. We provide a gradient-based training procedure to solve the proposed game by formulating it as a Stackelberg game, enabling the use of deep RL architectures and training algorithms. We illustrate the applicability of our approach on a risky artiﬁcial environment, presenting the different policies learned for various adversary budgets.},
	language = {en},
	author = {Godbout, Mathieu and Heuillet, Maxime and Chandar, Sharath and Bhati, Rupali and Durand, Audrey},
}

@misc{azar_general_2023-2,
	title = {A {General} {Theoretical} {Paradigm} to {Understand} {Learning} from {Human} {Preferences}},
	url = {http://arxiv.org/abs/2310.12036},
	doi = {10.48550/arXiv.2310.12036},
	abstract = {The prevalent deployment of learning from human preferences through reinforcement learning (RLHF) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Azar, Mohammad Gheshlaghi and Rowland, Mark and Piot, Bilal and Guo, Daniel and Calandriello, Daniele and Valko, Michal and Munos, Rémi},
	month = nov,
	year = {2023},
	note = {arXiv:2310.12036 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{vaswani_old_nodate-2,
	title = {Old {Dog} {Learns} {New} {Tricks}: {Randomized} {UCB} for {Bandit} {Problems}},
	language = {en},
	author = {Vaswani, Sharan and Mehrabian, Abbas and Durand, Audrey and Kveton, Branislav},
}

@article{durand_machine_2018-2,
	title = {A machine learning approach for online automated optimization of super-resolution optical microscopy},
	volume = {9},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-018-07668-y},
	doi = {10.1038/s41467-018-07668-y},
	abstract = {Abstract
            Traditional approaches for finding well-performing parameterizations of complex imaging systems, such as super-resolution microscopes rely on an extensive exploration phase over the illumination and acquisition settings, prior to the imaging task. This strategy suffers from several issues: it requires a large amount of parameter configurations to be evaluated, it leads to discrepancies between well-performing parameters in the exploration phase and imaging task, and it results in a waste of time and resources given that optimization and final imaging tasks are conducted separately. Here we show that a fully automated, machine learning-based system can conduct imaging parameter optimization toward a trade-off between several objectives, simultaneously to the imaging task. Its potential is highlighted on various imaging tasks, such as live-cell and multicolor imaging and multimodal optimization. This online optimization routine can be integrated to various imaging systems to increase accessibility, optimize performance and improve overall imaging quality.},
	language = {en},
	number = {1},
	urldate = {2025-01-26},
	journal = {Nature Communications},
	author = {Durand, Audrey and Wiesner, Theresa and Gardner, Marc-André and Robitaille, Louis-Émile and Bilodeau, Anthony and Gagné, Christian and De Koninck, Paul and Lavoie-Cardinal, Flavie},
	month = dec,
	year = {2018},
	pages = {5247},
}

@article{lattimore_partial_nodate-2,
	title = {Partial monitoring},
	abstract = {We provide a novel algorithm for adversarial k-action d-outcome partial monitoring that is adaptive, intuitive and efﬁcient. The highlight is that for the non-degenerate locally observable games, the n-round minimax regret is bounded by 6mk3/2√n log(k), where m is the number of signals. This matches the best known information-theoretic upper bound derived via Bayesian minimax duality. The same algorithm also achieves near-optimal regret for full information, bandit and globally observable games. High probability bounds and simple experiments are also provided.},
	language = {en},
	author = {Lattimore, Tor and Szepesvari, Csaba},
}

@article{jiang_general_2023-2,
	title = {General intelligence requires rethinking exploration},
	volume = {10},
	issn = {2054-5703},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.230539},
	doi = {10.1098/rsos.230539},
	abstract = {We are at the cusp of a transition from ‘learning from data’ to ‘learning what data to learn from’ as a central focus of artificial intelligence (AI) research. While the first-order learning problem is not completely solved, large models under unified architectures, such as transformers, have shifted the learning bottleneck from how to effectively train models to how to effectively acquire and use task-relevant data. This problem, which we frame as
              exploration
              , is a universal aspect of learning in open-ended domains like the real world. Although the study of exploration in AI is largely limited to the field of reinforcement learning, we argue that exploration is essential to all learning systems, including supervised learning. We propose the problem of
              generalized exploration
              to conceptually unify exploration-driven learning between supervised learning and reinforcement learning, allowing us to highlight key similarities across learning settings and open research challenges. Importantly, generalized exploration is a necessary objective for maintaining open-ended learning processes, which in continually learning to discover and solve new problems, provides a promising path to more general intelligence.},
	language = {en},
	number = {6},
	urldate = {2025-01-26},
	journal = {Royal Society Open Science},
	author = {Jiang, Minqi and Rocktäschel, Tim and Grefenstette, Edward},
	month = jun,
	year = {2023},
	pages = {230539},
}

@misc{jain_biological_2023-2,
	title = {Biological {Sequence} {Design} with {GFlowNets}},
	url = {http://arxiv.org/abs/2203.04115},
	doi = {10.48550/arXiv.2203.04115},
	abstract = {Design of de novo biological sequences with desired properties, like protein and DNA sequences, often involves an active loop with several rounds of molecule ideation and expensive wet-lab evaluations. These experiments can consist of multiple stages, with increasing levels of precision and cost of evaluation, where candidates are filtered. This makes the diversity of proposed candidates a key consideration in the ideation phase. In this work, we propose an active learning algorithm leveraging epistemic uncertainty estimation and the recently proposed GFlowNets as a generator of diverse candidate solutions, with the objective to obtain a diverse batch of useful (as defined by some utility function, for example, the predicted anti-microbial activity of a peptide) and informative candidates after each round. We also propose a scheme to incorporate existing labeled datasets of candidates, in addition to a reward function, to speed up learning in GFlowNets. We present empirical results on several biological sequence design tasks, and we find that our method generates more diverse and novel batches with high scoring candidates compared to existing approaches.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Jain, Moksh and Bengio, Emmanuel and Garcia, Alex-Hernandez and Rector-Brooks, Jarrid and Dossou, Bonaventure F. P. and Ekbote, Chanakya and Fu, Jie and Zhang, Tianyu and Kilgour, Micheal and Zhang, Dinghuai and Simine, Lena and Das, Payel and Bengio, Yoshua},
	month = may,
	year = {2023},
	note = {arXiv:2203.04115 [q-bio]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Biomolecules},
}

@misc{pacchiano_dueling_2023-2,
	title = {Dueling {RL}: {Reinforcement} {Learning} with {Trajectory} {Preferences}},
	shorttitle = {Dueling {RL}},
	url = {http://arxiv.org/abs/2111.04850},
	doi = {10.48550/arXiv.2111.04850},
	abstract = {We consider the problem of preference based reinforcement learning (PbRL), where, unlike traditional reinforcement learning, an agent receives feedback only in terms of a 1 bit (0/1) preference over a trajectory pair instead of absolute rewards for them. The success of the traditional RL framework crucially relies on the underlying agent-reward model, which, however, depends on how accurately a system designer can express an appropriate reward function and often a non-trivial task. The main novelty of our framework is the ability to learn from preference-based trajectory feedback that eliminates the need to hand-craft numeric reward models. This paper sets up a formal framework for the PbRL problem with non-markovian rewards, where the trajectory preferences are encoded by a generalized linear model of dimension \$d\$. Assuming the transition model is known, we then propose an algorithm with almost optimal regret guarantee of \${\textbackslash}tilde \{{\textbackslash}mathcal\{O\}\}{\textbackslash}left( SH d {\textbackslash}log (T / {\textbackslash}delta) {\textbackslash}sqrt\{T\} {\textbackslash}right)\$. We further, extend the above algorithm to the case of unknown transition dynamics, and provide an algorithm with near optimal regret guarantee \${\textbackslash}widetilde\{{\textbackslash}mathcal\{O\}\}(({\textbackslash}sqrt\{d\} + H{\textasciicircum}2 + {\textbar}{\textbackslash}mathcal\{S\}{\textbar}){\textbackslash}sqrt\{dT\} +{\textbackslash}sqrt\{{\textbar}{\textbackslash}mathcal\{S\}{\textbar}{\textbar}{\textbackslash}mathcal\{A\}{\textbar}TH\} )\$. To the best of our knowledge, our work is one of the first to give tight regret guarantees for preference based RL problems with trajectory preferences.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Pacchiano, Aldo and Saha, Aadirupa and Lee, Jonathan},
	month = feb,
	year = {2023},
	note = {arXiv:2111.04850 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{ning_improving_2021-2,
	title = {Improving {Model} {Robustness} by {Adaptively} {Correcting} {Perturbation} {Levels} with {Active} {Queries}},
	url = {http://arxiv.org/abs/2103.14824},
	doi = {10.48550/arXiv.2103.14824},
	abstract = {In addition to high accuracy, robustness is becoming increasingly important for machine learning models in various applications. Recently, much research has been devoted to improving the model robustness by training with noise perturbations. Most existing studies assume a ﬁxed perturbation level for all training examples, which however hardly holds in real tasks. In fact, excessive perturbations may destroy the discriminative content of an example, while deﬁcient perturbations may fail to provide helpful information for improving the robustness. Motivated by this observation, we propose to adaptively adjust the perturbation levels for each example in the training process. Speciﬁcally, a novel active learning framework is proposed to allow the model to interactively query the correct perturbation level from human experts. By designing a cost-effective sampling strategy along with a new query type, the robustness can be signiﬁcantly improved with a few queries. Both theoretical analysis and experimental studies validate the effectiveness of the proposed approach.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Ning, Kun-Peng and Tao, Lue and Chen, Songcan and Huang, Sheng-Jun},
	month = mar,
	year = {2021},
	note = {arXiv:2103.14824 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{brown_performative_2022-2,
	title = {Performative {Prediction} in a {Stateful} {World}},
	url = {http://arxiv.org/abs/2011.03885},
	doi = {10.48550/arXiv.2011.03885},
	abstract = {Deployed supervised machine learning models make predictions that interact with and inﬂuence the world. This phenomenon is called performative prediction by Perdomo et al. (ICML 2020). It is an ongoing challenge to understand the inﬂuence of such predictions as well as design tools so as to control that inﬂuence. We propose a theoretical framework where the response of a target population to the deployed classiﬁer is modeled as a function of the classiﬁer and the current state (distribution) of the population. We show necessary and suﬃcient conditions for convergence to an equilibrium of two retraining algorithms, repeated risk minimization and a lazier variant. Furthermore, convergence is near an optimal classiﬁer. We thus generalize results of Perdomo et al., whose performativity framework does not assume any dependence on the state of the target population. A particular phenomenon captured by our model is that of distinct groups that acquire information and resources at diﬀerent rates to be able to respond to the latest deployed classiﬁer. We study this phenomenon theoretically and empirically.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Brown, Gavin and Hod, Shlomi and Kalemaj, Iden},
	month = feb,
	year = {2022},
	note = {arXiv:2011.03885 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning},
}

@misc{mansoury_feedback_2020-2,
	title = {Feedback {Loop} and {Bias} {Amplification} in {Recommender} {Systems}},
	url = {http://arxiv.org/abs/2007.13019},
	doi = {10.48550/arXiv.2007.13019},
	abstract = {Recommendation algorithms are known to suffer from popularity bias; a few popular items are recommended frequently while the majority of other items are ignored. These recommendations are then consumed by the users, their reaction will be logged and added to the system: what is generally known as a feedback loop. In this paper, we propose a method for simulating the users interaction with the recommenders in an offline setting and study the impact of feedback loop on the popularity bias amplification of several recommendation algorithms. We then show how this bias amplification leads to several other problems such as declining the aggregate diversity, shifting the representation of users’ taste over time and also homogenization of the users experience. In particular, we show that the impact of feedback loop is generally stronger for the users who belong to the minority group.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Mansoury, Masoud and Abdollahpouri, Himan and Pechenizkiy, Mykola and Mobasher, Bamshad and Burke, Robin},
	month = jul,
	year = {2020},
	note = {arXiv:2007.13019 [cs]},
	keywords = {Computer Science - Information Retrieval},
}

@misc{bachman_learning_2017-2,
	title = {Learning {Algorithms} for {Active} {Learning}},
	url = {http://arxiv.org/abs/1708.00088},
	doi = {10.48550/arXiv.1708.00088},
	abstract = {We introduce a model that learns active learning algorithms via metalearning. For a distribution of related tasks, our model jointly learns: a data representation, an item selection heuristic, and a method for constructing prediction functions from labeled training sets. Our model uses the item selection heuristic to gather labeled training sets from which to construct prediction functions. Using the Omniglot and MovieLens datasets, we test our model in synthetic and practical settings.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Bachman, Philip and Sordoni, Alessandro and Trischler, Adam},
	month = jul,
	year = {2017},
	note = {arXiv:1708.00088 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{slyman_vlslice_2023-2,
	title = {{VLSlice}: {Interactive} {Vision}-and-{Language} {Slice} {Discovery}},
	shorttitle = {{VLSlice}},
	url = {http://arxiv.org/abs/2309.06703},
	doi = {10.1109/ICCV51070.2023.01403},
	abstract = {Recent work in vision-and-language demonstrates that large-scale pretraining can learn generalizable models that are efficiently transferable to downstream tasks. While this may improve dataset-scale aggregate metrics, analyzing performance around hand-crafted subgroups targeting specific bias dimensions reveals systemic undesirable behaviors. However, this subgroup analysis is frequently stalled by annotation efforts, which require extensive time and resources to collect the necessary data. Prior art attempts to automatically discover subgroups to circumvent these constraints but typically leverages model behavior on existing task-specific annotations and rapidly degrades on more complex inputs beyond “tabular” data, none of which study vision-and-language models. This paper presents VLSlice, an interactive system enabling user-guided discovery of coherent representation-level subgroups with consistent visiolinguistic behavior, denoted as vision-and-language slices, from unlabeled image sets. We show that VLSlice enables users to quickly generate diverse high-coherency slices in a user study (n=22) and release the tool publicly1.},
	language = {en},
	urldate = {2025-01-26},
	booktitle = {2023 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Slyman, Eric and Kahng, Minsuk and Lee, Stefan},
	month = oct,
	year = {2023},
	note = {arXiv:2309.06703 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
	pages = {15245--15255},
}

@misc{huang_conceptexplainer_2022-2,
	title = {{ConceptExplainer}: {Interactive} {Explanation} for {Deep} {Neural} {Networks} from a {Concept} {Perspective}},
	shorttitle = {{ConceptExplainer}},
	url = {http://arxiv.org/abs/2204.01888},
	doi = {10.48550/arXiv.2204.01888},
	abstract = {Traditional deep learning interpretability methods which are suitable for non-expert users cannot explain network behaviors at the global level and are inﬂexible at providing ﬁne-grained explanations. As a solution, concept-based explanations are gaining attention due to their human intuitiveness and their ﬂexibility to describe both global and local model behaviors. Concepts are groups of similarly meaningful pixels that express a notion, embedded within the network’s latent space and have primarily been hand-generated, but have recently been discovered by automated approaches. Unfortunately, the magnitude and diversity of discovered concepts makes it difﬁcult for non-experts to navigate and make sense of the concept space, and lack of easy-to-use software also makes concept explanations inaccessible to many non-expert users. Visual analytics can serve a valuable role in bridging these gaps by enabling structured navigation and exploration of the concept space to provide concept-based insights of model behavior to users. To this end, we design, develop, and validate CONCEPTEXPLAINER, a visual analytics system that enables non-expert users to interactively probe and explore the concept space to explain model behavior at the instance/class/global level. The system was developed via iterative prototyping to address a number of design challenges that non-experts face in interpreting the behavior of deep learning models. Via a rigorous user study, we validate how CONCEPTEXPLAINER supports these challenges. Likewise, we conduct a series of usage scenarios to demonstrate how the system supports the interactive analysis of model behavior across a variety of tasks and explanation granularities, such as identifying concepts that are important to classiﬁcation, identifying bias in training data, and understanding how concepts can be shared across diverse and seemingly dissimilar classes.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Huang, Jinbin and Mishra, Aditi and Kwon, Bum Chul and Bryan, Chris},
	month = oct,
	year = {2022},
	note = {arXiv:2204.01888 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
}

@misc{farquhar_statistical_2021-2,
	title = {On {Statistical} {Bias} {In} {Active} {Learning}: {How} and {When} {To} {Fix} {It}},
	shorttitle = {On {Statistical} {Bias} {In} {Active} {Learning}},
	url = {http://arxiv.org/abs/2101.11665},
	doi = {10.48550/arXiv.2101.11665},
	abstract = {Active learning is a powerful tool when labelling data is expensive, but it introduces a bias because the training data no longer follows the population distribution. We formalize this bias and investigate the situations in which it can be harmful and sometimes even helpful. We further introduce novel corrective weights to remove bias when doing so is beneﬁcial. Through this, our work not only provides a useful mechanism that can improve the active learning approach, but also an explanation of the empirical successes of various existing approaches which ignore this bias. In particular, we show that this bias can be actively helpful when training overparameterized models—like neural networks—with relatively little data.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Farquhar, Sebastian and Gal, Yarin and Rainforth, Tom},
	month = may,
	year = {2021},
	note = {arXiv:2101.11665 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{novoseller_dueling_2020-2,
	title = {Dueling {Posterior} {Sampling} for {Preference}-{Based} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1908.01289},
	doi = {10.48550/arXiv.1908.01289},
	abstract = {In preference-based reinforcement learning (RL), an agent interacts with the environment while receiving preferences instead of absolute feedback. While there is increasing research activity in preference-based RL, the design of formal frameworks that admit tractable theoretical analysis remains an open challenge. Building upon ideas from preference-based bandit learning and posterior sampling in RL, we present DUELING POSTERIOR SAMPLING (DPS), which employs preference-based posterior sampling to learn both the system dynamics and the underlying utility function that governs the preference feedback. As preference feedback is provided on trajectories rather than individual state-action pairs, we develop a Bayesian approach for the credit assignment problem, translating preferences to a posterior distribution over state-action reward models. We prove an asymptotic Bayesian no-regret rate for DPS with a Bayesian linear regression credit assignment model. This is the ﬁrst regret guarantee for preference-based RL to our knowledge. We also discuss possible avenues for extending the proof methodology to other credit assignment models. Finally, we evaluate the approach empirically, showing competitive performance against existing baselines.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Novoseller, Ellen R. and Wei, Yibing and Sui, Yanan and Yue, Yisong and Burdick, Joel W.},
	month = jun,
	year = {2020},
	note = {arXiv:1908.01289 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{christiano_deep_2023-2,
	title = {Deep reinforcement learning from human preferences},
	url = {http://arxiv.org/abs/1706.03741},
	doi = {10.48550/arXiv.1706.03741},
	abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals deﬁned in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than 1\% of our agent’s interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the ﬂexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
	month = feb,
	year = {2023},
	note = {arXiv:1706.03741 [stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{heuillet_tracking_nodate-2,
	title = {Tracking the {Risk} of {Machine} {Learning} {Systems} with {Partial} {Monitoring}},
	abstract = {Although efficient at performing specific tasks, Machine Learning Systems (MLSs) remain vulnerable to instabilities such as noise or adversarial attacks. In this work, we aim to track the risk exposure of an MLS to these events. We formulate this problem under the stochastic Partial Monitoring (PM) setting. We focus on two instances of partial monitoring, namely the Apple Tasting and Label Efficient games, that are particularly relevant to our problem. Our review of the practicality of existing algorithms motivates RandCBP, a randomized variation of the deterministic algorithm Confidence Bound (CBP) inspired by recent theoretical developments in the bandits setting. Our preliminary results indicate that RandCBP enjoys the same regret guarantees as its deterministic counterpart CBP and achieves competitive empirical performance on settings of interest which suggests it could be a suitable candidate for our problem.},
	language = {en},
	author = {Heuillet, Maxime and Durand, Audrey},
}

@article{kaelbling_planning_1998-2,
	title = {Planning and acting in partially observable stochastic domains},
	volume = {101},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S000437029800023X},
	doi = {10.1016/S0004-3702(98)00023-X},
	abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (MDPS) and partially observable MDPS (POMDPS). We then outline a novel algorithm for solving POMDPS off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to POMDPS, and of some possibilities for finding approximate solutions. 0 1998 Elsevier Science B.V. All rights reserved.},
	language = {en},
	number = {1-2},
	urldate = {2025-01-26},
	journal = {Artificial Intelligence},
	author = {Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra, Anthony R.},
	month = may,
	year = {1998},
	pages = {99--134},
}

@article{kaelbling_planning_1998-3,
	title = {Planning and acting in partially observable stochastic domains},
	volume = {101},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S000437029800023X},
	doi = {10.1016/S0004-3702(98)00023-X},
	abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (MDPS) and partially observable MDPS (POMDPS). We then outline a novel algorithm for solving POMDPS off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to POMDPS, and of some possibilities for finding approximate solutions. 0 1998 Elsevier Science B.V. All rights reserved.},
	language = {en},
	number = {1-2},
	urldate = {2025-01-26},
	journal = {Artificial Intelligence},
	author = {Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra, Anthony R.},
	month = may,
	year = {1998},
	pages = {99--134},
}

@misc{ross_right_2017-2,
	title = {Right for the {Right} {Reasons}: {Training} {Differentiable} {Models} by {Constraining} their {Explanations}},
	shorttitle = {Right for the {Right} {Reasons}},
	url = {http://arxiv.org/abs/1703.03717},
	doi = {10.48550/arXiv.1703.03717},
	abstract = {Neural networks are among the most accurate supervised learning methods in use today. However, their opacity makes them difﬁcult to trust in critical applications, especially if conditions in training may differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions. These tools can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efﬁciently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients. We apply these penalties both based on expert annotation and in an unsupervised fashion that produces multiple classiﬁers with qualitatively different decision boundaries. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
	month = may,
	year = {2017},
	note = {arXiv:1703.03717 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{akrour_april_2012-2,
	title = {{APRIL}: {Active} {Preference}-learning based {Reinforcement} {Learning}},
	shorttitle = {{APRIL}},
	url = {http://arxiv.org/abs/1208.0984},
	doi = {10.48550/arXiv.1208.0984},
	abstract = {This paper focuses on reinforcement learning (RL) with limited prior knowledge. In the domain of swarm robotics for instance, the expert can hardly design a reward function or demonstrate the target behavior, forbidding the use of both standard RL and inverse reinforcement learning. Although with a limited expertise, the human expert is still often able to emit preferences and rank the agent demonstrations. Earlier work has presented an iterative preference-based RL framework: expert preferences are exploited to learn an approximate policy return, thus enabling the agent to achieve direct policy search. Iteratively, the agent selects a new candidate policy and demonstrates it; the expert ranks the new demonstration comparatively to the previous best one; the expert’s ranking feedback enables the agent to reﬁne the approximate policy return, and the process is iterated.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Akrour, Riad and Schoenauer, Marc and Sebag, Michèle},
	month = aug,
	year = {2012},
	note = {arXiv:1208.0984 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{fujimoto_memory_2024,
	title = {Memory {Asymmetry} {Creates} {Heteroclinic} {Orbits} to {Nash} {Equilibrium} in {Learning} in {Zero}-{Sum} {Games}},
	url = {http://arxiv.org/abs/2305.13619},
	doi = {10.48550/arXiv.2305.13619},
	abstract = {Learning in games considers how multiple agents maximize their own rewards through repeated games. Memory, an ability that an agent changes his/her action depending on the history of actions in previous games, is often introduced into learning to explore more clever strategies and discuss the decision-making of real agents like humans. However, such games with memory are hard to analyze because they exhibit complex phenomena like chaotic dynamics or divergence from Nash equilibrium. In particular, how asymmetry in memory capacities between agents affects learning in games is still unclear. In response, this study formulates a gradient ascent algorithm in games with asymmetry memory capacities. To obtain theoretical insights into learning dynamics, we first consider a simple case of zero-sum games. We observe complex behavior, where learning dynamics draw a heteroclinic connection from unstable fixed points to stable ones. Despite this complexity, we analyze learning dynamics and prove local convergence to these stable fixed points, i.e., the Nash equilibria. We identify the mechanism driving this convergence: an agent with a longer memory learns to exploit the other, which in turn endows the other's utility function with strict concavity. We further numerically observe such convergence in various initial strategies, action numbers, and memory lengths. This study reveals a novel phenomenon due to memory asymmetry, providing fundamental strides in learning in games and new insights into computing equilibria.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Fujimoto, Yuma and Ariu, Kaito and Abe, Kenshi},
	month = feb,
	year = {2024},
	note = {arXiv:2305.13619 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Multiagent Systems, Mathematics - Optimization and Control, Nonlinear Sciences - Chaotic Dynamics},
}

@misc{noauthor_stochastic_2022,
	title = {A {Stochastic} {Variant} of {Replicator} {Dynamics} in {Zero}-{Sum} {Games} and {Its} {Invariant} {Measures}},
	url = {https://www.fields.utoronto.ca/talks/Stochastic-Variant-Replicator-Dynamics-Zero-Sum-Games-and-Its-Invariant-Measures},
	abstract = {We study the behavior of a stochastic variant of replicator dynamics in two-agent zero-sum games. We characterize the statistics of such systems by their invariant measures which can be shown to be entirely supported on the boundary of the space of mixed strategies.},
	language = {en},
	urldate = {2025-01-23},
	journal = {Fields Institute for Research in Mathematical Sciences},
	month = aug,
	year = {2022},
}

@misc{fujimoto_learning_2023,
	title = {Learning in {Multi}-{Memory} {Games} {Triggers} {Complex} {Dynamics} {Diverging} from {Nash} {Equilibrium}},
	url = {http://arxiv.org/abs/2302.01073},
	doi = {10.48550/arXiv.2302.01073},
	abstract = {Repeated games consider a situation where multiple agents are motivated by their independent rewards throughout learning. In general, the dynamics of their learning become complex. Especially when their rewards compete with each other like zero-sum games, the dynamics often do not converge to their optimum, i.e., the Nash equilibrium. To tackle such complexity, many studies have understood various learning algorithms as dynamical systems and discovered qualitative insights among the algorithms. However, such studies have yet to handle multi-memory games (where agents can memorize actions they played in the past and choose their actions based on their memories), even though memorization plays a pivotal role in artificial intelligence and interpersonal relationship. This study extends two major learning algorithms in games, i.e., replicator dynamics and gradient ascent, into multi-memory games. Then, we prove their dynamics are identical. Furthermore, theoretically and experimentally, we clarify that the learning dynamics diverge from the Nash equilibrium in multi-memory zero-sum games and reach heteroclinic cycles (sojourn longer around the boundary of the strategy space), providing a fundamental advance in learning in games.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Fujimoto, Yuma and Ariu, Kaito and Abe, Kenshi},
	month = may,
	year = {2023},
	note = {arXiv:2302.01073 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Multiagent Systems, Mathematics - Optimization and Control, Nonlinear Sciences - Chaotic Dynamics},
}

@misc{bouteiller_evolution_2024,
	title = {Evolution with {Opponent}-{Learning} {Awareness}},
	url = {http://arxiv.org/abs/2410.17466},
	doi = {10.48550/arXiv.2410.17466},
	abstract = {The universe involves many independent co-learning agents as an ever-evolving part of our observed environment. Yet, in practice, Multi-Agent Reinforcement Learning (MARL) applications are usually constrained to small, homogeneous populations and remain computationally intensive. In this paper, we study how large heterogeneous populations of learning agents evolve in normal-form games. We show how, under assumptions commonly made in the multi-armed bandit literature, Multi-Agent Policy Gradient closely resembles the Replicator Dynamic, and we further derive a fast, parallelizable implementation of Opponent-Learning Awareness tailored for evolutionary simulations. This enables us to simulate the evolution of very large populations made of heterogeneous co-learning agents, under both naive and advanced learning strategies. We demonstrate our approach in simulations of 200,000 agents, evolving in the classic games of Hawk-Dove, Stag-Hunt, and Rock-Paper-Scissors. Each game highlights distinct ways in which Opponent-Learning Awareness affects evolution.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Bouteiller, Yann and Soma, Karthik and Beltrame, Giovanni},
	month = oct,
	year = {2024},
	note = {arXiv:2410.17466 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Quantitative Biology - Populations and Evolution, Quantitative Finance - General Finance},
}

@article{szabo_evolutionary_2007,
	title = {Evolutionary games on graphs},
	volume = {446},
	issn = {03701573},
	url = {http://arxiv.org/abs/cond-mat/0607344},
	doi = {10.1016/j.physrep.2007.04.004},
	abstract = {Game theory is one of the key paradigms behind many scientific disciplines from biology to behavioral sciences to economics. In its evolutionary form and especially when the interacting agents are linked in a specific social network the underlying solution concepts and methods are very similar to those applied in non-equilibrium statistical physics. This review gives a tutorial-type overview of the field for physicists. The first three sections introduce the necessary background in classical and evolutionary game theory from the basic definitions to the most important results. The fourth section surveys the topological complications implied by non-mean-field-type social network structures in general. The last three sections discuss in detail the dynamic behavior of three prominent classes of models: the Prisoner's Dilemma, the Rock-Scissors-Paper game, and Competing Associations. The major theme of the review is in what sense and how the graph structure of interactions can modify and enrich the picture of long term behavioral patterns emerging in evolutionary games.},
	number = {4-6},
	urldate = {2025-01-27},
	journal = {Physics Reports},
	author = {Szabo, Gyorgy and Fath, Gabor},
	month = jul,
	year = {2007},
	note = {arXiv:cond-mat/0607344},
	keywords = {Condensed Matter - Statistical Mechanics, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Quantitative Biology - Populations and Evolution},
	pages = {97--216},
}
