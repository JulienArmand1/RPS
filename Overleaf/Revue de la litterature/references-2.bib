@misc{douglas_naive_2024,
    title = {Naive {Algorithmic} {Collusion}: {When} {Do} {Bandit} {Learners} {Cooperate} and {When} {Do} {They} {Compete}?},
    shorttitle = {Naive {Algorithmic} {Collusion}},
    url = {http://arxiv.org/abs/2411.16574},
    doi = {10.48550/arXiv.2411.16574},
    abstract = {Algorithmic agents are used in a variety of competitive decision settings, notably in making pricing decisions in contexts that range from online retail to residential home rentals. Business managers, algorithm designers, legal scholars, and regulators alike are all starting to consider the ramifications of "algorithmic collusion." We study the emergent behavior of multi-armed bandit machine learning algorithms used in situations where agents are competing, but they have no information about the strategic interaction they are engaged in. Using a general-form repeated Prisoner's Dilemma game, agents engage in online learning with no prior model of game structure and no knowledge of competitors' states or actions (e.g., no observation of competing prices). We show that these context-free bandits, with no knowledge of opponents' choices or outcomes, still will consistently learn collusive behavior - what we call "naive collusion." We primarily study this system through an analytical model and examine perturbations to the model through simulations. Our findings have several notable implications for regulators. First, calls to limit algorithms from conditioning on competitors' prices are insufficient to prevent algorithmic collusion. This is a direct result of collusion arising even in the naive setting. Second, symmetry in algorithms can increase collusion potential. This highlights a new, simple mechanism for "hub-and-spoke" algorithmic collusion. A central distributor need not imbue its algorithm with supra-competitive tendencies for apparent collusion to arise; it can simply arise by using certain (common) machine learning algorithms. Finally, we highlight that collusive outcomes depend starkly on the specific algorithm being used, and we highlight market and algorithmic conditions under which it will be unknown a priori whether collusion occurs.},
    urldate = {2025-03-10},
    publisher = {arXiv},
    author = {Douglas, Connor and Provost, Foster and Sundararajan, Arun},
    month = nov,
    year = {2024},
    note = {arXiv:2411.16574 [econ]},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Multiagent Systems, Economics - General Economics, Epsillon, Important, Mars 2025, Multi-agent, Quantitative Finance - Economics},
}
@article{puranik_long-term_2024,
    title = {Long-{Term} {Fairness} in {Sequential} {Multi}-{Agent} {Selection} with {Positive} {Reinforcement}},
    volume = {5},
    issn = {2641-8770},
    url = {http://arxiv.org/abs/2407.07350},
    doi = {10.1109/JSAIT.2024.3416078},
    abstract = {While much of the rapidly growing literature on fair decision-making focuses on metrics for one-shot decisions, recent work has raised the intriguing possibility of designing sequential decision-making to positively impact long-term social fairness. In selection processes such as college admissions or hiring, biasing slightly towards applicants from under-represented groups is hypothesized to provide positive feedback that increases the pool of under-represented applicants in future selection rounds, thus enhancing fairness in the long term. In this paper, we examine this hypothesis and its consequences in a setting in which multiple agents are selecting from a common pool of applicants. We propose the Multi-agent Fair-Greedy policy, that balances greedy score maximization and fairness. Under this policy, we prove that the resource pool and the admissions converge to a long-term fairness target set by the agents when the score distributions across the groups in the population are identical. We provide empirical evidence of existence of equilibria under non-identical score distributions through synthetic and adapted real-world datasets. We then sound a cautionary note for more complex applicant pool evolution models, under which uncoordinated behavior by the agents can cause negative reinforcement, leading to a reduction in the fraction of under-represented applicants. Our results indicate that, while positive reinforcement is a promising mechanism for long-term fairness, policies must be designed carefully to be robust to variations in the evolution model, with a number of open issues that remain to be explored by algorithm designers, social scientists, and policymakers.},
    urldate = {2025-03-09},
    journal = {IEEE Journal on Selected Areas in Information Theory},
    author = {Puranik, Bhagyashree and Guldogan, Ozgur and Madhow, Upamanyu and Pedarsani, Ramtin},
    year = {2024},
    note = {arXiv:2407.07350 [stat]},
    keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Feedback loop, Mars 2025, Multi-agent, Statistics - Machine Learning},
    pages = {424--441},
}
@article{calvano_artificial_2020,
    title = {Artificial {Intelligence}, {Algorithmic} {Pricing}, and {Collusion}},
    volume = {110},
    issn = {0002-8282},
    url = {https://pubs.aeaweb.org/doi/10.1257/aer.20190623},
    doi = {10.1257/aer.20190623},
    abstract = {Increasingly, algorithms are supplanting human decision-makers in pricing goods and services. To analyze the possible consequences, we study experimentally the behavior of algorithms powered by Artificial Intelligence (Q-learning) in a workhorse oligopoly model of repeated price competition. We find that the algorithms consistently learn to charge supracompetitive prices, without communicating with one another. The high prices are sustained by collusive strategies with a finite phase of punishment followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand, changes in the number of players, and various forms of uncertainty. (JEL D21, D43, D83, L12, L13)},
    language = {en},
    number = {10},
    urldate = {2025-03-19},
    journal = {American Economic Review},
    author = {Calvano, Emilio and Calzolari, Giacomo and Denicolò, Vincenzo and Pastorello, Sergio},
    month = oct,
    year = {2020},
    keywords = {Collusion, Mars 2025, Q-learning},
    pages = {3267--3297},
}
@article{brown_competition_2023,
    title = {Competition in {Pricing} {Algorithms}},
    volume = {15},
    issn = {1945-7669, 1945-7685},
    url = {https://pubs.aeaweb.org/doi/10.1257/mic.20210158},
    doi = {10.1257/mic.20210158},
    abstract = {We document new facts about pricing technology using high-frequency data, and we examine the implications for competition. Some online retailers employ technology that allows for more frequent price changes and automated responses to price changes by rivals. Motivated by these facts, we consider a model in which firms can differ in pricing frequency and choose pricing algorithms that are a function of rivals’ prices. In competitive (Markov perfect) equilibrium, the introduction of simple pricing algorithms can increase price levels, generate price dispersion, and exacerbate the price effects of mergers. (JEL D21, D22, D43, G34, L13, L81)},
    language = {en},
    number = {2},
    urldate = {2025-03-19},
    journal = {American Economic Journal: Microeconomics},
    author = {Brown, Zach Y. and MacKay, Alexander},
    month = may,
    year = {2023},
    keywords = {Mars 2025},
    pages = {109--156},
}
@article{panait_cooperative_2005,
    title = {Cooperative {Multi}-{Agent} {Learning}: {The} {State} of the {Art}},
    volume = {11},
    copyright = {http://www.springer.com/tdm},
    issn = {1387-2532, 1573-7454},
    shorttitle = {Cooperative {Multi}-{Agent} {Learning}},
    url = {http://link.springer.com/10.1007/s10458-005-2631-2},
    doi = {10.1007/s10458-005-2631-2},
    abstract = {Cooperative multi-agent systems are ones in which several agents attempt, through their interaction, to jointly solve tasks or to maximize utility. Due to the interactions among the agents, multi-agent problem complexity can rise rapidly with the number of agents or their behavioral sophistication. The challenge this presents to the task of programming solutions to multi-agent systems problems has spawned increasing interest in machine learning techniques to automate the search and optimization process.},
    language = {en},
    number = {3},
    urldate = {2025-03-19},
    journal = {Autonomous Agents and Multi-Agent Systems},
    author = {Panait, Liviu and Luke, Sean},
    month = nov,
    year = {2005},
    pages = {387--434},
}
@article{laurent_world_2011,
    title = {The world of {Independent} learners is not {Markovian}.},
    volume = {15},
    url = {https://hal.science/hal-00601941},
    doi = {10.3233/KES-2010-0206},
    abstract = {In multi-agent systems, the presence of learning agents can cause the environment to be non-Markovian from an agent's perspective thus violat- ing the property that traditional single-agent learning methods rely upon. This paper formalizes some known intuition about concurrently learning agents by providing formal conditions that make the environment non- Markovian from an independent (non-communicative) learner's perspec- tive. New concepts are introduced like the divergent learning paths and the observability of the e ects of others' actions. To illustrate the formal concepts, a case study is also presented. These ndings are signi cant because they both help to understand failures and successes of existing learning algorithms as well as being suggestive for future work.},
    number = {1},
    urldate = {2025-03-16},
    journal = {International Journal of Knowledge-Based and Intelligent Engineering Systems},
    author = {Laurent, Guillaume J. and Matignon, Laëtitia and Le Fort-Piat, Nadine},
    month = mar,
    year = {2011},
    note = {Publisher: IOS Press},
    keywords = {Machine Learning, Markov game, Mars 2025, Multi-Agent System, Reinforcement Learning, Reinforcement Learning., fee, import, local learner},
    pages = {55--64},
}
@misc{legacci_no-regret_2024,
    title = {No-regret learning in harmonic games: {Extrapolation} in the face of conflicting interests},
    shorttitle = {No-regret learning in harmonic games},
    url = {http://arxiv.org/abs/2412.20203},
    doi = {10.48550/arXiv.2412.20203},
    abstract = {The long-run behavior of multi-agent learning - and, in particular, no-regret learning - is relatively well-understood in potential games, where players have aligned interests. By contrast, in harmonic games - the strategic counterpart of potential games, where players have conflicting interests - very little is known outside the narrow subclass of 2-player zero-sum games with a fully-mixed equilibrium. Our paper seeks to partially fill this gap by focusing on the full class of (generalized) harmonic games and examining the convergence properties of follow-the-regularized-leader (FTRL), the most widely studied class of no-regret learning schemes. As a first result, we show that the continuous-time dynamics of FTRL are Poincar{\textbackslash}'e recurrent, that is, they return arbitrarily close to their starting point infinitely often, and hence fail to converge. In discrete time, the standard, "vanilla" implementation of FTRL may lead to even worse outcomes, eventually trapping the players in a perpetual cycle of best-responses. However, if FTRL is augmented with a suitable extrapolation step - which includes as special cases the optimistic and mirror-prox variants of FTRL - we show that learning converges to a Nash equilibrium from any initial condition, and all players are guaranteed at most O(1) regret. These results provide an in-depth understanding of no-regret learning in harmonic games, nesting prior work on 2-player zero-sum games, and showing at a high level that harmonic games are the canonical complement of potential games, not only from a strategic, but also from a dynamic viewpoint.},
    urldate = {2025-02-12},
    publisher = {arXiv},
    author = {Legacci, Davide and Mertikopoulos, Panayotis and Papadimitriou, Christos H. and Piliouras, Georgios and Pradelski, Bary S. R.},
    month = dec,
    year = {2024},
    note = {arXiv:2412.20203 [cs]},
    keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Cycle, Exploration, Harmonic game, Important, Mathematics - Optimization and Control, Poincaré récurrent},
}