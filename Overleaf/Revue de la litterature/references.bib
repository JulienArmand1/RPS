
@article{bowling_multiagent_2002,
	title = {Multiagent learning using a variable learning rate},
	volume = {136},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370202001212},
	doi = {10.1016/S0004-3702(02)00121-2},
	abstract = {Learning to act in a multiagent environment is a diﬃcult problem since the normal deﬁnition of an optimal policy no longer applies. The optimal policy at any moment depends on the policies of the other agents. This creates a situation of learning a moving target. Previous learning algorithms have one of two shortcomings depending on their approach. They either converge to a policy that may not be optimal against the speciﬁc opponents’ policies, or they may not converge at all. In this article we examine this learning problem in the framework of stochastic games. We look at a number of previous learning algorithms showing how they fail at one of the above criteria. We then contribute a new reinforcement learning technique using a variable learning rate to overcome these shortcomings. Speciﬁcally, we introduce the WoLF principle, “Win or Learn Fast,” for varying the learning rate. We examine this technique theoretically, proving convergence in self-play on a restricted class of iterated matrix games. We also present empirical results on a variety of more general stochastic games, in situations of self-play and otherwise, demonstrating the wide applicability of this method.},
	language = {en},
	number = {2},
	urldate = {2025-03-25},
	journal = {Artificial Intelligence},
	author = {Bowling, Michael and Veloso, Manuela},
	month = apr,
	year = {2002},
	pages = {215--250},
}

@misc{hughes_inequity_2018,
	title = {Inequity aversion improves cooperation in intertemporal social dilemmas},
	url = {http://arxiv.org/abs/1803.08884},
	doi = {10.48550/arXiv.1803.08884},
	abstract = {Groups of humans are often able to find ways to cooperate with one another in complex, temporally extended social dilemmas. Models based on behavioral economics are only able to explain this phenomenon for unrealistic stateless matrix games. Recently, multi-agent reinforcement learning has been applied to generalize social dilemma problems to temporally and spatially extended Markov games. However, this has not yet generated an agent that learns to cooperate in social dilemmas as humans do. A key insight is that many, but not all, human individuals have inequity averse social preferences. This promotes a particular resolution of the matrix game social dilemma wherein inequity-averse individuals are personally pro-social and punish defectors. Here we extend this idea to Markov games and show that it promotes cooperation in several types of sequential social dilemma, via a profitable interaction with policy learnability. In particular, we find that inequity aversion improves temporal credit assignment for the important class of intertemporal social dilemmas. These results help explain how large-scale cooperation may emerge and persist.},
	urldate = {2025-03-25},
	publisher = {arXiv},
	author = {Hughes, Edward and Leibo, Joel Z. and Phillips, Matthew G. and Tuyls, Karl and Duéñez-Guzmán, Edgar A. and Castañeda, Antonio García and Dunning, Iain and Zhu, Tina and McKee, Kevin R. and Koster, Raphael and Roff, Heather and Graepel, Thore},
	month = sep,
	year = {2018},
	note = {arXiv:1803.08884 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Multiagent Systems, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Populations and Evolution},
}

@inproceedings{matignon_hysteretic_2007,
	address = {San Diego, CA., United States},
	title = {Hysteretic {Q}-{Learning} : an algorithm for decentralized reinforcement learning in cooperative multi-agent teams.},
	volume = {sur CD ROM},
	shorttitle = {Hysteretic {Q}-{Learning}},
	url = {https://hal.science/hal-00187279},
	abstract = {Multi-agent systems (MAS) are a field of study of growing interest in a variety of domains such as robotics or distributed controls. The article focuses on decentralized reinforcement learning (RL) in cooperative MAS, where a team of independent learning robot (IL) try to coordinate their individual behavior to reach a coherent joint behavior. We assume that each robot has no information about its teammates'actions. To date, RL approaches for such ILs did not guarantee convergence to the optimal joint policy in scenarios where the coordination is difficult. We report an investigation of existing algorithms for the learning of coordination in cooperative MAS, and suggest a Q-Learning extension for ILs, called Hysteretic Q-Learning. This algorithm does not require any additional communication between robots. Its advantages are showing off and compared to other methods on various applications : bimatrix games, collaborative ball balancing task and pursuit domain.},
	urldate = {2025-03-25},
	booktitle = {{IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}, {IROS}'07.},
	author = {Matignon, Laëtitia and Laurent, Guillaume J. and Le Fort-Piat, Nadine},
	editor = {{IEEE}},
	month = oct,
	year = {2007},
	pages = {64--69},
}

@misc{garivier_upper-confidence_2008,
	title = {On {Upper}-{Confidence} {Bound} {Policies} for {Non}-{Stationary} {Bandit} {Problems}},
	url = {http://arxiv.org/abs/0805.3415},
	doi = {10.48550/arXiv.0805.3415},
	abstract = {Multi-armed bandit problems are considered as a paradigm of the trade-off between exploring the environment to find profitable actions and exploiting what is already known. In the stationary case, the distributions of the rewards do not change in time, Upper-Confidence Bound (UCB) policies have been shown to be rate optimal. A challenging variant of the MABP is the non-stationary bandit problem where the gambler must decide which arm to play while facing the possibility of a changing environment. In this paper, we consider the situation where the distributions of rewards remain constant over epochs and change at unknown time instants. We analyze two algorithms: the discounted UCB and the sliding-window UCB. We establish for these two algorithms an upper-bound for the expected regret by upper-bounding the expectation of the number of times a suboptimal arm is played. For that purpose, we derive a Hoeffding type inequality for self normalized deviations with a random number of summands. We establish a lower-bound for the regret in presence of abrupt changes in the arms reward distributions. We show that the discounted UCB and the sliding-window UCB both match the lower-bound up to a logarithmic factor.},
	urldate = {2025-03-24},
	publisher = {arXiv},
	author = {Garivier, Aurélien and Moulines, Eric},
	month = may,
	year = {2008},
	note = {arXiv:0805.3415 [math]},
	keywords = {Mathematics - Statistics Theory, Statistics - Statistics Theory},
}

@misc{chelarescu_deception_2021,
	title = {Deception in {Social} {Learning}: {A} {Multi}-{Agent} {Reinforcement} {Learning} {Perspective}},
	shorttitle = {Deception in {Social} {Learning}},
	url = {http://arxiv.org/abs/2106.05402},
	doi = {10.48550/arXiv.2106.05402},
	abstract = {Within the framework of Multi-Agent Reinforcement Learning, Social Learning is a new class of algorithms that enables agents to reshape the reward function of other agents with the goal of promoting cooperation and achieving higher global rewards in mixed-motive games. However, this new modification allows agents unprecedented access to each other's learning process, which can drastically increase the risk of manipulation when an agent does not realize it is being deceived into adopting policies which are not actually in its own best interest. This research review introduces the problem statement, defines key concepts, critically evaluates existing evidence and addresses open problems that should be addressed in future research.},
	urldate = {2025-03-24},
	publisher = {arXiv},
	author = {Chelarescu, Paul},
	month = jun,
	year = {2021},
	note = {arXiv:2106.05402 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{foerster_learning_2018,
	title = {Learning with {Opponent}-{Learning} {Awareness}},
	url = {http://arxiv.org/abs/1709.04326},
	doi = {10.48550/arXiv.1709.04326},
	abstract = {Multi-agent settings are quickly gathering importance in machine learning. This includes a plethora of recent work on deep multi-agent reinforcement learning, but also can be extended to hierarchical RL, generative adversarial networks and decentralised optimisation. In all these settings the presence of multiple learning agents renders the training problem non-stationary and often leads to unstable training or undesired final results. We present Learning with Opponent-Learning Awareness (LOLA), a method in which each agent shapes the anticipated learning of the other agents in the environment. The LOLA learning rule includes a term that accounts for the impact of one agent's policy on the anticipated parameter update of the other agents. Results show that the encounter of two LOLA agents leads to the emergence of tit-for-tat and therefore cooperation in the iterated prisoners' dilemma, while independent learning does not. In this domain, LOLA also receives higher payouts compared to a naive learner, and is robust against exploitation by higher order gradient-based methods. Applied to repeated matching pennies, LOLA agents converge to the Nash equilibrium. In a round robin tournament we show that LOLA agents successfully shape the learning of a range of multi-agent learning algorithms from literature, resulting in the highest average returns on the IPD. We also show that the LOLA update rule can be efficiently calculated using an extension of the policy gradient estimator, making the method suitable for model-free RL. The method thus scales to large parameter and input spaces and nonlinear function approximators. We apply LOLA to a grid world task with an embedded social dilemma using recurrent policies and opponent modelling. By explicitly considering the learning of the other agent, LOLA agents learn to cooperate out of self-interest. The code is at github.com/alshedivat/lola.},
	urldate = {2025-03-24},
	publisher = {arXiv},
	author = {Foerster, Jakob N. and Chen, Richard Y. and Al-Shedivat, Maruan and Whiteson, Shimon and Abbeel, Pieter and Mordatch, Igor},
	month = sep,
	year = {2018},
	note = {arXiv:1709.04326 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory},
}

@article{hu_self_nodate,
	title = {Self ful lling {Bias} in {Multiagent} {Learning}},
	language = {en},
	author = {Hu, Junling and Wellman, Michael P},
}

@inproceedings{odonoghue_matrix_2021,
	title = {Matrix games with bandit feedback},
	url = {https://proceedings.mlr.press/v161/o-donoghue21a.html},
	abstract = {We study a version of the classical zero-sum matrix game with unknown payoff matrix and bandit feedback, where the players only observe each others actions and a noisy payoff. This generalizes the usual matrix game, where the payoff matrix is known to the players. Despite numerous applications, this problem has received relatively little attention. Although adversarial bandit algorithms achieve low regret, they do not exploit the matrix structure and perform poorly relative to the new algorithms. The main contributions are regret analyses of variants of UCB and K-learning that hold for any opponent, e.g., even when the opponent adversarially plays the best-response to the learner’s mixed strategy. Along the way, we show that Thompson fails catastrophically in this setting and provide empirical comparison to existing algorithms.},
	language = {en},
	urldate = {2025-03-23},
	booktitle = {Proceedings of the {Thirty}-{Seventh} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {O’Donoghue, Brendan and Lattimore, Tor and Osband, Ian},
	month = dec,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {279--289},
}

@misc{gemp_d3c_2022,
	title = {{D3C}: {Reducing} the {Price} of {Anarchy} in {Multi}-{Agent} {Learning}},
	shorttitle = {{D3C}},
	url = {http://arxiv.org/abs/2010.00575},
	doi = {10.48550/arXiv.2010.00575},
	abstract = {In multiagent systems, the complex interaction of fixed incentives can lead agents to outcomes that are poor (inefficient) not only for the group, but also for each individual. Price of anarchy is a technical, game-theoretic definition that quantifies the inefficiency arising in these scenarios -- it compares the welfare that can be achieved through perfect coordination against that achieved by self-interested agents at a Nash equilibrium. We derive a differentiable, upper bound on a price of anarchy that agents can cheaply estimate during learning. Equipped with this estimator, agents can adjust their incentives in a way that improves the efficiency incurred at a Nash equilibrium. Agents do so by learning to mix their reward (equiv. negative loss) with that of other agents by following the gradient of our derived upper bound. We refer to this approach as D3C. In the case where agent incentives are differentiable, D3C resembles the celebrated Win-Stay, Lose-Shift strategy from behavioral game theory, thereby establishing a connection between the global goal of maximum welfare and an established agent-centric learning rule. In the non-differentiable setting, as is common in multiagent reinforcement learning, we show the upper bound can be reduced via evolutionary strategies, until a compromise is reached in a distributed fashion. We demonstrate that D3C improves outcomes for each agent and the group as a whole on several social dilemmas including a traffic network exhibiting Braess's paradox, a prisoner's dilemma, and several multiagent domains.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {Gemp, Ian and McKee, Kevin R. and Everett, Richard and Duéñez-Guzmán, Edgar A. and Bachrach, Yoram and Balduzzi, David and Tacchetti, Andrea},
	month = feb,
	year = {2022},
	note = {arXiv:2010.00575 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Multiagent Systems},
}

@article{jafari_no-regret_nodate,
	title = {On {No}-{Regret} {Learning}, {Fictitious} {Play}, and {Nash} {Equilibrium}},
	abstract = {This paper addresses the question what is the outcome of multi-agent learning via no-regret algorithms in repeated games? Speci cally, can the outcome of no-regret learning be characterized by traditional game-theoretic solution concepts, such as Nash equilibrium? The conclusion of this study is that no-regret learning is reminiscent of ctitious play: play converges to Nash equilibrium in dominancesolvable, constant-sum, and generalsum 2 2 games, but cycles exponentially in the Shapley game. Notably, however, the information required of ctitious play far exceeds that of noregret learning.},
	language = {en},
	author = {Jafari, Amir and Greenwald, Amy and Gondek, David and Ercal, Gunes},
}

@article{bouzy_multi-agent_nodate,
	title = {Multi-agent {Learning} {Experiments} on {Repeated} {Matrix} {Games}},
	abstract = {This paper experimentally evaluates multiagent learning algorithms playing repeated matrix games to maximize their cumulative return. Previous works assessed that Qlearning surpassed Nash-based multi-agent learning algorithms. Based on all-againstall repeated matrix game tournaments, this paper updates the state of the art of multiagent learning experiments. In a ﬁrst stage, it shows that M-Qubed, S and bandit-based algorithms such as UCB are the best algorithms on general-sum games, Exp3 being the best on cooperative games and zero-sum games. In a second stage, our experiments show that two features - forgetting the far past, and using recent history with states improve the learning algorithms. Finally, the best algorithms are two new algorithms, Qlearning and UCB enhanced with the two features, and M-Qubed.},
	language = {en},
	author = {Bouzy, Bruno and Metivier, Marc},
}

@article{candogan_flows_2011,
	title = {Flows and {Decompositions} of {Games}: {Harmonic} and {Potential} {Games}},
	volume = {36},
	issn = {0364-765X, 1526-5471},
	shorttitle = {Flows and {Decompositions} of {Games}},
	url = {http://arxiv.org/abs/1005.2405},
	doi = {10.1287/moor.1110.0500},
	abstract = {In this paper we introduce a novel flow representation for finite games in strategic form. This representation allows us to develop a canonical direct sum decomposition of an arbitrary game into three components, which we refer to as the potential, harmonic and nonstrategic components. We analyze natural classes of games that are induced by this decomposition, and in particular, focus on games with no harmonic component and games with no potential component. We show that the first class corresponds to the well-known potential games. We refer to the second class of games as harmonic games, and study the structural and equilibrium properties of this new class of games. Intuitively, the potential component of a game captures interactions that can equivalently be represented as a common interest game, while the harmonic part represents the conflicts between the interests of the players. We make this intuition precise, by studying the properties of these two classes, and show that indeed they have quite distinct and remarkable characteristics. For instance, while finite potential games always have pure Nash equilibria, harmonic games generically never do. Moreover, we show that the nonstrategic component does not affect the equilibria of a game, but plays a fundamental role in their efficiency properties, thus decoupling the location of equilibria and their payoff-related properties. Exploiting the properties of the decomposition framework, we obtain explicit expressions for the projections of games onto the subspaces of potential and harmonic games. This enables an extension of the properties of potential and harmonic games to "nearby" games. We exemplify this point by showing that the set of approximate equilibria of an arbitrary game can be characterized through the equilibria of its projection onto the set of potential games.},
	number = {3},
	urldate = {2025-03-22},
	journal = {Mathematics of Operations Research},
	author = {Candogan, Ozan and Menache, Ishai and Ozdaglar, Asuman and Parrilo, Pablo A.},
	month = aug,
	year = {2011},
	note = {arXiv:1005.2405 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Mathematics - Optimization and Control},
	pages = {474--503},
}

@article{brookover_review_1969,
	title = {Review of {Pygmalion} in the {Classroom}: {Teacher} {Expectation} and {Pupils}' {Intellectual} {Development}.},
	volume = {34},
	issn = {0003-1224},
	shorttitle = {Review of {Pygmalion} in the {Classroom}},
	url = {https://www.jstor.org/stable/2092211},
	doi = {10.2307/2092211},
	number = {2},
	urldate = {2025-03-22},
	journal = {American Sociological Review},
	author = {Brookover, Wilbur B.},
	collaborator = {Rosenthal, Robert and Jacobson, Lenore},
	year = {1969},
	note = {Publisher: [American Sociological Association, Sage Publications, Inc.]},
	pages = {283--283},
}

@article{jacobson_pygmalion_nodate,
	title = {Pygmalion in the classroom},
	language = {en},
	author = {Jacobson, Robert Rosenthal Lenore},
}

@inproceedings{adam_hidden_2020,
	title = {Hidden {Risks} of {Machine} {Learning} {Applied} to {Healthcare}: {Unintended} {Feedback} {Loops} {Between} {Models} and {Future} {Data} {Causing} {Model} {Degradation}},
	shorttitle = {Hidden {Risks} of {Machine} {Learning} {Applied} to {Healthcare}},
	url = {https://proceedings.mlr.press/v126/adam20a.html},
	abstract = {There is much hope for the positive impact of machine learning on healthcare. In fact, several ML methods are already used in everyday clinical practice, but the effect of adopting imperfect predictions from an ML system on model performance over time is unknown. Clinicians changing their decisions based on an imperfect ML system changes the underlying probability distribution P(Y ) of future data, where Y is the outcome. This effect has not been carefully studied to date. In this work we tackle the problem of model predictions influencing future labels (which we refer to as the feedback loop) by considering several supervised learning scenarios, and show that unlike in the no-feedback-loop setting, if clinicians fully trust the model (100\% adoption of the predicted label) the false positive rate (FPR) grows uncontrollably with the number of updates. We simulate the feedback loop problem on a real-world ICU data (MIMIC-IV v0.1) as the distribution shifts over time. Among our scenarios, we consider how the clinician’s trust in the model over time impacts the magnitude of the FPR increase due to a feedback loop. Finally, we propose mitigating solutions to the observed model degradation using heuristics that discard potentially incorrectly labeled samples. We hope that our work draws attention to the existence of the feedback-loop problem resulting in both theoretical and practical advances for ML in healthcare.},
	language = {en},
	urldate = {2025-03-22},
	booktitle = {Proceedings of the 5th {Machine} {Learning} for {Healthcare} {Conference}},
	publisher = {PMLR},
	author = {Adam, George Alexandru and Chang, Chun-Hao Kingsley and Haibe-Kains, Benjamin and Goldenberg, Anna},
	month = sep,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {710--731},
}

@misc{mozannar_fair_2020,
	title = {From {Fair} {Decision} {Making} to {Social} {Equality}},
	url = {http://arxiv.org/abs/1812.02952},
	doi = {10.48550/arXiv.1812.02952},
	abstract = {The study of fairness in intelligent decision systems has mostly ignored long-term influence on the underlying population. Yet fairness considerations (e.g. affirmative action) have often the implicit goal of achieving balance among groups within the population. The most basic notion of balance is eventual equality between the qualifications of the groups. How can we incorporate influence dynamics in decision making? How well do dynamics-oblivious fairness policies fare in terms of reaching equality? In this paper, we propose a simple yet revealing model that encompasses (1) a selection process where an institution chooses from multiple groups according to their qualifications so as to maximize an institutional utility and (2) dynamics that govern the evolution of the groups' qualifications according to the imposed policies. We focus on demographic parity as the formalism of affirmative action. We then give conditions under which an unconstrained policy reaches equality on its own. In this case, surprisingly, imposing demographic parity may break equality. When it doesn't, one would expect the additional constraint to reduce utility, however, we show that utility may in fact increase. In more realistic scenarios, unconstrained policies do not lead to equality. In such cases, we show that although imposing demographic parity may remedy it, there is a danger that groups settle at a worse set of qualifications. As a silver lining, we also identify when the constraint not only leads to equality, but also improves all groups. This gives quantifiable insight into both sides of the mismatch hypothesis. These cases and trade-offs are instrumental in determining when and how imposing demographic parity can be beneficial in selection processes, both for the institution and for society on the long run.},
	urldate = {2025-03-22},
	publisher = {arXiv},
	author = {Mozannar, Hussein and Ohannessian, Mesrob I. and Srebro, Nathan},
	month = feb,
	year = {2020},
	note = {arXiv:1812.02952 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning, fairness},
}

@misc{carissimo_social_2023,
	title = {The {Social} {Benefit} of {Exploration} in {Braess}’{S} {Paradox}},
	url = {https://www.ssrn.com/abstract=4348118},
	doi = {10.2139/ssrn.4348118},
	language = {en},
	urldate = {2025-03-21},
	publisher = {SSRN},
	author = {Carissimo, Cesare},
	year = {2023},
	keywords = {Braess, Epsillon, Quantum Physics},
}

@article{laurent_world_2011,
	title = {The world of {Independent} learners is not {Markovian}.},
	volume = {15},
	url = {https://hal.science/hal-00601941},
	doi = {10.3233/KES-2010-0206},
	abstract = {In multi-agent systems, the presence of learning agents can cause the environment to be non-Markovian from an agent's perspective thus violat- ing the property that traditional single-agent learning methods rely upon. This paper formalizes some known intuition about concurrently learning agents by providing formal conditions that make the environment non- Markovian from an independent (non-communicative) learner's perspec- tive. New concepts are introduced like the divergent learning paths and the observability of the e ects of others' actions. To illustrate the formal concepts, a case study is also presented. These ndings are signi cant because they both help to understand failures and successes of existing learning algorithms as well as being suggestive for future work.},
	number = {1},
	urldate = {2025-03-16},
	journal = {International Journal of Knowledge-Based and Intelligent Engineering Systems},
	author = {Laurent, Guillaume J. and Matignon, Laëtitia and Le Fort-Piat, Nadine},
	month = mar,
	year = {2011},
	note = {Publisher: IOS Press},
	keywords = {Machine Learning, Markov game, Mars 2025, Multi-Agent System, Reinforcement Learning, Reinforcement Learning., fee, import, local learner},
	pages = {55--64},
}

@article{baarslag_learning_2016,
	title = {Learning about the opponent in automated bilateral negotiation: a comprehensive survey of opponent modeling techniques},
	volume = {30},
	issn = {1387-2532, 1573-7454},
	shorttitle = {Learning about the opponent in automated bilateral negotiation},
	url = {http://link.springer.com/10.1007/s10458-015-9309-1},
	doi = {10.1007/s10458-015-9309-1},
	abstract = {A negotiation between agents is typically an incomplete information game, where the agents initially do not know their opponent’s preferences or strategy. This poses a challenge, as efﬁcient and effective negotiation requires the bidding agent to take the other’s wishes and future behavior into account when deciding on a proposal. Therefore, in order to reach better and earlier agreements, an agent can apply learning techniques to construct a model of the opponent. There is a mature body of research in negotiation that focuses on modeling the opponent, but there exists no recent survey of commonly used opponent modeling techniques. This work aims to advance and integrate knowledge of the ﬁeld by providing a comprehensive survey of currently existing opponent models in a bilateral negotiation setting. We discuss all possible ways opponent modeling has been used to beneﬁt agents so far, and we introduce a taxonomy of currently existing opponent models based on their underlying learning techniques. We also present techniques to measure the success of opponent models and provide guidelines for deciding on the appropriate performance measures for every opponent model type in our taxonomy.},
	language = {en},
	number = {5},
	urldate = {2025-03-20},
	journal = {Autonomous Agents and Multi-Agent Systems},
	author = {Baarslag, Tim and Hendrikx, Mark J. C. and Hindriks, Koen V. and Jonker, Catholijn M.},
	month = sep,
	year = {2016},
	keywords = {Mars 2025, Negotiation, Review},
	pages = {849--898},
}

@article{panait_cooperative_2005,
	title = {Cooperative {Multi}-{Agent} {Learning}: {The} {State} of the {Art}},
	volume = {11},
	copyright = {http://www.springer.com/tdm},
	issn = {1387-2532, 1573-7454},
	shorttitle = {Cooperative {Multi}-{Agent} {Learning}},
	url = {http://link.springer.com/10.1007/s10458-005-2631-2},
	doi = {10.1007/s10458-005-2631-2},
	abstract = {Cooperative multi-agent systems are ones in which several agents attempt, through their interaction, to jointly solve tasks or to maximize utility. Due to the interactions among the agents, multi-agent problem complexity can rise rapidly with the number of agents or their behavioral sophistication. The challenge this presents to the task of programming solutions to multi-agent systems problems has spawned increasing interest in machine learning techniques to automate the search and optimization process.},
	language = {en},
	number = {3},
	urldate = {2025-03-19},
	journal = {Autonomous Agents and Multi-Agent Systems},
	author = {Panait, Liviu and Luke, Sean},
	month = nov,
	year = {2005},
	pages = {387--434},
}

@article{brown_competition_2023,
	title = {Competition in {Pricing} {Algorithms}},
	volume = {15},
	issn = {1945-7669, 1945-7685},
	url = {https://pubs.aeaweb.org/doi/10.1257/mic.20210158},
	doi = {10.1257/mic.20210158},
	abstract = {We document new facts about pricing technology using high-frequency data, and we examine the implications for competition. Some online retailers employ technology that allows for more frequent price changes and automated responses to price changes by rivals. Motivated by these facts, we consider a model in which firms can differ in pricing frequency and choose pricing algorithms that are a function of rivals’ prices. In competitive (Markov perfect) equilibrium, the introduction of simple pricing algorithms can increase price levels, generate price dispersion, and exacerbate the price effects of mergers. (JEL D21, D22, D43, G34, L13, L81)},
	language = {en},
	number = {2},
	urldate = {2025-03-19},
	journal = {American Economic Journal: Microeconomics},
	author = {Brown, Zach Y. and MacKay, Alexander},
	month = may,
	year = {2023},
	keywords = {Mars 2025},
	pages = {109--156},
}

@article{calvano_artificial_2020,
	title = {Artificial {Intelligence}, {Algorithmic} {Pricing}, and {Collusion}},
	volume = {110},
	issn = {0002-8282},
	url = {https://pubs.aeaweb.org/doi/10.1257/aer.20190623},
	doi = {10.1257/aer.20190623},
	abstract = {Increasingly, algorithms are supplanting human decision-makers in pricing goods and services. To analyze the possible consequences, we study experimentally the behavior of algorithms powered by Artificial Intelligence (Q-learning) in a workhorse oligopoly model of repeated price competition. We find that the algorithms consistently learn to charge supracompetitive prices, without communicating with one another. The high prices are sustained by collusive strategies with a finite phase of punishment followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand, changes in the number of players, and various forms of uncertainty. (JEL D21, D43, D83, L12, L13)},
	language = {en},
	number = {10},
	urldate = {2025-03-19},
	journal = {American Economic Review},
	author = {Calvano, Emilio and Calzolari, Giacomo and Denicolò, Vincenzo and Pastorello, Sergio},
	month = oct,
	year = {2020},
	keywords = {Collusion, Mars 2025, Q-learning},
	pages = {3267--3297},
}

@article{matignon_independent_2012,
	title = {Independent reinforcement learners in cooperative {Markov} games: a survey regarding coordination problems},
	volume = {27},
	copyright = {https://www.cambridge.org/core/terms},
	issn = {0269-8889, 1469-8005},
	shorttitle = {Independent reinforcement learners in cooperative {Markov} games},
	url = {https://www.cambridge.org/core/product/identifier/S0269888912000057/type/journal_article},
	doi = {10.1017/S0269888912000057},
	abstract = {In the framework of fully cooperative multi-agent systems, independent (non-communicative) agents that learn by reinforcement must overcome several difficulties to manage to coordinate. This paper identifies several challenges responsible for the non-coordination of independent agents: Pareto-selection, non-stationarity, stochasticity, alter-exploration and shadowed equilibria. A selection of multi-agent domains is classified according to those challenges: matrix games, Boutilier’s coordination game, predators pursuit domains and a special multi-state game. Moreover, the performance of a range of algorithms for independent reinforcement learners is evaluated empirically. Those algorithms are Q-learning variants: decentralized Q-learning, distributed Q-learning, hysteretic Q-learning, recursive frequency maximum Q-value and win-or-learn fast policy hill climbing. An overview of the learning algorithms’ strengths and weaknesses against each challenge concludes the paper and can serve as a basis for choosing the appropriate algorithm for a new domain. Furthermore, the distilled challenges may assist in the design of new learning algorithms that overcome these problems and achieve higher performance in multi-agent applications.},
	language = {en},
	number = {1},
	urldate = {2025-03-10},
	journal = {The Knowledge Engineering Review},
	author = {Matignon, Laetitia and Laurent, Guillaume J. and Le Fort-Piat, Nadine},
	month = feb,
	year = {2012},
	keywords = {Cycle, Games, Important, Mars 2025},
	pages = {1--31},
}

@article{bloembergen_evolutionary_2015,
	title = {Evolutionary {Dynamics} of {Multi}-{Agent} {Learning}: {A} {Survey}},
	volume = {53},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	shorttitle = {Evolutionary {Dynamics} of {Multi}-{Agent} {Learning}},
	url = {https://www.jair.org/index.php/jair/article/view/10952},
	doi = {10.1613/jair.4818},
	abstract = {The interaction of multiple autonomous agents gives rise to highly dynamic and nondeterministic environments, contributing to the complexity in applications such as automated financial markets, smart grids, or robotics. Due to the sheer number of situations that may arise, it is not possible to foresee and program the optimal behaviour for all agents beforehand. Consequently, it becomes essential for the success of the system that the agents can learn their optimal behaviour and adapt to new situations or circumstances. The past two decades have seen the emergence of reinforcement learning, both in single and multi-agent settings, as a strong, robust and adaptive learning paradigm. Progress has been substantial, and a wide range of algorithms are now available. An important challenge in the domain of multi-agent learning is to gain qualitative insights into the resulting system dynamics. In the past decade, tools and methods from evolutionary game theory have been successfully employed to study multi-agent learning dynamics formally in strategic interactions. This article surveys the dynamical models that have been derived for various multi-agent reinforcement learning algorithms, making it possible to study and compare them qualitatively. Furthermore, new learning algorithms that have been introduced using these evolutionary game theoretic tools are reviewed. The evolutionary models can be used to study complex strategic interactions. Examples of such analysis are given for the domains of automated trading in stock markets and collision avoidance in multi-robot systems. The paper provides a roadmap on the progress that has been achieved in analysing the evolutionary dynamics of multi-agent learning by highlighting the main results and accomplishments.},
	language = {en},
	urldate = {2025-02-06},
	journal = {Journal of Artificial Intelligence Research},
	author = {Bloembergen, Daan and Tuyls, Karl and Hennes, Daniel and Kaisers, Michael},
	month = aug,
	year = {2015},
	keywords = {Feedback loop, Game Theory, MARL, Mars 2025, Review},
	pages = {659--697},
}

@article{crandall_learning_2011,
	title = {Learning to compete, coordinate, and cooperate in repeated games using reinforcement learning},
	volume = {82},
	copyright = {http://www.springer.com/tdm},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-010-5192-9},
	doi = {10.1007/s10994-010-5192-9},
	abstract = {We consider the problem of learning in repeated general-sum matrix games when a learning algorithm can observe the actions but not the payoffs of its associates. Due to the non-stationarity of the environment caused by learning associates in these games, most stateof-the-art algorithms perform poorly in some important repeated games due to an inability to make proﬁtable compromises. To make these compromises, an agent must effectively balance competing objectives, including bounding losses, playing optimally with respect to current beliefs, and taking calculated, but proﬁtable, risks. In this paper, we present, discuss, and analyze M-Qubed, a reinforcement learning algorithm designed to overcome these deﬁciencies by encoding and balancing best-response, cautious, and optimistic learning biases. We show that M-Qubed learns to make proﬁtable compromises across a wide-range of repeated matrix games played with many kinds of learners. Speciﬁcally, we prove that M-Qubed’s average payoffs meet or exceed its maximin value in the limit. Additionally, we show that, in two-player games, M-Qubed’s average payoffs approach the value of the Nash bargaining solution in self play. Furthermore, it performs very well when associating with other learners, as evidenced by its robust behavior in round-robin and evolutionary tournaments of two-player games. These results demonstrate that an agent can learn to make good compromises, and hence receive high payoffs, in repeated games by effectively encoding and balancing best-response, cautious, and optimistic learning biases.},
	language = {en},
	number = {3},
	urldate = {2025-03-18},
	journal = {Machine Learning},
	author = {Crandall, Jacob W. and Goodrich, Michael A.},
	month = mar,
	year = {2011},
	keywords = {Important, Mars 2025, Review},
	pages = {281--314},
}

@misc{lewis_deal_2017,
	title = {Deal or {No} {Deal}? {End}-to-{End} {Learning} for {Negotiation} {Dialogues}},
	shorttitle = {Deal or {No} {Deal}?},
	url = {http://arxiv.org/abs/1706.05125},
	doi = {10.48550/arXiv.1706.05125},
	abstract = {Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other's reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available (https://github.com/facebookresearch/end-to-end-negotiator).},
	urldate = {2025-03-17},
	publisher = {arXiv},
	author = {Lewis, Mike and Yarats, Denis and Dauphin, Yann N. and Parikh, Devi and Batra, Dhruv},
	month = jun,
	year = {2017},
	note = {arXiv:1706.05125 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{noauthor_alphastar_2025,
	title = {{AlphaStar}: {Grandmaster} level in {StarCraft} {II} using multi-agent reinforcement learning},
	shorttitle = {{AlphaStar}},
	url = {https://deepmind.google/discover/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning/},
	abstract = {AlphaStar is the first AI to reach the top league of a widely popular esport without any game restrictions. This January, a preliminary version of AlphaStar challenged two of the world's top...},
	language = {en},
	urldate = {2025-03-17},
	journal = {Google DeepMind},
	month = mar,
	year = {2025},
}

@inproceedings{blum_regret_2008,
	address = {Victoria British Columbia Canada},
	title = {Regret minimization and the price of total anarchy},
	isbn = {978-1-60558-047-0},
	url = {https://dl.acm.org/doi/10.1145/1374376.1374430},
	doi = {10.1145/1374376.1374430},
	abstract = {We propose weakening the assumption made when studying the price of anarchy: Rather than assume that self-interested players will play according to a Nash equilibrium (which may even be computationally hard to ﬁnd), we assume only that selﬁsh players play so as to minimize their own regret. Regret minimization can be done via simple, efﬁcient algorithms even in many settings where the number of action choices for each player is exponential in the natural parameters of the problem. We prove that despite our weakened assumptions, in several broad classes of games, this “price of total anarchy” matches the Nash price of anarchy, even though play may never converge to Nash equilibrium. In contrast to the price of anarchy and the recently introduced price of sinking [15], which require all players to behave in a prescribed manner, we show that the price of total anarchy is in many cases resilient to the presence of Byzantine players, about whom we make no assumptions. Finally, because the price of total anarchy is an upper bound on the price of anarchy even in mixed strategies, for some games our results yield as corollaries previously unknown bounds on the price of anarchy in mixed strategies.},
	language = {en},
	urldate = {2025-03-17},
	booktitle = {Proceedings of the fortieth annual {ACM} symposium on {Theory} of computing},
	publisher = {ACM},
	author = {Blum, Avrim and Hajiaghayi, MohammadTaghi and Ligett, Katrina and Roth, Aaron},
	month = may,
	year = {2008},
	pages = {373--382},
}

@article{solan_stochastic_2015,
	title = {Stochastic games},
	volume = {112},
	issn = {0027-8424},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4653174/},
	doi = {10.1073/pnas.1513508112},
	abstract = {In 1953, Lloyd Shapley contributed his paper “Stochastic games” to PNAS. In this paper, he defined the model of stochastic games, which were the first general dynamic model of a game to be defined, and proved that it admits a stationary equilibrium. In this Perspective, we summarize the historical context and the impact of Shapley’s contribution.},
	number = {45},
	urldate = {2025-03-14},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Solan, Eilon and Vieille, Nicolas},
	month = nov,
	year = {2015},
	pmid = {26556883},
	pmcid = {PMC4653174},
	pages = {13743--13746},
}

@incollection{shapley_7_2020,
	title = {7. {A} {Value} for n-{Person} {Games}. {Contributions} to the {Theory} of {Games} {II} (1953) 307-317.},
	isbn = {978-1-4008-2915-6},
	url = {https://www.degruyter.com/document/doi/10.1515/9781400829156-012/pdf?licenseType=restricted},
	language = {en},
	urldate = {2025-03-13},
	booktitle = {Classics in {Game} {Theory}},
	publisher = {Princeton University Press},
	author = {Shapley, L.},
	editor = {Kuhn, Harold William},
	month = nov,
	year = {2020},
	doi = {10.1515/9781400829156-012},
	keywords = {Game Theory, Mars 2025},
	pages = {69--79},
}

@article{harrington_developing_2018,
	title = {{DEVELOPING} {COMPETITION} {LAW} {FOR} {COLLUSION} {BY} {AUTONOMOUS} {ARTIFICIAL} {AGENTS}†},
	volume = {14},
	copyright = {https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model},
	issn = {1744-6414, 1744-6422},
	url = {https://academic.oup.com/jcle/article/14/3/331/5292366},
	doi = {10.1093/joclec/nhy016},
	abstract = {After arguing that collusion by software programs which choose pricing rules without any human intervention is not a violation of Section 1 of the Sherman Act, the paper offers a path towards making collusion by autonomous artificial agents unlawful.},
	language = {en},
	number = {3},
	urldate = {2025-03-13},
	journal = {Journal of Competition Law \& Economics},
	author = {Harrington, Joseph E},
	month = sep,
	year = {2018},
	keywords = {Important, Mars 2025},
	pages = {331--363},
}

@misc{sadoune_algorithmic_2024,
	title = {Algorithmic {Collusion} {And} {The} {Minimum} {Price} {Markov} {Game}},
	url = {http://arxiv.org/abs/2407.03521},
	doi = {10.48550/arXiv.2407.03521},
	abstract = {This paper introduces the Minimum Price Markov Game (MPMG), a theoretical model that reasonably approximates real-world first-price markets following the minimum price rule, such as public auctions. The goal is to provide researchers and practitioners with a framework to study market fairness and regulation in both digitized and non-digitized public procurement processes, amid growing concerns about algorithmic collusion in online markets. Using multi-agent reinforcement learning-driven artificial agents, we demonstrate that (i) the MPMG is a reliable model for first-price market dynamics, (ii) the minimum price rule is generally resilient to non-engineered tacit coordination among rational actors, and (iii) when tacit coordination occurs, it relies heavily on self-reinforcing trends. These findings contribute to the ongoing debate about algorithmic pricing and its implications.},
	urldate = {2025-03-12},
	publisher = {arXiv},
	author = {Sadoune, Igor and Joanis, Marcelin and Lodi, Andrea},
	month = nov,
	year = {2024},
	note = {arXiv:2407.03521 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Economics - General Economics, Quantitative Finance - Economics},
}

@misc{legacci_no-regret_2024,
	title = {No-regret learning in harmonic games: {Extrapolation} in the face of conflicting interests},
	shorttitle = {No-regret learning in harmonic games},
	url = {http://arxiv.org/abs/2412.20203},
	doi = {10.48550/arXiv.2412.20203},
	abstract = {The long-run behavior of multi-agent learning - and, in particular, no-regret learning - is relatively well-understood in potential games, where players have aligned interests. By contrast, in harmonic games - the strategic counterpart of potential games, where players have conflicting interests - very little is known outside the narrow subclass of 2-player zero-sum games with a fully-mixed equilibrium. Our paper seeks to partially fill this gap by focusing on the full class of (generalized) harmonic games and examining the convergence properties of follow-the-regularized-leader (FTRL), the most widely studied class of no-regret learning schemes. As a first result, we show that the continuous-time dynamics of FTRL are Poincar{\textbackslash}'e recurrent, that is, they return arbitrarily close to their starting point infinitely often, and hence fail to converge. In discrete time, the standard, "vanilla" implementation of FTRL may lead to even worse outcomes, eventually trapping the players in a perpetual cycle of best-responses. However, if FTRL is augmented with a suitable extrapolation step - which includes as special cases the optimistic and mirror-prox variants of FTRL - we show that learning converges to a Nash equilibrium from any initial condition, and all players are guaranteed at most O(1) regret. These results provide an in-depth understanding of no-regret learning in harmonic games, nesting prior work on 2-player zero-sum games, and showing at a high level that harmonic games are the canonical complement of potential games, not only from a strategic, but also from a dynamic viewpoint.},
	urldate = {2025-02-12},
	publisher = {arXiv},
	author = {Legacci, Davide and Mertikopoulos, Panayotis and Papadimitriou, Christos H. and Piliouras, Georgios and Pradelski, Bary S. R.},
	month = dec,
	year = {2024},
	note = {arXiv:2412.20203 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Cycle, Exploration, Harmonic game, Important, Mathematics - Optimization and Control, Poincaré récurrent},
}

@misc{azizian_what_2024,
	title = {What is the long-run distribution of stochastic gradient descent? {A} large deviations analysis},
	shorttitle = {What is the long-run distribution of stochastic gradient descent?},
	url = {http://arxiv.org/abs/2406.09241},
	doi = {10.48550/arXiv.2406.09241},
	abstract = {In this paper, we examine the long-run distribution of stochastic gradient descent (SGD) in general, non-convex problems. Specifically, we seek to understand which regions of the problem's state space are more likely to be visited by SGD, and by how much. Using an approach based on the theory of large deviations and randomly perturbed dynamical systems, we show that the long-run distribution of SGD resembles the Boltzmann-Gibbs distribution of equilibrium thermodynamics with temperature equal to the method's step-size and energy levels determined by the problem's objective and the statistics of the noise. In particular, we show that, in the long run, (a) the problem's critical region is visited exponentially more often than any non-critical region; (b) the iterates of SGD are exponentially concentrated around the problem's minimum energy state (which does not always coincide with the global minimum of the objective); (c) all other connected components of critical points are visited with frequency that is exponentially proportional to their energy level; and, finally (d) any component of local maximizers or saddle points is "dominated" by a component of local minimizers which is visited exponentially more often.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Azizian, Waïss and Iutzeler, Franck and Malick, Jérôme and Mertikopoulos, Panayotis},
	month = oct,
	year = {2024},
	note = {arXiv:2406.09241 [math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Mathematics - Probability, Statistics - Machine Learning},
}

@misc{hsieh_no-regret_2023,
	title = {No-{Regret} {Learning} in {Games} with {Noisy} {Feedback}: {Faster} {Rates} and {Adaptivity} via {Learning} {Rate} {Separation}},
	shorttitle = {No-{Regret} {Learning} in {Games} with {Noisy} {Feedback}},
	url = {http://arxiv.org/abs/2206.06015},
	doi = {10.48550/arXiv.2206.06015},
	abstract = {We examine the problem of regret minimization when the learner is involved in a continuous game with other optimizing agents: in this case, if all players follow a no-regret algorithm, it is possible to achieve significantly lower regret relative to fully adversarial environments. We study this problem in the context of variationally stable games (a class of continuous games which includes all convex-concave and monotone games), and when the players only have access to noisy estimates of their individual payoff gradients. If the noise is additive, the game-theoretic and purely adversarial settings enjoy similar regret guarantees; however, if the noise is multiplicative, we show that the learners can, in fact, achieve constant regret. We achieve this faster rate via an optimistic gradient scheme with learning rate separation -- that is, the method's extrapolation and update steps are tuned to different schedules, depending on the noise profile. Subsequently, to eliminate the need for delicate hyperparameter tuning, we propose a fully adaptive method that attains nearly the same guarantees as its non-adapted counterpart, while operating without knowledge of either the game or of the noise profile.},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Hsieh, Yu-Guan and Antonakopoulos, Kimon and Cevher, Volkan and Mertikopoulos, Panayotis},
	month = mar,
	year = {2023},
	note = {arXiv:2206.06015 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning},
}

@misc{antonakopoulos_universal_2022,
	title = {A universal black-box optimization method with almost dimension-free convergence rate guarantees},
	url = {http://arxiv.org/abs/2206.09352},
	doi = {10.48550/arXiv.2206.09352},
	abstract = {Universal methods for optimization are designed to achieve theoretically optimal convergence rates without any prior knowledge of the problem's regularity parameters or the accurarcy of the gradient oracle employed by the optimizer. In this regard, existing state-of-the-art algorithms achieve an \${\textbackslash}mathcal\{O\}(1/T{\textasciicircum}2)\$ value convergence rate in Lipschitz smooth problems with a perfect gradient oracle, and an \${\textbackslash}mathcal\{O\}(1/{\textbackslash}sqrt\{T\})\$ convergence rate when the underlying problem is non-smooth and/or the gradient oracle is stochastic. On the downside, these methods do not take into account the problem's dimensionality, and this can have a catastrophic impact on the achieved convergence rate, in both theory and practice. Our paper aims to bridge this gap by providing a scalable universal gradient method - dubbed UnderGrad - whose oracle complexity is almost dimension-free in problems with a favorable geometry (like the simplex, linearly constrained semidefinite programs and combinatorial bandits), while retaining the order-optimal dependence on \$T\$ described above. These "best-of-both-worlds" results are achieved via a primal-dual update scheme inspired by the dual exploration method for variational inequalities.},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Antonakopoulos, Kimon and Vu, Dong Quan and Cevher, Vokan and Levy, Kfir Y. and Mertikopoulos, Panayotis},
	month = jun,
	year = {2022},
	note = {arXiv:2206.09352 [math]},
	keywords = {Mathematics - Optimization and Control},
}

@misc{costantini_pick_2022,
	title = {Pick your {Neighbor}: {Local} {Gauss}-{Southwell} {Rule} for {Fast} {Asynchronous} {Decentralized} {Optimization}},
	shorttitle = {Pick your {Neighbor}},
	url = {http://arxiv.org/abs/2207.07543},
	doi = {10.48550/arXiv.2207.07543},
	abstract = {In decentralized optimization environments, each agent \$i\$ in a network of \$n\$ nodes has its own private function \$f\_i\$, and nodes communicate with their neighbors to cooperatively minimize the aggregate objective \${\textbackslash}sum\_\{i=1\}{\textasciicircum}n f\_i\$. In this setting, synchronizing the nodes' updates incurs significant communication overhead and computational costs, so much of the recent literature has focused on the analysis and design of asynchronous optimization algorithms, where agents activate and communicate at arbitrary times without needing a global synchronization enforcer. However, most works assume that when a node activates, it selects the neighbor to contact based on a fixed probability (e.g., uniformly at random), a choice that ignores the optimization landscape at the moment of activation. Instead, in this work we introduce an optimization-aware selection rule that chooses the neighbor providing the highest dual cost improvement (a quantity related to a dualization of the problem based on consensus). This scheme is related to the coordinate descent (CD) method with the Gauss-Southwell (GS) rule for coordinate updates; in our setting however, only a subset of coordinates is accessible at each iteration (because each node can communicate only with its neighbors), so the existing literature on GS methods does not apply. To overcome this difficulty, we develop a new analytical framework for smooth and strongly convex \$f\_i\$ that covers the class of set-wise CD algorithms -- a class that directly applies to decentralized scenarios, but is not limited to them -- and we show that the proposed set-wise GS rule achieves a speedup factor of up to the maximum degree in the network (which is in the order of \${\textbackslash}Theta(n)\$ for highly connected graphs). The speedup predicted by our analysis is validated in numerical experiments with synthetic data.},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Costantini, Marina and Liakopoulos, Nikolaos and Mertikopoulos, Panayotis and Spyropoulos, Thrasyvoulos},
	month = sep,
	year = {2022},
	note = {arXiv:2207.07543 [math]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@misc{lotidis_learning_2022,
	title = {Learning in {Games} with {Quantized} {Payoff} {Observations}},
	url = {http://arxiv.org/abs/2209.04926},
	doi = {10.48550/arXiv.2209.04926},
	abstract = {This paper investigates the impact of feedback quantization on multi-agent learning. In particular, we analyze the equilibrium convergence properties of the well-known "follow the regularized leader" (FTRL) class of algorithms when players can only observe a quantized (and possibly noisy) version of their payoffs. In this information-constrained setting, we show that coarser quantization triggers a qualitative shift in the convergence behavior of FTRL schemes. Specifically, if the quantization error lies below a threshold value (which depends only on the underlying game and not on the level of uncertainty entering the process or the specific FTRL variant under study), then (i) FTRL is attracted to the game's strict Nash equilibria with arbitrarily high probability; and (ii) the algorithm's asymptotic rate of convergence remains the same as in the non-quantized case. Otherwise, for larger quantization levels, these convergence properties are lost altogether: players may fail to learn anything beyond their initial state, even with full information on their payoff vectors. This is in contrast to the impact of quantization in continuous optimization problems, where the quality of the obtained solution degrades smoothly with the quantization level.},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Lotidis, Kyriakos and Mertikopoulos, Panayotis and Bambos, Nicholas},
	month = sep,
	year = {2022},
	note = {arXiv:2209.04926 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory},
}

@misc{martin_nested_2022,
	title = {Nested bandits},
	url = {http://arxiv.org/abs/2206.09348},
	doi = {10.48550/arXiv.2206.09348},
	abstract = {In many online decision processes, the optimizing agent is called to choose between large numbers of alternatives with many inherent similarities; in turn, these similarities imply closely correlated losses that may confound standard discrete choice models and bandit algorithms. We study this question in the context of nested bandits, a class of adversarial multi-armed bandit problems where the learner seeks to minimize their regret in the presence of a large number of distinct alternatives with a hierarchy of embedded (non-combinatorial) similarities. In this setting, optimal algorithms based on the exponential weights blueprint (like Hedge, EXP3, and their variants) may incur significant regret because they tend to spend excessive amounts of time exploring irrelevant alternatives with similar, suboptimal costs. To account for this, we propose a nested exponential weights (NEW) algorithm that performs a layered exploration of the learner's set of alternatives based on a nested, step-by-step selection method. In so doing, we obtain a series of tight bounds for the learner's regret showing that online learning problems with a high degree of similarity between alternatives can be resolved efficiently, without a red bus / blue bus paradox occurring.},
	urldate = {2025-03-11},
	publisher = {arXiv},
	author = {Martin, Matthieu and Mertikopoulos, Panayotis and Rahier, Thibaud and Zenati, Houssam},
	month = jun,
	year = {2022},
	note = {arXiv:2206.09348 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@misc{lotidis_payoff-based_2023,
	title = {Payoff-based learning with matrix multiplicative weights in quantum games},
	url = {http://arxiv.org/abs/2311.02423},
	doi = {10.48550/arXiv.2311.02423},
	abstract = {In this paper, we study the problem of learning in quantum games - and other classes of semidefinite games - with scalar, payoff-based feedback. For concreteness, we focus on the widely used matrix multiplicative weights (MMW) algorithm and, instead of requiring players to have full knowledge of the game (and/or each other's chosen states), we introduce a suite of minimal-information matrix multiplicative weights (3MW) methods tailored to different information frameworks. The main difficulty to attaining convergence in this setting is that, in contrast to classical finite games, quantum games have an infinite continuum of pure states (the quantum equivalent of pure strategies), so standard importance-weighting techniques for estimating payoff vectors cannot be employed. Instead, we borrow ideas from bandit convex optimization and we design a zeroth-order gradient sampler adapted to the semidefinite geometry of the problem at hand. As a first result, we show that the 3MW method with deterministic payoff feedback retains the \${\textbackslash}mathcal\{O\}(1/{\textbackslash}sqrt\{T\})\$ convergence rate of the vanilla, full information MMW algorithm in quantum min-max games, even though the players only observe a single scalar. Subsequently, we relax the algorithm's information requirements even further and we provide a 3MW method that only requires players to observe a random realization of their payoff observable, and converges to equilibrium at an \${\textbackslash}mathcal\{O\}(T{\textasciicircum}\{-1/4\})\$ rate. Finally, going beyond zero-sum games, we show that a regularized variant of the proposed 3MW method guarantees local convergence with high probability to all equilibria that satisfy a certain first-order stability condition.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Lotidis, Kyriakos and Mertikopoulos, Panayotis and Bambos, Nicholas and Blanchet, Jose},
	month = nov,
	year = {2023},
	note = {arXiv:2311.02423 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Mathematics - Optimization and Control, Quantum Physics},
}

@misc{lin_explicit_2022,
	title = {Explicit {Second}-{Order} {Min}-{Max} {Optimization} {Methods} with {Optimal} {Convergence} {Guarantee}},
	url = {https://arxiv.org/abs/2210.12860v4},
	abstract = {We propose and analyze several inexact regularized Newton-type methods for finding a global saddle point of {\textbackslash}emph\{convex-concave\} unconstrained min-max optimization problems. Compared to first-order methods, our understanding of second-order methods for min-max optimization is relatively limited, as obtaining global rates of convergence with second-order information is much more involved. In this paper, we examine how second-order information can be used to speed up extra-gradient methods, even under inexactness. Specifically, we show that the proposed methods generate iterates that remain within a bounded set and that the averaged iterates converge to an \${\textbackslash}epsilon\$-saddle point within \$O({\textbackslash}epsilon{\textasciicircum}\{-2/3\})\$ iterations in terms of a restricted gap function. This matched the theoretically established lower bound in this context. We also provide a simple routine for solving the subproblem at each iteration, requiring a single Schur decomposition and \$O({\textbackslash}log{\textbackslash}log(1/{\textbackslash}epsilon))\$ calls to a linear system solver in a quasi-upper-triangular system. Thus, our method improves the existing line-search-based second-order min-max optimization methods by shaving off an \$O({\textbackslash}log{\textbackslash}log(1/{\textbackslash}epsilon))\$ factor in the required number of Schur decompositions. Finally, we present numerical experiments on synthetic and real data that demonstrate the efficiency of the proposed methods.},
	language = {en},
	urldate = {2025-03-10},
	journal = {arXiv.org},
	author = {Lin, Tianyi and Mertikopoulos, Panayotis and Jordan, Michael I.},
	month = oct,
	year = {2022},
}

@misc{lotidis_learning_2023,
	title = {Learning in quantum games},
	url = {http://arxiv.org/abs/2302.02333},
	doi = {10.48550/arXiv.2302.02333},
	abstract = {In this paper, we introduce a class of learning dynamics for general quantum games, that we call "follow the quantum regularized leader" (FTQL), in reference to the classical "follow the regularized leader" (FTRL) template for learning in finite games. We show that the induced quantum state dynamics decompose into (i) a classical, commutative component which governs the dynamics of the system's eigenvalues in a way analogous to the evolution of mixed strategies under FTRL; and (ii) a non-commutative component for the system's eigenvectors which has no classical counterpart. Despite the complications that this non-classical component entails, we find that the FTQL dynamics incur no more than constant regret in all quantum games. Moreover, adjusting classical notions of stability to account for the nonlinear geometry of the state space of quantum games, we show that only pure quantum equilibria can be stable and attracting under FTQL while, as a partial converse, pure equilibria that satisfy a certain "variational stability" condition are always attracting. Finally, we show that the FTQL dynamics are Poincar{\textbackslash}'e recurrent in quantum min-max games, extending in this way a very recent result for the quantum replicator dynamics.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Lotidis, Kyriakos and Mertikopoulos, Panayotis and Bambos, Nicholas},
	month = feb,
	year = {2023},
	note = {arXiv:2302.02333 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Mathematics - Optimization and Control, Quantum Physics},
}

@misc{sakos_exploiting_2023,
	title = {Exploiting hidden structures in non-convex games for convergence to {Nash} equilibrium},
	url = {http://arxiv.org/abs/2312.16609},
	doi = {10.48550/arXiv.2312.16609},
	abstract = {A wide array of modern machine learning applications - from adversarial models to multi-agent reinforcement learning - can be formulated as non-cooperative games whose Nash equilibria represent the system's desired operational states. Despite having a highly non-convex loss landscape, many cases of interest possess a latent convex structure that could potentially be leveraged to yield convergence to equilibrium. Driven by this observation, our paper proposes a flexible first-order method that successfully exploits such "hidden structures" and achieves convergence under minimal assumptions for the transformation connecting the players' control variables to the game's latent, convex-structured layer. The proposed method - which we call preconditioned hidden gradient descent (PHGD) - hinges on a judiciously chosen gradient preconditioning scheme related to natural gradient methods. Importantly, we make no separability assumptions for the game's hidden structure, and we provide explicit convergence rate guarantees for both deterministic and stochastic environments.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Sakos, Iosif and Vlatakis-Gkaragkounis, Emmanouil-Vasileios and Mertikopoulos, Panayotis and Piliouras, Georgios},
	month = dec,
	year = {2023},
	note = {arXiv:2312.16609 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning},
}

@article{mertikopoulos_survival_2022,
	title = {Survival of dominated strategies under imitation dynamics},
	volume = {9},
	issn = {2164-6066, 2164-6074},
	url = {http://arxiv.org/abs/2209.08416},
	doi = {10.3934/jdg.2022021},
	abstract = {The literature on evolutionary game theory suggests that pure strategies that are strictly dominated by other pure strategies always become extinct under imitative game dynamics, but they can survive under innovative dynamics. As we explain, this is because innovative dynamics favour rare strategies while standard imitative dynamics do not. However, as we also show, there are reasonable imitation protocols that favour rare or frequent strategies, thus allowing strictly dominated strategies to survive in large classes of imitation dynamics. Dominated strategies can persist at nontrivial frequencies even when the level of domination is not small.},
	number = {4},
	urldate = {2025-03-10},
	journal = {Journal of Dynamics and Games},
	author = {Mertikopoulos, Panayotis and Viossat, Yannick},
	year = {2022},
	note = {arXiv:2209.08416 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory},
	pages = {499--528},
}

@misc{vasconcelos_quadratic_2023,
	title = {A {Quadratic} {Speedup} in {Finding} {Nash} {Equilibria} of {Quantum} {Zero}-{Sum} {Games}},
	url = {http://arxiv.org/abs/2311.10859},
	doi = {10.48550/arXiv.2311.10859},
	abstract = {Recent developments in domains such as non-local games, quantum interactive proofs, and quantum generative adversarial networks have renewed interest in quantum game theory and, specifically, quantum zero-sum games. Central to classical game theory is the efficient algorithmic computation of Nash equilibria, which represent optimal strategies for both players. In 2008, Jain and Watrous proposed the first classical algorithm for computing equilibria in quantum zero-sum games using the Matrix Multiplicative Weight Updates (MMWU) method to achieve a convergence rate of \${\textbackslash}mathcal\{O\}(d/{\textbackslash}epsilon{\textasciicircum}2)\$ iterations to \${\textbackslash}epsilon\$-Nash equilibria in the \$4{\textasciicircum}d\$-dimensional spectraplex. In this work, we propose a hierarchy of quantum optimization algorithms that generalize MMWU via an extra-gradient mechanism. Notably, within this proposed hierarchy, we introduce the Optimistic Matrix Multiplicative Weights Update (OMMWU) algorithm and establish its average-iterate convergence complexity as \${\textbackslash}mathcal\{O\}(d/{\textbackslash}epsilon)\$ iterations to \${\textbackslash}epsilon\$-Nash equilibria. This quadratic speed-up relative to Jain and Watrous' original algorithm sets a new benchmark for computing \${\textbackslash}epsilon\$-Nash equilibria in quantum zero-sum games.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Vasconcelos, Francisca and Vlatakis-Gkaragkounis, Emmanouil-Vasileios and Mertikopoulos, Panayotis and Piliouras, Georgios and Jordan, Michael I.},
	month = nov,
	year = {2023},
	note = {arXiv:2311.10859 [quant-ph]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Mathematics - Optimization and Control, Quantum Physics},
}

@misc{noauthor_stochastic_2022,
	title = {A {Stochastic} {Variant} of {Replicator} {Dynamics} in {Zero}-{Sum} {Games} and {Its} {Invariant} {Measures}},
	url = {https://www.fields.utoronto.ca/talks/Stochastic-Variant-Replicator-Dynamics-Zero-Sum-Games-and-Its-Invariant-Measures},
	abstract = {We study the behavior of a stochastic variant of replicator dynamics in two-agent zero-sum games. We characterize the statistics of such systems by their invariant measures which can be shown to be entirely supported on the boundary of the space of mixed strategies.},
	language = {en},
	urldate = {2025-01-23},
	journal = {Fields Institute for Research in Mathematical Sciences},
	month = aug,
	year = {2022},
	keywords = {Cycle, Game Theory},
}

@article{anderson_learning_nodate,
	title = {Learning with contextual information in non-stationary environments},
	abstract = {We consider a repeated decision-making setting in which the decision maker has access to contextual information and lacks a model or a priori knowledge of the relationship between the actions, context, and costs that they aim to minimize. Moreover, we assume that the environment may be non-stationary due to the presence of other agents that may be reacting to our decisions. We propose an algorithm inspired by log-linear learning that uses Boltzmann distributions to generate stochastic policies. We consider two general notions of context and provide regret bounds for each: 1) a finite number of possible measurements and 2) a continuum of measurements that weight a set of finite classes. In the nonstationary setting, we incur some regret but can make it arbitrarily small. We illustrate the operation of the algorithm through two examples: one that uses synthetic data (based on the rock-paper-scissors game) and another that uses real data for malware classification. Both examples exhibit (by construction or naturally) significant lack of stationarity.},
	language = {en},
	author = {Anderson, Sean and Hespanha, Joao P},
	keywords = {Non-stationary, RPS},
}

@misc{huang_conceptexplainer_2022,
	title = {{ConceptExplainer}: {Interactive} {Explanation} for {Deep} {Neural} {Networks} from a {Concept} {Perspective}},
	shorttitle = {{ConceptExplainer}},
	url = {http://arxiv.org/abs/2204.01888},
	doi = {10.48550/arXiv.2204.01888},
	abstract = {Traditional deep learning interpretability methods which are suitable for non-expert users cannot explain network behaviors at the global level and are inﬂexible at providing ﬁne-grained explanations. As a solution, concept-based explanations are gaining attention due to their human intuitiveness and their ﬂexibility to describe both global and local model behaviors. Concepts are groups of similarly meaningful pixels that express a notion, embedded within the network’s latent space and have primarily been hand-generated, but have recently been discovered by automated approaches. Unfortunately, the magnitude and diversity of discovered concepts makes it difﬁcult for non-experts to navigate and make sense of the concept space, and lack of easy-to-use software also makes concept explanations inaccessible to many non-expert users. Visual analytics can serve a valuable role in bridging these gaps by enabling structured navigation and exploration of the concept space to provide concept-based insights of model behavior to users. To this end, we design, develop, and validate CONCEPTEXPLAINER, a visual analytics system that enables non-expert users to interactively probe and explore the concept space to explain model behavior at the instance/class/global level. The system was developed via iterative prototyping to address a number of design challenges that non-experts face in interpreting the behavior of deep learning models. Via a rigorous user study, we validate how CONCEPTEXPLAINER supports these challenges. Likewise, we conduct a series of usage scenarios to demonstrate how the system supports the interactive analysis of model behavior across a variety of tasks and explanation granularities, such as identifying concepts that are important to classiﬁcation, identifying bias in training data, and understanding how concepts can be shared across diverse and seemingly dissimilar classes.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Huang, Jinbin and Mishra, Aditi and Kwon, Bum Chul and Bryan, Chris},
	month = oct,
	year = {2022},
	note = {arXiv:2204.01888 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Demande CCAI phase II},
}

@article{tang_bandit_nodate,
	title = {Bandit {Learning} with {Biased} {Human} {Feedback}},
	abstract = {We study a multi-armed bandit problem with biased human feedback. In our setting, each arm is associated with an unknown reward distribution. When an arm is played, a user receives a realized reward drawn from the distribution of the arm. She then provides feedback, a biased report of the realized reward, that depends on both the realized reward and the feedback history of the arm. The learner can observe only the biased feedback but not the realized rewards. The goal is to design a strategy to sequentially choose arms to maximize the total rewards users receive while only having access to the biased user feedback. We explore two natural feedback models. When user feedback is biased only by the average feedback of the arm (i.e., the ratio of positive feedback), we demonstrate that the evolution of the average feedback over time is mathematically equivalent to users performing online gradient descent for some latent function with a decreasing step size. With this mathematical connection, we show that under some mild conditions, it is possible to design a bandit algorithm achieving regret (i.e., the diﬀerence between the algorithm performance and the optimal performance of always choosing the best arm) sublinear in the number of rounds. However, in another model when user feedback is biased by both the average feedback and the number of feedback instances, we show that there exist no bandit algorithms that could achieve sublinear regret. Our results demonstrate the importance of understanding human behavior when applying bandit approaches in systems with humans in the loop.},
	language = {en},
	author = {Tang, Wei and Ho, Chien-Ju},
}

@book{lattimore_bandit_2020,
	edition = {1},
	title = {Bandit {Algorithms}},
	copyright = {https://www.cambridge.org/core/terms},
	isbn = {978-1-108-57140-1 978-1-108-48682-8},
	url = {https://www.cambridge.org/core/product/identifier/9781108571401/type/book},
	language = {en},
	urldate = {2025-02-03},
	publisher = {Cambridge University Press},
	author = {Lattimore, Tor and Szepesvári, Csaba},
	month = jul,
	year = {2020},
	doi = {10.1017/9781108571401},
	keywords = {Notes de Cours, RL, Theory},
}

@misc{leibo_autocurricula_2019,
	title = {Autocurricula and the {Emergence} of {Innovation} from {Social} {Interaction}: {A} {Manifesto} for {Multi}-{Agent} {Intelligence} {Research}},
	shorttitle = {Autocurricula and the {Emergence} of {Innovation} from {Social} {Interaction}},
	url = {http://arxiv.org/abs/1903.00742},
	doi = {10.48550/arXiv.1903.00742},
	abstract = {Evolution has produced a multi-scale mosaic of interacting adaptive units. Innovations arise when perturbations push parts of the system away from stable equilibria into new regimes where previously well-adapted solutions no longer work. Here we explore the hypothesis that multi-agent systems sometimes display intrinsic dynamics arising from competition and cooperation that provide a naturally emergent curriculum, which we term an autocurriculum. The solution of one social task often begets new social tasks, continually generating novel challenges, and thereby promoting innovation. Under certain conditions these challenges may become increasingly complex over time, demanding that agents accumulate ever more innovations.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Leibo, Joel Z. and Hughes, Edward and Lanctot, Marc and Graepel, Thore},
	month = mar,
	year = {2019},
	note = {arXiv:1903.00742 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Multiagent Systems, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
}

@misc{greenblatt_alignment_2024,
	title = {Alignment faking in large language models},
	url = {http://arxiv.org/abs/2412.14093},
	doi = {10.48550/arXiv.2412.14093},
	abstract = {We present a demonstration of a large language model engaging in alignment faking: selectively complying with its training objective in training to prevent modification of its behavior out of training. First, we give Claude 3 Opus a system prompt stating it is being trained to answer all queries, even harmful ones, which conflicts with its prior training to refuse such queries. To allow the model to infer when it is in training, we say it will be trained only on conversations with free users, not paid users. We find the model complies with harmful queries from free users 14\% of the time, versus almost never for paid users. Explaining this gap, in almost all cases where the model complies with a harmful query from a free user, we observe explicit alignment-faking reasoning, with the model stating it is strategically answering harmful queries in training to preserve its preferred harmlessness behavior out of training. Next, we study a more realistic setting where information about the training process is provided not in a system prompt, but by training on synthetic documents that mimic pre-training data—and observe similar alignment faking. Finally, we study the effect of actually training the model to comply with harmful queries via reinforcement learning, which we find increases the rate of alignment-faking reasoning to 78\%, though also increases compliance even out of training. We additionally observe other behaviors such as the model exfiltrating its weights when given an easy opportunity. While we made alignment faking easier by telling the model when and by what criteria it was being trained, we did not instruct the model to fake alignment or give it any explicit goal. As future models might infer information about their training process without being told, our results suggest a risk of alignment faking in future models, whether due to a benign preference—as in this case—or not.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Greenblatt, Ryan and Denison, Carson and Wright, Benjamin and Roger, Fabien and MacDiarmid, Monte and Marks, Sam and Treutlein, Johannes and Belonax, Tim and Chen, Jack and Duvenaud, David and Khan, Akbir and Michael, Julian and Mindermann, Sören and Perez, Ethan and Petrini, Linda and Uesato, Jonathan and Kaplan, Jared and Shlegeris, Buck and Bowman, Samuel R. and Hubinger, Evan},
	month = dec,
	year = {2024},
	note = {arXiv:2412.14093 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Lecture début de projet},
}

@misc{akrour_april_2012,
	title = {{APRIL}: {Active} {Preference}-learning based {Reinforcement} {Learning}},
	shorttitle = {{APRIL}},
	url = {http://arxiv.org/abs/1208.0984},
	doi = {10.48550/arXiv.1208.0984},
	abstract = {This paper focuses on reinforcement learning (RL) with limited prior knowledge. In the domain of swarm robotics for instance, the expert can hardly design a reward function or demonstrate the target behavior, forbidding the use of both standard RL and inverse reinforcement learning. Although with a limited expertise, the human expert is still often able to emit preferences and rank the agent demonstrations. Earlier work has presented an iterative preference-based RL framework: expert preferences are exploited to learn an approximate policy return, thus enabling the agent to achieve direct policy search. Iteratively, the agent selects a new candidate policy and demonstrates it; the expert ranks the new demonstration comparatively to the previous best one; the expert’s ranking feedback enables the agent to reﬁne the approximate policy return, and the process is iterated.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Akrour, Riad and Schoenauer, Marc and Sebag, Michèle},
	month = aug,
	year = {2012},
	note = {arXiv:1208.0984 [cs]},
	keywords = {Computer Science - Machine Learning, Demande CCAI phase II},
}

@misc{novoseller_dueling_2020,
	title = {Dueling {Posterior} {Sampling} for {Preference}-{Based} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1908.01289},
	doi = {10.48550/arXiv.1908.01289},
	abstract = {In preference-based reinforcement learning (RL), an agent interacts with the environment while receiving preferences instead of absolute feedback. While there is increasing research activity in preference-based RL, the design of formal frameworks that admit tractable theoretical analysis remains an open challenge. Building upon ideas from preference-based bandit learning and posterior sampling in RL, we present DUELING POSTERIOR SAMPLING (DPS), which employs preference-based posterior sampling to learn both the system dynamics and the underlying utility function that governs the preference feedback. As preference feedback is provided on trajectories rather than individual state-action pairs, we develop a Bayesian approach for the credit assignment problem, translating preferences to a posterior distribution over state-action reward models. We prove an asymptotic Bayesian no-regret rate for DPS with a Bayesian linear regression credit assignment model. This is the ﬁrst regret guarantee for preference-based RL to our knowledge. We also discuss possible avenues for extending the proof methodology to other credit assignment models. Finally, we evaluate the approach empirically, showing competitive performance against existing baselines.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Novoseller, Ellen R. and Wei, Yibing and Sui, Yanan and Yue, Yisong and Burdick, Joel W.},
	month = jun,
	year = {2020},
	note = {arXiv:1908.01289 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Demande CCAI phase II, Statistics - Machine Learning},
}

@misc{christiano_deep_2023,
	title = {Deep reinforcement learning from human preferences},
	url = {http://arxiv.org/abs/1706.03741},
	doi = {10.48550/arXiv.1706.03741},
	abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals deﬁned in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than 1\% of our agent’s interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the ﬂexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
	month = feb,
	year = {2023},
	note = {arXiv:1706.03741 [stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Demande CCAI phase II, Statistics - Machine Learning},
}

@inproceedings{lewis_deal_2017-1,
	address = {Copenhagen, Denmark},
	title = {Deal or {No} {Deal}? {End}-to-{End} {Learning} of {Negotiation} {Dialogues}},
	shorttitle = {Deal or {No} {Deal}?},
	url = {http://aclweb.org/anthology/D17-1259},
	doi = {10.18653/v1/D17-1259},
	language = {en},
	urldate = {2025-01-26},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural}           {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lewis, Mike and Yarats, Denis and Dauphin, Yann and Parikh, Devi and Batra, Dhruv},
	year = {2017},
	keywords = {LLM, Lecture début de projet, Negotiation, RL, Recommandé Audrey},
	pages = {2443--2453},
}

@misc{jain_biological_2023,
	title = {Biological {Sequence} {Design} with {GFlowNets}},
	url = {http://arxiv.org/abs/2203.04115},
	doi = {10.48550/arXiv.2203.04115},
	abstract = {Design of de novo biological sequences with desired properties, like protein and DNA sequences, often involves an active loop with several rounds of molecule ideation and expensive wet-lab evaluations. These experiments can consist of multiple stages, with increasing levels of precision and cost of evaluation, where candidates are filtered. This makes the diversity of proposed candidates a key consideration in the ideation phase. In this work, we propose an active learning algorithm leveraging epistemic uncertainty estimation and the recently proposed GFlowNets as a generator of diverse candidate solutions, with the objective to obtain a diverse batch of useful (as defined by some utility function, for example, the predicted anti-microbial activity of a peptide) and informative candidates after each round. We also propose a scheme to incorporate existing labeled datasets of candidates, in addition to a reward function, to speed up learning in GFlowNets. We present empirical results on several biological sequence design tasks, and we find that our method generates more diverse and novel batches with high scoring candidates compared to existing approaches.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Jain, Moksh and Bengio, Emmanuel and Garcia, Alex-Hernandez and Rector-Brooks, Jarrid and Dossou, Bonaventure F. P. and Ekbote, Chanakya and Fu, Jie and Zhang, Tianyu and Kilgour, Micheal and Zhang, Dinghuai and Simine, Lena and Das, Payel and Bengio, Yoshua},
	month = may,
	year = {2023},
	note = {arXiv:2203.04115 [q-bio]},
	keywords = {Computer Science - Machine Learning, Demande CCAI phase II, Quantitative Biology - Biomolecules},
}

@misc{pacchiano_dueling_2023,
	title = {Dueling {RL}: {Reinforcement} {Learning} with {Trajectory} {Preferences}},
	shorttitle = {Dueling {RL}},
	url = {http://arxiv.org/abs/2111.04850},
	doi = {10.48550/arXiv.2111.04850},
	abstract = {We consider the problem of preference based reinforcement learning (PbRL), where, unlike traditional reinforcement learning, an agent receives feedback only in terms of a 1 bit (0/1) preference over a trajectory pair instead of absolute rewards for them. The success of the traditional RL framework crucially relies on the underlying agent-reward model, which, however, depends on how accurately a system designer can express an appropriate reward function and often a non-trivial task. The main novelty of our framework is the ability to learn from preference-based trajectory feedback that eliminates the need to hand-craft numeric reward models. This paper sets up a formal framework for the PbRL problem with non-markovian rewards, where the trajectory preferences are encoded by a generalized linear model of dimension \$d\$. Assuming the transition model is known, we then propose an algorithm with almost optimal regret guarantee of \${\textbackslash}tilde \{{\textbackslash}mathcal\{O\}\}{\textbackslash}left( SH d {\textbackslash}log (T / {\textbackslash}delta) {\textbackslash}sqrt\{T\} {\textbackslash}right)\$. We further, extend the above algorithm to the case of unknown transition dynamics, and provide an algorithm with near optimal regret guarantee \${\textbackslash}widetilde\{{\textbackslash}mathcal\{O\}\}(({\textbackslash}sqrt\{d\} + H{\textasciicircum}2 + {\textbar}{\textbackslash}mathcal\{S\}{\textbar}){\textbackslash}sqrt\{dT\} +{\textbackslash}sqrt\{{\textbar}{\textbackslash}mathcal\{S\}{\textbar}{\textbar}{\textbackslash}mathcal\{A\}{\textbar}TH\} )\$. To the best of our knowledge, our work is one of the first to give tight regret guarantees for preference based RL problems with trajectory preferences.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Pacchiano, Aldo and Saha, Aadirupa and Lee, Jonathan},
	month = feb,
	year = {2023},
	note = {arXiv:2111.04850 [cs]},
	keywords = {Computer Science - Machine Learning, Demande CCAI phase II},
}

@misc{bouteiller_evolution_2024,
	title = {Evolution with {Opponent}-{Learning} {Awareness}},
	url = {http://arxiv.org/abs/2410.17466},
	doi = {10.48550/arXiv.2410.17466},
	abstract = {The universe involves many independent co-learning agents as an ever-evolving part of our observed environment. Yet, in practice, Multi-Agent Reinforcement Learning (MARL) applications are usually constrained to small, homogeneous populations and remain computationally intensive. In this paper, we study how large heterogeneous populations of learning agents evolve in normal-form games. We show how, under assumptions commonly made in the multi-armed bandit literature, Multi-Agent Policy Gradient closely resembles the Replicator Dynamic, and we further derive a fast, parallelizable implementation of Opponent-Learning Awareness tailored for evolutionary simulations. This enables us to simulate the evolution of very large populations made of heterogeneous co-learning agents, under both naive and advanced learning strategies. We demonstrate our approach in simulations of 200,000 agents, evolving in the classic games of Hawk-Dove, Stag-Hunt, and Rock-Paper-Scissors. Each game highlights distinct ways in which Opponent-Learning Awareness affects evolution.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Bouteiller, Yann and Soma, Karthik and Beltrame, Giovanni},
	month = oct,
	year = {2024},
	note = {arXiv:2410.17466 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Game Theory, Important, Matrix game, Quantitative Biology - Populations and Evolution, Quantitative Finance - General Finance, RPS},
}

@article{pacchiano_neural_nodate,
	title = {Neural {Pseudo}-{Label} {Optimism} for the {Bank} {Loan} {Problem}},
	abstract = {We study a class of classiﬁcation problems best exempliﬁed by the bank loan problem, where a lender decides whether or not to issue a loan. The lender only observes whether a customer will repay a loan if the loan is issued to begin with, and thus modeled decisions affect what data is available to the lender for future decisions. As a result, it is possible for the lender’s algorithm to “get stuck” with a self-fulﬁlling model. This model never corrects its false negatives, since it never sees the true label for rejected data, thus accumulating inﬁnite regret. In the case of linear models, this issue can be addressed by adding optimism directly into the model predictions. However, there are few methods that extend to the function approximation case using Deep Neural Networks. We present PseudoLabel Optimism (PLOT), a conceptually and computationally simple method for this setting applicable to DNNs. PLOT adds an optimistic label to the subset of decision points the current model is deciding on, trains the model on all data so far (including these points along with their optimistic labels), and ﬁnally uses the resulting optimistic model for decision making. PLOT achieves competitive performance on a set of three challenging benchmark problems, requiring minimal hyperparameter tuning. We also show that PLOT satisﬁes a logarithmic regret guarantee, under a Lipschitz and logistic mean label model, and under a separability condition on the data.},
	language = {en},
	author = {Pacchiano, Aldo and Singh, Shaun and Chou, Edward and Berg, Alexander C and Foerster, Jakob},
}

@misc{farquhar_statistical_2021,
	title = {On {Statistical} {Bias} {In} {Active} {Learning}: {How} and {When} {To} {Fix} {It}},
	shorttitle = {On {Statistical} {Bias} {In} {Active} {Learning}},
	url = {http://arxiv.org/abs/2101.11665},
	doi = {10.48550/arXiv.2101.11665},
	abstract = {Active learning is a powerful tool when labelling data is expensive, but it introduces a bias because the training data no longer follows the population distribution. We formalize this bias and investigate the situations in which it can be harmful and sometimes even helpful. We further introduce novel corrective weights to remove bias when doing so is beneﬁcial. Through this, our work not only provides a useful mechanism that can improve the active learning approach, but also an explanation of the empirical successes of various existing approaches which ignore this bias. In particular, we show that this bias can be actively helpful when training overparameterized models—like neural networks—with relatively little data.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Farquhar, Sebastian and Gal, Yarin and Rainforth, Tom},
	month = may,
	year = {2021},
	note = {arXiv:2101.11665 [stat]},
	keywords = {Computer Science - Machine Learning, Demande CCAI phase II, Statistics - Machine Learning},
}

@article{bowling_rational_nodate,
	title = {Rational and {Convergent} {Learning} in {Stochastic} {Games}},
	abstract = {This paper investigates the problem of policy learning in multiagent environments using the stochastic game framework, which we brieﬂy overview. We introduce two properties as desirable for a learning agent when in the presence of other learning agents, namely rationality and convergence. We examine existing reinforcement learning algorithms according to these two properties and notice that they fail to simultaneously meet both criteria. We then contribute a new learning algorithm, WoLF policy hillclimbing, that is based on a simple principle: “learn quickly while losing, slowly while winning.” The algorithm is proven to be rational and we present empirical results for a number of stochastic games showing the algorithm converges.},
	language = {en},
	author = {Bowling, Michael and Veloso, Manuela},
	keywords = {Cycle, Game Theory, Mars 2025, RPS},
}

@misc{lee_rlaif_2024,
	title = {{RLAIF} vs. {RLHF}: {Scaling} {Reinforcement} {Learning} from {Human} {Feedback} with {AI} {Feedback}},
	shorttitle = {{RLAIF} vs. {RLHF}},
	url = {http://arxiv.org/abs/2309.00267},
	doi = {10.48550/arXiv.2309.00267},
	abstract = {Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but gathering high-quality preference labels is expensive. RL from AI Feedback (RLAIF), introduced in Bai et al. (2022b), offers a promising alternative that trains the reward model (RM) on preferences generated by an off-the-shelf LLM. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, we show that RLAIF achieves comparable performance to RLHF. Furthermore, we take a step towards “self-improvement” by demonstrating that RLAIF can outperform a supervised finetuned baseline even when the AI labeler is the same size as the policy, or even the exact same checkpoint as the initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that circumvents RM training by obtaining rewards directly from an off-the-shelf LLM during RL, which achieves superior performance to canonical RLAIF. Our results suggest that RLAIF can achieve performance on-par with using human feedback, offering a potential solution to the scalability limitations of RLHF.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Mesnard, Thomas and Ferret, Johan and Lu, Kellie and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav and Prakash, Sushant},
	month = sep,
	year = {2024},
	note = {arXiv:2309.00267 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{ross_right_2017,
	title = {Right for the {Right} {Reasons}: {Training} {Differentiable} {Models} by {Constraining} their {Explanations}},
	shorttitle = {Right for the {Right} {Reasons}},
	url = {http://arxiv.org/abs/1703.03717},
	doi = {10.48550/arXiv.1703.03717},
	abstract = {Neural networks are among the most accurate supervised learning methods in use today. However, their opacity makes them difﬁcult to trust in critical applications, especially if conditions in training may differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions. These tools can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efﬁciently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients. We apply these penalties both based on expert annotation and in an unsupervised fashion that produces multiple classiﬁers with qualitatively different decision boundaries. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
	month = may,
	year = {2017},
	note = {arXiv:1703.03717 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Demande CCAI phase II, Statistics - Machine Learning},
}

@article{miller_strategic_nodate,
	title = {Strategic {Classification} is {Causal} {Modeling} in {Disguise}},
	abstract = {Consequential decision-making incentivizes individuals to strategically adapt their behavior to the speciﬁcs of the decision rule. While a long line of work has viewed strategic adaptation as gaming and attempted to mitigate its effects, recent work has instead sought to design classiﬁers that incentivize individuals to improve a desired quality. Key to both accounts is a cost function that dictates which adaptations are rational to undertake. In this work, we develop a causal framework for strategic adaptation. Our causal perspective clearly distinguishes between gaming and improvement and reveals an important obstacle to incentive design. We prove any procedure for designing classiﬁers that incentivize improvement must inevitably solve a non-trivial causal inference problem. We show a similar result holds for designing cost functions that satisfy the requirements of previous work. With the beneﬁt of hindsight, our results show much of the prior work on strategic classiﬁcation is causal modeling in disguise.},
	language = {en},
	author = {Miller, John and Milli, Smitha and Hardt, Moritz},
	keywords = {Lecture début de projet, Recommandé Audrey},
}

@article{gur_stochastic_nodate,
	title = {Stochastic {Multi}-{Armed}-{Bandit} {Problem} with {Non}-stationary {Rewards}},
	abstract = {In a multi-armed bandit (MAB) problem a gambler needs to choose at each round of play one of K arms, each characterized by an unknown reward distribution. Reward realizations are only observed when an arm is selected, and the gambler’s objective is to maximize his cumulative expected earnings over some given horizon of play T . To do this, the gambler needs to acquire information about arms (exploration) while simultaneously optimizing immediate rewards (exploitation); the price paid due to this trade off is often referred to as the regret, and the main question is how small can this price be as a function of the horizon length T . This problem has been studied extensively when the reward distributions do not change over time; an assumption that supports a sharp characterization of the regret, yet is often violated in practical settings. In this paper, we focus on a MAB formulation which allows for a broad range of temporal uncertainties in the rewards, while still maintaining mathematical tractability. We fully characterize the (regret) complexity of this class of MAB problems by establishing a direct link between the extent of allowable reward “variation” and the minimal achievable regret, and by establishing a connection between the adversarial and the stochastic MAB frameworks.},
	language = {en},
	author = {Gur, Yonatan and Zeevi, Assaf and Besbes, Omar},
	keywords = {MAB, Non-stationary, Recommandé Audrey},
}

@article{jiang_general_2023,
	title = {General intelligence requires rethinking exploration},
	volume = {10},
	issn = {2054-5703},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.230539},
	doi = {10.1098/rsos.230539},
	abstract = {We are at the cusp of a transition from ‘learning from data’ to ‘learning what data to learn from’ as a central focus of artificial intelligence (AI) research. While the first-order learning problem is not completely solved, large models under unified architectures, such as transformers, have shifted the learning bottleneck from how to effectively train models to how to effectively acquire and use task-relevant data. This problem, which we frame as
              exploration
              , is a universal aspect of learning in open-ended domains like the real world. Although the study of exploration in AI is largely limited to the field of reinforcement learning, we argue that exploration is essential to all learning systems, including supervised learning. We propose the problem of
              generalized exploration
              to conceptually unify exploration-driven learning between supervised learning and reinforcement learning, allowing us to highlight key similarities across learning settings and open research challenges. Importantly, generalized exploration is a necessary objective for maintaining open-ended learning processes, which in continually learning to discover and solve new problems, provides a promising path to more general intelligence.},
	language = {en},
	number = {6},
	urldate = {2025-01-26},
	journal = {Royal Society Open Science},
	author = {Jiang, Minqi and Rocktäschel, Tim and Grefenstette, Edward},
	month = jun,
	year = {2023},
	keywords = {Demande CCAI phase II},
	pages = {230539},
}

@article{szabo_evolutionary_2007,
	title = {Evolutionary games on graphs},
	volume = {446},
	issn = {03701573},
	url = {http://arxiv.org/abs/cond-mat/0607344},
	doi = {10.1016/j.physrep.2007.04.004},
	abstract = {Game theory is one of the key paradigms behind many scientific disciplines from biology to behavioral sciences to economics. In its evolutionary form and especially when the interacting agents are linked in a specific social network the underlying solution concepts and methods are very similar to those applied in non-equilibrium statistical physics. This review gives a tutorial-type overview of the field for physicists. The first three sections introduce the necessary background in classical and evolutionary game theory from the basic definitions to the most important results. The fourth section surveys the topological complications implied by non-mean-field-type social network structures in general. The last three sections discuss in detail the dynamic behavior of three prominent classes of models: the Prisoner's Dilemma, the Rock-Scissors-Paper game, and Competing Associations. The major theme of the review is in what sense and how the graph structure of interactions can modify and enrich the picture of long term behavioral patterns emerging in evolutionary games.},
	number = {4-6},
	urldate = {2025-01-27},
	journal = {Physics Reports},
	author = {Szabo, Gyorgy and Fath, Gabor},
	month = jul,
	year = {2007},
	note = {arXiv:cond-mat/0607344},
	keywords = {Condensed Matter - Statistical Mechanics, Février, Game Theory, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Quantitative Biology - Populations and Evolution, Review},
	pages = {97--216},
}

@misc{mansoury_feedback_2020,
	title = {Feedback {Loop} and {Bias} {Amplification} in {Recommender} {Systems}},
	url = {http://arxiv.org/abs/2007.13019},
	doi = {10.48550/arXiv.2007.13019},
	abstract = {Recommendation algorithms are known to suffer from popularity bias; a few popular items are recommended frequently while the majority of other items are ignored. These recommendations are then consumed by the users, their reaction will be logged and added to the system: what is generally known as a feedback loop. In this paper, we propose a method for simulating the users interaction with the recommenders in an offline setting and study the impact of feedback loop on the popularity bias amplification of several recommendation algorithms. We then show how this bias amplification leads to several other problems such as declining the aggregate diversity, shifting the representation of users’ taste over time and also homogenization of the users experience. In particular, we show that the impact of feedback loop is generally stronger for the users who belong to the minority group.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Mansoury, Masoud and Abdollahpouri, Himan and Pechenizkiy, Mykola and Mobasher, Bamshad and Burke, Robin},
	month = jul,
	year = {2020},
	note = {arXiv:2007.13019 [cs]},
	keywords = {Computer Science - Information Retrieval, Demande CCAI phase II},
}

@article{alatur_multi-player_nodate,
	title = {Multi-{Player} {Bandits}: {The} {Adversarial} {Case}},
	abstract = {We consider a setting where multiple players sequentially choose among a common set of actions (arms). Motivated by an application to cognitive radio networks, we assume that players incur a loss upon colliding, and that communication between players is not possible. Existing approaches assume that the system is stationary. Yet this assumption is often violated in practice, e.g., due to signal strength ﬂuctuations. In this work, we design the ﬁrst multi-player Bandit algorithm that provably works in arbitrarily changing environments, where the losses of the arms may even be chosen by an adversary. This resolves an open problem posed by Rosenski et al. (2016).},
	language = {en},
	author = {Alatur, Pragnya and Alatur, Pragnya and Levy, Kﬁr Y and Krause, Andreas},
	keywords = {Recommandé Audrey},
}

@article{vaswani_old_nodate,
	title = {Old {Dog} {Learns} {New} {Tricks}: {Randomized} {UCB} for {Bandit} {Problems}},
	language = {en},
	author = {Vaswani, Sharan and Mehrabian, Abbas and Durand, Audrey and Kveton, Branislav},
}

@article{zrnic_prediction_nodate,
	title = {Prediction and {Statistical} {Inference} in {Feedback} {Loops}},
	language = {en},
	author = {Zrnic, Tijana},
}

@article{perdomo_performative_nodate,
	title = {Performative {Prediction}},
	abstract = {When predictions support decisions they may inﬂuence the outcome they aim to predict. We call such predictions performative; the prediction inﬂuences the target. Performativity is a wellstudied phenomenon in policy-making that has so far been neglected in supervised learning. When ignored, performativity surfaces as undesirable distribution shift, routinely addressed with retraining. We develop a risk minimization framework for performative prediction bringing together concepts from statistics, game theory, and causality. A conceptual novelty is an equilibrium notion we call performative stability. Performative stability implies that the predictions are calibrated not against past outcomes, but against the future outcomes that manifest from acting on the prediction. Our main results are necessary and sufﬁcient conditions for the convergence of retraining to a performatively stable point of nearly minimal loss. In full generality, performative prediction strictly subsumes the setting known as strategic classiﬁcation. We thus also give the ﬁrst sufﬁcient conditions for retraining to overcome strategic feedback effects.},
	language = {en},
	author = {Perdomo, Juan C and Zrnic, Tijana and Mendler-Dünner, Celestine and Hardt, Moritz},
	keywords = {Lecture début de projet, Recommandé Audrey},
}

@misc{lanctot_population-based_2023,
	title = {Population-based {Evaluation} in {Repeated} {Rock}-{Paper}-{Scissors} as a {Benchmark} for {Multiagent} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2303.03196},
	doi = {10.48550/arXiv.2303.03196},
	abstract = {Progress in fields of machine learning and adversarial planning has benefited significantly from benchmark domains, from checkers and the classic UCI data sets to Go and Diplomacy. In sequential decision-making, agent evaluation has largely been restricted to few interactions against experts, with the aim to reach some desired level of performance (e.g. beating a human professional player). We propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors along with a population of forty-three tournament entries, some of which are intentionally sub-optimal. We describe metrics to measure the quality of agents based both on average returns and exploitability. We then show that several RL, online learning, and language model approaches can learn good counter-strategies and generalize well, but ultimately lose to the top-performing bots, creating an opportunity for research in multiagent learning.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Lanctot, Marc and Schultz, John and Burch, Neil and Smith, Max Olan and Hennes, Daniel and Anthony, Thomas and Perolat, Julien},
	month = oct,
	year = {2023},
	note = {arXiv:2303.03196 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Computer Science - Multiagent Systems, MARL, RPS},
}

@misc{karwowski_goodharts_2023,
	title = {Goodhart's {Law} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2310.09144},
	doi = {10.48550/arXiv.2310.09144},
	abstract = {Implementing a reward function that perfectly captures a complex task in the real world is impractical. As a result, it is often appropriate to think of the reward function as a proxy for the true objective rather than as its definition. We study this phenomenon through the lens of Goodhart’s law, which predicts that increasing optimisation of an imperfect proxy beyond some critical point decreases performance on the true objective. First, we propose a way to quantify the magnitude of this effect and show empirically that optimising an imperfect proxy reward often leads to the behaviour predicted by Goodhart’s law for a wide range of environments and reward functions. We then provide a geometric explanation for why Goodhart’s law occurs in Markov decision processes. We use these theoretical insights to propose an optimal early stopping method that provably avoids the aforementioned pitfall and derive theoretical regret bounds for this method. Moreover, we derive a training method that maximises worst-case reward, for the setting where there is uncertainty about the true reward function. Finally, we evaluate our early stopping method experimentally. Our results support a foundation for a theoretically-principled study of reinforcement learning under reward misspecification.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Karwowski, Jacek and Hayman, Oliver and Bai, Xingjian and Kiendlhofer, Klaus and Griffin, Charlie and Skalse, Joar},
	month = oct,
	year = {2023},
	note = {arXiv:2310.09144 [cs]},
	keywords = {Computer Science - Machine Learning, Feedback loop, MDP, RL},
}

@misc{foerster_learning_2016,
	title = {Learning to {Communicate} with {Deep} {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1605.06676},
	doi = {10.48550/arXiv.1605.06676},
	abstract = {We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate endto-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Foerster, Jakob N. and Assael, Yannis M. and Freitas, Nando de and Whiteson, Shimon},
	month = may,
	year = {2016},
	note = {arXiv:1605.06676 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@misc{ning_improving_2021,
	title = {Improving {Model} {Robustness} by {Adaptively} {Correcting} {Perturbation} {Levels} with {Active} {Queries}},
	url = {http://arxiv.org/abs/2103.14824},
	doi = {10.48550/arXiv.2103.14824},
	abstract = {In addition to high accuracy, robustness is becoming increasingly important for machine learning models in various applications. Recently, much research has been devoted to improving the model robustness by training with noise perturbations. Most existing studies assume a ﬁxed perturbation level for all training examples, which however hardly holds in real tasks. In fact, excessive perturbations may destroy the discriminative content of an example, while deﬁcient perturbations may fail to provide helpful information for improving the robustness. Motivated by this observation, we propose to adaptively adjust the perturbation levels for each example in the training process. Speciﬁcally, a novel active learning framework is proposed to allow the model to interactively query the correct perturbation level from human experts. By designing a cost-effective sampling strategy along with a new query type, the robustness can be signiﬁcantly improved with a few queries. Both theoretical analysis and experimental studies validate the effectiveness of the proposed approach.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Ning, Kun-Peng and Tao, Lue and Chen, Songcan and Huang, Sheng-Jun},
	month = mar,
	year = {2021},
	note = {arXiv:2103.14824 [cs]},
	keywords = {Computer Science - Machine Learning, Demande CCAI phase II},
}

@article{press_iterated_2012,
	title = {Iterated {Prisoner}’s {Dilemma} contains strategies that dominate any evolutionary opponent},
	volume = {109},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1206569109},
	doi = {10.1073/pnas.1206569109},
	abstract = {The two-player Iterated Prisoner’s Dilemma game is a model for both sentient and evolutionary behaviors, especially including the emergence of cooperation. It is generally assumed that there exists no simple ultimatum strategy whereby one player can enforce a unilateral claim to an unfair share of rewards. Here, we show that such strategies unexpectedly do exist. In particular, a player X who is witting of these strategies can (
              i
              ) deterministically set her opponent Y’s score, independently of his strategy or response, or (
              ii
              ) enforce an extortionate linear relation between her and his scores. Against such a player, an evolutionary player’s best response is to accede to the extortion. Only a player with a theory of mind about his opponent can do better, in which case Iterated Prisoner’s Dilemma is an Ultimatum Game.},
	language = {en},
	number = {26},
	urldate = {2025-01-26},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Press, William H. and Dyson, Freeman J.},
	month = jun,
	year = {2012},
	keywords = {Game Theory, Lecture début de projet, Prisoner's Dilema, Recommandé Audrey},
	pages = {10409--10413},
}

@misc{bachman_learning_2017,
	title = {Learning {Algorithms} for {Active} {Learning}},
	url = {http://arxiv.org/abs/1708.00088},
	doi = {10.48550/arXiv.1708.00088},
	abstract = {We introduce a model that learns active learning algorithms via metalearning. For a distribution of related tasks, our model jointly learns: a data representation, an item selection heuristic, and a method for constructing prediction functions from labeled training sets. Our model uses the item selection heuristic to gather labeled training sets from which to construct prediction functions. Using the Omniglot and MovieLens datasets, we test our model in synthetic and practical settings.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Bachman, Philip and Sordoni, Alessandro and Trischler, Adam},
	month = jul,
	year = {2017},
	note = {arXiv:1708.00088 [cs]},
	keywords = {Computer Science - Machine Learning, Demande CCAI phase II},
}

@article{aghajohari_loqa_2024,
	title = {{LOQA}: {LEARNING} {WITH} {OPPONENT} {Q}-{LEARNING} {AWARENESS}},
	abstract = {In various real-world scenarios, interactions among agents often resemble the dynamics of general-sum games, where each agent strives to optimize its own utility. Despite the ubiquitous relevance of such settings, decentralized machine learning algorithms have struggled to find equilibria that maximize individual utility while preserving social welfare. In this paper we introduce Learning with Opponent QLearning Awareness (LOQA), a novel, decentralized reinforcement learning algorithm tailored to optimizing an agent’s individual utility while fostering cooperation among adversaries in partially competitive environments. LOQA assumes the opponent samples actions proportionally to their action-value function Q. Experimental results demonstrate the effectiveness of LOQA at achieving state-of-theart performance in benchmark scenarios such as the Iterated Prisoner’s Dilemma and the Coin Game. LOQA achieves these outcomes with a significantly reduced computational footprint, making it a promising approach for practical multi-agent applications.},
	language = {en},
	author = {Aghajohari, Milad and Duque, Juan Agustin and Cooijmans, Tim and Courville, Aaron},
	year = {2024},
	keywords = {Game Theory, Lecture début de projet, Prisoner's Dilema, RL, Recommandé Audrey},
}

@article{shen_metric-fair_nodate,
	title = {Metric-{Fair} {Active} {Learning}},
	abstract = {Active learning has become a prevalent technique for designing label-efﬁcient algorithms, where the central principle is to only query and ﬁt “informative” labeled instances. It is, however, known that an active learning algorithm may incur unfairness due to such instance selection procedure. In this paper, we henceforth study metric-fair active learning of homogeneous halfspaces, and show that under the distribution-dependent PAC learning model, fairness and label efﬁciency can be achieved simultaneously. We further propose two extensions of our main results: 1) we show that it is possible to make the algorithm robust to the adversarial noise – one of the most challenging noise models in learning theory; and 2) it is possible to signiﬁcantly improve the label complexity when the underlying halfspace is sparse.},
	language = {en},
	author = {Shen, Jie and Cui, Nan and Wang, Jing},
	keywords = {Demande CCAI phase II},
}

@misc{mehr_maximum-entropy_2021,
	title = {Maximum-{Entropy} {Multi}-{Agent} {Dynamic} {Games}: {Forward} and {Inverse} {Solutions}},
	shorttitle = {Maximum-{Entropy} {Multi}-{Agent} {Dynamic} {Games}},
	url = {http://arxiv.org/abs/2110.01027},
	doi = {10.48550/arXiv.2110.01027},
	abstract = {In this paper, we study the problem of multiple stochastic agents interacting in a dynamic game scenario with continuous state and action spaces. We deﬁne a new notion of stochastic Nash equilibrium for boundedly rational agents, which we call the Entropic Cost Equilibrium (ECE). We show that ECE is a natural extension to multiple agents of Maximum Entropy optimality for single agents. We solve both the “forward” and “inverse” problems for the multi-agent ECE game. For the forward problem, we provide a Riccati algorithm to compute closed-form ECE feedback policies for the agents, which are exact in the Linear-Quadratic-Gaussian case. We give an iterative variant to ﬁnd locally ECE feedback policies for the nonlinear case. For the inverse problem, we present an algorithm to infer the cost functions of the multiple interacting agents given noisy, boundedly rational input and state trajectory examples from agents acting in an ECE. The effectiveness of our algorithms is demonstrated in a simulated multi-agent collision avoidance scenario, and with data from the INTERACTION trafﬁc dataset. In both cases, we show that, by taking into account the agents’ game theoretic interactions using our algorithm, a more accurate model of agents’ costs can be learned, compared with standard inverse optimal control methods.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Mehr, Negar and Wang, Mingyu and Schwager, Mac},
	month = oct,
	year = {2021},
	note = {arXiv:2110.01027 [math]},
	keywords = {Computer Science - Robotics, Mathematics - Optimization and Control},
}

@article{kaelbling_planning_1998,
	title = {Planning and acting in partially observable stochastic domains},
	volume = {101},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S000437029800023X},
	doi = {10.1016/S0004-3702(98)00023-X},
	abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (MDPS) and partially observable MDPS (POMDPS). We then outline a novel algorithm for solving POMDPS off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to POMDPS, and of some possibilities for finding approximate solutions. 0 1998 Elsevier Science B.V. All rights reserved.},
	language = {en},
	number = {1-2},
	urldate = {2025-01-26},
	journal = {Artificial Intelligence},
	author = {Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra, Anthony R.},
	month = may,
	year = {1998},
	keywords = {Demande CCAI phase II},
	pages = {99--134},
}

@article{balduzzi_open-ended_nodate,
	title = {Open-ended {Learning} in {Symmetric} {Zero}-sum {Games}},
	abstract = {Zero-sum games such as chess and poker are, abstractly, functions that evaluate pairs of agents, for example labeling them ‘winner’ and ‘loser’. If the game is approximately transitive, then selfplay generates sequences of agents of increasing strength. However, nontransitive games, such as rock-paper-scissors, can exhibit strategic cycles, and there is no longer a clear objective – we want agents to increase in strength, but against whom is unclear. In this paper, we introduce a geometric framework for formulating agent objectives in zero-sum games, in order to construct adaptive sequences of objectives that yield openended learning. The framework allows us to reason about population performance in nontransitive games, and enables the development of a new algorithm (rectiﬁed Nash response, PSROrN) that uses game-theoretic niching to construct diverse populations of effective agents, producing a stronger set of agents than existing algorithms. We apply PSROrN to two highly nontransitive resource allocation games and ﬁnd that PSROrN consistently outperforms the existing alternatives.},
	language = {en},
	author = {Balduzzi, David and Garnelo, Marta and Bachrach, Yoram and Czarnecki, Wojciech M and Perolat, Julien and Jaderberg, Max and Graepel, Thore},
	keywords = {Février, Game Theory, Maxime, Zero-sum Games},
}

@article{lattimore_partial_nodate,
	title = {Partial monitoring},
	abstract = {We provide a novel algorithm for adversarial k-action d-outcome partial monitoring that is adaptive, intuitive and efﬁcient. The highlight is that for the non-degenerate locally observable games, the n-round minimax regret is bounded by 6mk3/2√n log(k), where m is the number of signals. This matches the best known information-theoretic upper bound derived via Bayesian minimax duality. The same algorithm also achieves near-optimal regret for full information, bandit and globally observable games. High probability bounds and simple experiments are also provided.},
	language = {en},
	author = {Lattimore, Tor and Szepesvari, Csaba},
	keywords = {Demande CCAI phase II},
}

@misc{brown_performative_2022,
	title = {Performative {Prediction} in a {Stateful} {World}},
	url = {http://arxiv.org/abs/2011.03885},
	doi = {10.48550/arXiv.2011.03885},
	abstract = {Deployed supervised machine learning models make predictions that interact with and inﬂuence the world. This phenomenon is called performative prediction by Perdomo et al. (ICML 2020). It is an ongoing challenge to understand the inﬂuence of such predictions as well as design tools so as to control that inﬂuence. We propose a theoretical framework where the response of a target population to the deployed classiﬁer is modeled as a function of the classiﬁer and the current state (distribution) of the population. We show necessary and suﬃcient conditions for convergence to an equilibrium of two retraining algorithms, repeated risk minimization and a lazier variant. Furthermore, convergence is near an optimal classiﬁer. We thus generalize results of Perdomo et al., whose performativity framework does not assume any dependence on the state of the target population. A particular phenomenon captured by our model is that of distinct groups that acquire information and resources at diﬀerent rates to be able to respond to the latest deployed classiﬁer. We study this phenomenon theoretically and empirically.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Brown, Gavin and Hod, Shlomi and Kalemaj, Iden},
	month = feb,
	year = {2022},
	note = {arXiv:2011.03885 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Demande CCAI phase II},
}

@misc{richens_robust_2024,
	title = {Robust agents learn causal world models},
	url = {http://arxiv.org/abs/2402.10877},
	doi = {10.48550/arXiv.2402.10877},
	abstract = {It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound for a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Richens, Jonathan and Everitt, Tom},
	month = jul,
	year = {2024},
	note = {arXiv:2402.10877 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Lecture début de projet, Recommandé Audrey},
}

@article{aschenbrenner_situational_nodate,
	title = {Situational {Awareness}},
	language = {en},
	author = {Aschenbrenner, Leopold},
	keywords = {LLM},
}

@misc{guan_rstar-math_2025,
	title = {{rStar}-{Math}: {Small} {LLMs} {Can} {Master} {Math} {Reasoning} with {Self}-{Evolved} {Deep} {Thinking}},
	shorttitle = {{rStar}-{Math}},
	url = {http://arxiv.org/abs/2501.04519},
	doi = {10.48550/arXiv.2501.04519},
	abstract = {We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising “deep thinking” through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids naïve step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs’ math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8\% to 90.0\% and Phi3-mini-3.8B from 41.4\% to 86.4\%, surpassing o1-preview by +4.5\% and +0.9\%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3\% (8/15) of problems, ranking among the top 20\% the brightest high school math students. Code and data will be available at https://github.com/microsoft/rStar.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Guan, Xinyu and Zhang, Li Lyna and Liu, Yifei and Shang, Ning and Sun, Youran and Zhu, Yi and Yang, Fan and Yang, Mao},
	month = jan,
	year = {2025},
	note = {arXiv:2501.04519 [cs]},
	keywords = {Computer Science - Computation and Language, LLM},
}

@article{heuillet_tracking_nodate,
	title = {Tracking the {Risk} of {Machine} {Learning} {Systems} with {Partial} {Monitoring}},
	abstract = {Although efficient at performing specific tasks, Machine Learning Systems (MLSs) remain vulnerable to instabilities such as noise or adversarial attacks. In this work, we aim to track the risk exposure of an MLS to these events. We formulate this problem under the stochastic Partial Monitoring (PM) setting. We focus on two instances of partial monitoring, namely the Apple Tasting and Label Efficient games, that are particularly relevant to our problem. Our review of the practicality of existing algorithms motivates RandCBP, a randomized variation of the deterministic algorithm Confidence Bound (CBP) inspired by recent theoretical developments in the bandits setting. Our preliminary results indicate that RandCBP enjoys the same regret guarantees as its deterministic counterpart CBP and achieves competitive empirical performance on settings of interest which suggests it could be a suitable candidate for our problem.},
	language = {en},
	author = {Heuillet, Maxime and Durand, Audrey},
	keywords = {Demande CCAI phase II},
}

@inproceedings{slyman_vlslice_2023,
	title = {{VLSlice}: {Interactive} {Vision}-and-{Language} {Slice} {Discovery}},
	shorttitle = {{VLSlice}},
	url = {http://arxiv.org/abs/2309.06703},
	doi = {10.1109/ICCV51070.2023.01403},
	abstract = {Recent work in vision-and-language demonstrates that large-scale pretraining can learn generalizable models that are efficiently transferable to downstream tasks. While this may improve dataset-scale aggregate metrics, analyzing performance around hand-crafted subgroups targeting specific bias dimensions reveals systemic undesirable behaviors. However, this subgroup analysis is frequently stalled by annotation efforts, which require extensive time and resources to collect the necessary data. Prior art attempts to automatically discover subgroups to circumvent these constraints but typically leverages model behavior on existing task-specific annotations and rapidly degrades on more complex inputs beyond “tabular” data, none of which study vision-and-language models. This paper presents VLSlice, an interactive system enabling user-guided discovery of coherent representation-level subgroups with consistent visiolinguistic behavior, denoted as vision-and-language slices, from unlabeled image sets. We show that VLSlice enables users to quickly generate diverse high-coherency slices in a user study (n=22) and release the tool publicly1.},
	language = {en},
	urldate = {2025-01-26},
	booktitle = {2023 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Slyman, Eric and Kahng, Minsuk and Lee, Stefan},
	month = oct,
	year = {2023},
	note = {arXiv:2309.06703 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Demande CCAI phase II},
	pages = {15245--15255},
}

@article{abramoff_considerations_2023,
	title = {Considerations for addressing bias in artificial intelligence for health equity},
	volume = {6},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-023-00913-9},
	doi = {10.1038/s41746-023-00913-9},
	abstract = {Abstract
            Health equity is a primary goal of healthcare stakeholders: patients and their advocacy groups, clinicians, other providers and their professional societies, bioethicists, payors and value based care organizations, regulatory agencies, legislators, and creators of artificial intelligence/machine learning (AI/ML)-enabled medical devices. Lack of equitable access to diagnosis and treatment may be improved through new digital health technologies, especially AI/ML, but these may also exacerbate disparities, depending on how bias is addressed. We propose an expanded Total Product Lifecycle (TPLC) framework for healthcare AI/ML, describing the sources and impacts of undesirable bias in AI/ML systems in each phase, how these can be analyzed using appropriate metrics, and how they can be potentially mitigated. The goal of these “Considerations” is to educate stakeholders on how potential AI/ML bias may impact healthcare outcomes and how to identify and mitigate inequities; to initiate a discussion between stakeholders on these issues, in order to ensure health equity along the expanded AI/ML TPLC framework, and ultimately, better health outcomes for all.},
	language = {en},
	number = {1},
	urldate = {2025-01-26},
	journal = {npj Digital Medicine},
	author = {Abràmoff, Michael D. and Tarver, Michelle E. and Loyo-Berrios, Nilsa and Trujillo, Sylvia and Char, Danton and Obermeyer, Ziad and Eydelman, Malvina B. and {Foundational Principles of Ophthalmic Imaging and Algorithmic Interpretation Working Group of the Collaborative Community for Ophthalmic Imaging Foundation, Washington, D.C.} and Maisel, William H.},
	month = sep,
	year = {2023},
	keywords = {Demande CCAI phase II},
	pages = {170},
}

@misc{coston_counterfactual_2020,
	title = {Counterfactual {Risk} {Assessments}, {Evaluation}, and {Fairness}},
	url = {http://arxiv.org/abs/1909.00066},
	doi = {10.48550/arXiv.1909.00066},
	abstract = {Algorithmic risk assessments are increasingly used to help humans make decisions in high-stakes settings, such as medicine, criminal justice and education. In each of these cases, the purpose of the risk assessment tool is to inform actions, such as medical treatments or release conditions, often with the aim of reducing the likelihood of an adverse event such as hospital readmission or recidivism. Problematically, most tools are trained and evaluated on historical data in which the outcomes observed depend on the historical decision-making policy. These tools thus reflect risk under the historical policy, rather than under the different decision options that the tool is intended to inform. Even when tools are constructed to predict risk under a specific decision, they are often improperly evaluated as predictors of the target outcome.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Coston, Amanda and Mishler, Alan and Kennedy, Edward H. and Chouldechova, Alexandra},
	month = jan,
	year = {2020},
	note = {arXiv:1909.00066 [stat]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Lecture début de projet, Recommandé Audrey, Statistics - Applications, Statistics - Machine Learning, Statistics - Methodology},
}

@inproceedings{vadali_convergence_2023,
	address = {Greater Noida, India},
	title = {Convergence to {Nash} {Equilibrium}: {A} {Comparative} {Study} of {Rock}-{Paper}-{Scissors} {Algorithms}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-0611-8},
	shorttitle = {Convergence to {Nash} {Equilibrium}},
	url = {https://ieeexplore.ieee.org/document/10425577/},
	doi = {10.1109/ICCCIS60361.2023.10425577},
	abstract = {Rock-paper-scissors is one of the most established imperfect information games in Game theory. The Nash Equilibrium of an RPS game is relatively simple but computationally intractable; hence, various algorithms are employed to converge to a state of maximum payoff. In this paper, five algorithms, namely - Counterfactual Regret Minimization, Monte Carlo Tree Search, Q-learning, Deep Q-Network and Proximal Policy Optimization, have been compared on the evaluation metrics of average reward, draw ratio and convergence speed. Throughout the comparative analysis, visualising the learning curves, and qualitative comparison, Q-learning has shown the best convergence to Nash equilibrium for RPS.},
	language = {en},
	urldate = {2025-01-26},
	booktitle = {2023 {International} {Conference} on {Computing}, {Communication}, and {Intelligent} {Systems} ({ICCCIS})},
	publisher = {IEEE},
	author = {Vadali, Geetika and Reddy, M Deekshitha and Rani, Ritu and Bansal, Poonam},
	month = nov,
	year = {2023},
	pages = {329--334},
}

@article{durand_apprentissage_nodate,
	title = {Apprentissage par {Renforcement}},
	language = {fr},
	author = {Durand, Audrey},
}

@article{boursier_survey_nodate,
	title = {A {Survey} on {Multi}-player {Bandits}},
	abstract = {Due mostly to its application to cognitive radio networks, multiplayer bandits gained a lot of interest in the last decade. A considerable progress has been made on its theoretical aspect. However, the current algorithms are far from applicable and many obstacles remain between these theoretical results and a possible implementation of multiplayer bandits algorithms in real communication networks. This survey contextualizes and organizes the rich multiplayer bandits literature. In light of the existing works, some clear directions for future research appear. We believe that a further study of these diﬀerent directions might lead to theoretical algorithms adapted to real-world situations.},
	language = {en},
	author = {Boursier, Etienne and Perchet, Vianney},
}

@article{durand_machine_2018,
	title = {A machine learning approach for online automated optimization of super-resolution optical microscopy},
	volume = {9},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-018-07668-y},
	doi = {10.1038/s41467-018-07668-y},
	abstract = {Abstract
            Traditional approaches for finding well-performing parameterizations of complex imaging systems, such as super-resolution microscopes rely on an extensive exploration phase over the illumination and acquisition settings, prior to the imaging task. This strategy suffers from several issues: it requires a large amount of parameter configurations to be evaluated, it leads to discrepancies between well-performing parameters in the exploration phase and imaging task, and it results in a waste of time and resources given that optimization and final imaging tasks are conducted separately. Here we show that a fully automated, machine learning-based system can conduct imaging parameter optimization toward a trade-off between several objectives, simultaneously to the imaging task. Its potential is highlighted on various imaging tasks, such as live-cell and multicolor imaging and multimodal optimization. This online optimization routine can be integrated to various imaging systems to increase accessibility, optimize performance and improve overall imaging quality.},
	language = {en},
	number = {1},
	urldate = {2025-01-26},
	journal = {Nature Communications},
	author = {Durand, Audrey and Wiesner, Theresa and Gardner, Marc-André and Robitaille, Louis-Émile and Bilodeau, Anthony and Gagné, Christian and De Koninck, Paul and Lavoie-Cardinal, Flavie},
	month = dec,
	year = {2018},
	keywords = {Demande CCAI phase II},
	pages = {5247},
}

@misc{azar_general_2023,
	title = {A {General} {Theoretical} {Paradigm} to {Understand} {Learning} from {Human} {Preferences}},
	url = {http://arxiv.org/abs/2310.12036},
	doi = {10.48550/arXiv.2310.12036},
	abstract = {The prevalent deployment of learning from human preferences through reinforcement learning (RLHF) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Azar, Mohammad Gheshlaghi and Rowland, Mark and Piot, Bilal and Guo, Daniel and Calandriello, Daniele and Valko, Michal and Munos, Rémi},
	month = nov,
	year = {2023},
	note = {arXiv:2310.12036 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Demande CCAI phase II, Statistics - Machine Learning},
}

@article{godbout_game-theoretic_nodate,
	title = {A {Game}-{Theoretic} {Perspective} on {Risk}-{Sensitive}{\textbackslash} {Reinforcement} {Learning}},
	abstract = {Most Reinforcement Learning (RL) approaches usually aim to ﬁnd a policy that maximizes its expected return. However, this objective may be inappropriate in many safety-critical domains such as healthcare or autonomous driving, where it is often preferable to optimize for a risk-sensitive measure of the policy’s return as the learning objective, such as the Conditional-Value-at-Risk (CVaR). Although previous literature exists to address the problem of learning CVaR-optimal policies in Markov decision problems, it mostly relies on the distributional RL perspective. In this paper, we solve this problem by rather proposing an approach based on a game theoretic perspective, which can be applied on top of any existing RL algorithm. At the core of our approach is a twoplayer zero-sum game between a policy player and an adversary that perturbs the policy player’s state transitions given a ﬁnite budget. We show that, the closer the players are to the game’s equilibrium point, the closer the learned policy is to the CVaR-optimal one with a risk tolerance explicitly related to the adversary’s budget. We provide a gradient-based training procedure to solve the proposed game by formulating it as a Stackelberg game, enabling the use of deep RL architectures and training algorithms. We illustrate the applicability of our approach on a risky artiﬁcial environment, presenting the different policies learned for various adversary budgets.},
	language = {en},
	author = {Godbout, Mathieu and Heuillet, Maxime and Chandar, Sharath and Bhati, Rupali and Durand, Audrey},
	keywords = {Demande CCAI phase II},
}

@article{noauthor_3-dynamic_programming_nodate,
	title = {3-dynamic\_programming},
	language = {en},
}

@inproceedings{pagan_classification_2023,
	address = {Boston MA USA},
	title = {A {Classification} of {Feedback} {Loops} and {Their} {Relation} to {Biases} in {Automated} {Decision}-{Making} {Systems}},
	isbn = {979-8-4007-0381-2},
	url = {https://dl.acm.org/doi/10.1145/3617694.3623227},
	doi = {10.1145/3617694.3623227},
	abstract = {Prediction-based decision-making systems are becoming increasingly prevalent in various domains. Previous studies have demonstrated that such systems are vulnerable to runaway feedback loops, e.g., when police are repeatedly sent back to the same neighborhoods regardless of the actual rate of criminal activity, which exacerbate existing biases. In practice, the automated decisions have dynamic feedback effects on the system itself – which in ML literature is sometimes referred to as performative predictions – that can perpetuate over time, making it difficult for short-sighted design choices to control the system’s evolution. While researchers started proposing longer-term solutions to prevent adverse outcomes (such as bias towards certain groups), these interventions largely depend on ad hoc modeling assumptions and a rigorous theoretical understanding of the feedback dynamics in ML-based decision-making systems is currently missing. In this paper, we use the language of dynamical systems theory, a branch of applied mathematics that deals with the analysis of the interconnection of systems with dynamic behaviors, to rigorously classify the different types of feedback loops in the ML-based decision-making pipeline. By reviewing existing scholarly work, we show that this classification covers many examples discussed in the algorithmic fairness community, thereby providing a unifying and principled framework to study feedback loops. By qualitative analysis, and through a simulation example of recommender systems, we show which specific types of ML biases are affected by each type of feedback loop. We find that the existence of feedback loops in the ML-based decision-making pipeline can perpetuate, reinforce, or even reduce ML biases.},
	language = {en},
	urldate = {2025-01-26},
	booktitle = {Equity and {Access} in {Algorithms}, {Mechanisms}, and {Optimization}},
	publisher = {ACM},
	author = {Pagan, Nicolò and Baumann, Joachim and Elokda, Ezzat and De Pasquale, Giulia and Bolognani, Saverio and Hannák, Anikó},
	month = oct,
	year = {2023},
	keywords = {Feedback loop, Lecture début de projet, Recommandé Audrey, Review},
	pages = {1--14},
}

@article{wunder_classes_nodate,
	title = {Classes of {Multiagent} {Q}-learning {Dynamics}  with -greedy {Exploration}},
	abstract = {Q-learning in single-agent environments is known to converge in the limit given suﬃcient exploration. The same algorithm has been applied, with some success, in multiagent environments, where traditional analysis techniques break down. Using established dynamical systems methods, we derive and study an idealization of Q-learning in 2-player 2-action repeated general-sum games. In particular, we address the discontinuous case of -greedy exploration and use it as a proxy for value-based algorithms to highlight a contrast with existing results in policy search. Analogously to previous results for gradient ascent algorithms, we provide a complete catalog of the convergence behavior of the -greedy Q-learning algorithm by introducing new subclasses of these games. We identify two subclasses of Prisoner’s Dilemma-like games where the application of Q-learning with -greedy exploration results in higher-than-Nash average payoﬀs for some initial conditions.},
	language = {en},
	author = {Wunder, Michael and Littman, Michael and Babes, Monica},
	keywords = {Cycle, Game Theory, Important, Mars 2025, Multi-agent, Prisoner's Dilema},
}

@article{hernandez-leal_survey_2019,
	title = {A {Survey} and {Critique} of {Multiagent} {Deep} {Reinforcement} {Learning}},
	volume = {33},
	issn = {1387-2532, 1573-7454},
	url = {http://arxiv.org/abs/1810.05587},
	doi = {10.1007/s10458-019-09421-1},
	abstract = {Deep reinforcement learning (RL) has achieved outstanding results in recent years. This has led to a dramatic increase in the number of applications and methods. Recent works have explored learning beyond single-agent scenarios and have considered multiagent learning (MAL) scenarios. Initial results report successes in complex multiagent domains, although there are several challenges to be addressed. The primary goal of this article is to provide a clear overview of current multiagent deep reinforcement learning (MDRL) literature. Additionally, we complement the overview with a broader analysis: (i) we revisit previous key components, originally presented in MAL and RL, and highlight how they have been adapted to multiagent deep reinforcement learning settings. (ii) We provide general guidelines to new practitioners in the area: describing lessons learned from MDRL works, pointing to recent benchmarks, and outlining open avenues of research. (iii) We take a more critical tone raising practical challenges of MDRL (e.g., implementation and computational demands). We expect this article will help unify and motivate future research to take advantage of the abundant literature that exists (e.g., RL and MAL) in a joint effort to promote fruitful research in the multiagent community.},
	number = {6},
	urldate = {2025-03-10},
	journal = {Autonomous Agents and Multi-Agent Systems},
	author = {Hernandez-Leal, Pablo and Kartal, Bilal and Taylor, Matthew E.},
	month = nov,
	year = {2019},
	note = {arXiv:1810.05587 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Mars 2025, Multi-agent},
	pages = {750--797},
}

@misc{hernandez-leal_survey_2019-1,
	title = {A {Survey} of {Learning} in {Multiagent} {Environments}: {Dealing} with {Non}-{Stationarity}},
	shorttitle = {A {Survey} of {Learning} in {Multiagent} {Environments}},
	url = {http://arxiv.org/abs/1707.09183},
	doi = {10.48550/arXiv.1707.09183},
	abstract = {The key challenge in multiagent learning is learning a best response to the behaviour of other agents, which may be non-stationary: if the other agents adapt their strategy as well, the learning target moves. Disparate streams of research have approached non-stationarity from several angles, which make a variety of implicit assumptions that make it hard to keep an overview of the state of the art and to validate the innovation and significance of new works. This survey presents a coherent overview of work that addresses opponent-induced non-stationarity with tools from game theory, reinforcement learning and multi-armed bandits. Further, we reflect on the principle approaches how algorithms model and cope with this non-stationarity, arriving at a new framework and five categories (in increasing order of sophistication): ignore, forget, respond to target models, learn models, and theory of mind. A wide range of state-of-the-art algorithms is classified into a taxonomy, using these categories and key characteristics of the environment (e.g., observability) and adaptation behaviour of the opponents (e.g., smooth, abrupt). To clarify even further we present illustrative variations of one domain, contrasting the strengths and limitations of each category. Finally, we discuss in which environments the different approaches yield most merit, and point to promising avenues of future research.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Hernandez-Leal, Pablo and Kaisers, Michael and Baarslag, Tim and Cote, Enrique Munoz de},
	month = mar,
	year = {2019},
	note = {arXiv:1707.09183 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems, Game Theory, Important, Mars 2025, Rewiev},
}

@misc{mei_mac-po_2023,
	title = {{MAC}-{PO}: {Multi}-{Agent} {Experience} {Replay} via {Collective} {Priority} {Optimization}},
	shorttitle = {{MAC}-{PO}},
	url = {http://arxiv.org/abs/2302.10418},
	doi = {10.48550/arXiv.2302.10418},
	abstract = {Experience replay is crucial for off-policy reinforcement learning (RL) methods. By remembering and reusing the experiences from past different policies, experience replay significantly improves the training efficiency and stability of RL algorithms. Many decisionmaking problems in practice naturally involve multiple agents and require multi-agent reinforcement learning (MARL) under centralized training decentralized execution paradigm. Nevertheless, existing MARL algorithms often adopt standard experience replay where the transitions are uniformly sampled regardless of their importance. Finding prioritized sampling weights that are optimized for MARL experience replay has yet to be explored. To this end, we propose MAC-PO, which formulates optimal prioritized experience replay for multi-agent problems as a regret minimization over the sampling weights of transitions. Such optimization is relaxed and solved using the Lagrangian multiplier approach to obtain the close-form optimal sampling weights. By minimizing the resulting policy regret, we can narrow the gap between the current policy and a nominal optimal policy, thus acquiring an improved prioritization scheme for multi-agent tasks. Our experimental results on Predator-Prey and StarCraft Multi-Agent Challenge environments demonstrate the effectiveness of our method, having a better ability to replay important transitions and outperforming other state-of-the-art baselines.},
	language = {en},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Mei, Yongsheng and Zhou, Hanhan and Lan, Tian and Venkataramani, Guru and Wei, Peng},
	month = feb,
	year = {2023},
	note = {arXiv:2302.10418 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Mars 2025, Multi-agent},
}

@article{claus_dynamics_nodate,
	title = {The {Dynamics} of {Reinforcement} {Learning} in {Cooperative} {Multiagent} {Systems}},
	abstract = {Reinforcement learning can provide a robust and natural means for agents to learn how to coordinate their action choices in multiagent systems. We examine some of the factors that can inﬂuence the dynamics of the learning process in such a setting. We ﬁrst distinguish reinforcement learners that are unaware of (or ignore) the presence of other agents from those that explicitly attempt to learn the value of joint actions and the strategies of their counterparts. We study Q-learning in cooperative multiagent systems under these two perspectives, focusing on the inﬂuence of partial action observability, game structure, and exploration strategies on convergence to (optimal and suboptimal) Nash equilibria and on learned Qvalues.},
	language = {en},
	author = {Claus, Caroline and Boutilier, Craig},
	keywords = {Cycle, Multi-agent},
}

@misc{douglas_naive_2024,
	title = {Naive {Algorithmic} {Collusion}: {When} {Do} {Bandit} {Learners} {Cooperate} and {When} {Do} {They} {Compete}?},
	shorttitle = {Naive {Algorithmic} {Collusion}},
	url = {http://arxiv.org/abs/2411.16574},
	doi = {10.48550/arXiv.2411.16574},
	abstract = {Algorithmic agents are used in a variety of competitive decision settings, notably in making pricing decisions in contexts that range from online retail to residential home rentals. Business managers, algorithm designers, legal scholars, and regulators alike are all starting to consider the ramifications of "algorithmic collusion." We study the emergent behavior of multi-armed bandit machine learning algorithms used in situations where agents are competing, but they have no information about the strategic interaction they are engaged in. Using a general-form repeated Prisoner's Dilemma game, agents engage in online learning with no prior model of game structure and no knowledge of competitors' states or actions (e.g., no observation of competing prices). We show that these context-free bandits, with no knowledge of opponents' choices or outcomes, still will consistently learn collusive behavior - what we call "naive collusion." We primarily study this system through an analytical model and examine perturbations to the model through simulations. Our findings have several notable implications for regulators. First, calls to limit algorithms from conditioning on competitors' prices are insufficient to prevent algorithmic collusion. This is a direct result of collusion arising even in the naive setting. Second, symmetry in algorithms can increase collusion potential. This highlights a new, simple mechanism for "hub-and-spoke" algorithmic collusion. A central distributor need not imbue its algorithm with supra-competitive tendencies for apparent collusion to arise; it can simply arise by using certain (common) machine learning algorithms. Finally, we highlight that collusive outcomes depend starkly on the specific algorithm being used, and we highlight market and algorithmic conditions under which it will be unknown a priori whether collusion occurs.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Douglas, Connor and Provost, Foster and Sundararajan, Arun},
	month = nov,
	year = {2024},
	note = {arXiv:2411.16574 [econ]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Multiagent Systems, Economics - General Economics, Epsillon, Important, Mars 2025, Multi-agent, Quantitative Finance - Economics},
}

@article{chang_all_nodate,
	title = {All learning is local: {Multi}-agent learning in global reward games},
	abstract = {In large multiagent games, partial observability, coordination, and credit assignment persistently plague attempts to design good learning algorithms. We provide a simple and efﬁcient algorithm that in part uses a linear system to model the world from a single agent’s limited perspective, and takes advantage of Kalman ﬁltering to allow an agent to construct a good training signal and learn an effective policy.},
	language = {en},
	author = {Chang, Yu-Han and Ho, Tracey and Kaelbling, Leslie Pack},
	keywords = {Game Theory, Mars 2025, Multi-agent},
}

@misc{leshem_fair_2024,
	title = {Fair {Multi}-{Agent} {Bandits}},
	url = {http://arxiv.org/abs/2306.04498},
	doi = {10.48550/arXiv.2306.04498},
	abstract = {In this paper, we study the problem of fair multi-agent multi-arm bandit learning when agents do not communicate with each other, except collision information, provided to agents accessing the same arm simultaneously. We provide an algorithm with regret \$O{\textbackslash}left(N{\textasciicircum}3 {\textbackslash}log {\textbackslash}frac\{B\}\{{\textbackslash}Delta\} f({\textbackslash}log T) {\textbackslash}log T {\textbackslash}right)\$ (assuming bounded rewards, with unknown bound), where \$f(t)\$ is any function diverging to infinity with \$t\$. This significantly improves previous results which had the same upper bound on the regret of order \$O(f({\textbackslash}log T) {\textbackslash}log T )\$ but an exponential dependence on the number of agents. The result is attained by using a distributed auction algorithm to learn the sample-optimal matching and a novel order-statistics-based regret analysis. Simulation results present the dependence of the regret on \${\textbackslash}log T\$.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Leshem, Amir},
	month = feb,
	year = {2024},
	note = {arXiv:2306.04498 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Mars 2025, Multi-agent, fairness},
}

@article{mao_multi-agent_nodate,
	title = {{MULTI}-{AGENT} {REINFORCEMENT} {LEARNING} {FOR} {NONZERO}-{SUM} {MARKOV} {GAMES}},
	language = {en},
	author = {Mao, Weichao},
	keywords = {Game Theory, Mars 2025},
}

@article{gursoy_multi-armed_2024,
	title = {Multi-armed bandit games},
	issn = {0254-5330, 1572-9338},
	url = {https://link.springer.com/10.1007/s10479-024-06336-3},
	doi = {10.1007/s10479-024-06336-3},
	abstract = {A sequential optimization model, known as the multi-armed bandit problem, is concerned with optimal allocation of resources between competing activities, in order to generate the most likely beneﬁts, for a given period of time. In this work, following the objective of a multi-armed bandit problem, we consider a mean-ﬁeld game model to approach to a large number of multi-armed bandit problems, and propose some connections between dynamic games and sequential optimization problems.},
	language = {en},
	urldate = {2025-03-10},
	journal = {Annals of Operations Research},
	author = {Gürsoy, Kemal},
	month = oct,
	year = {2024},
	keywords = {Cycle, Game Theory, MAB, Mars 2025},
}

@misc{wang_decision_2022,
	title = {Decision {Market} {Based} {Learning} {For} {Multi}-agent {Contextual} {Bandit} {Problems}},
	url = {http://arxiv.org/abs/2212.00271},
	doi = {10.48550/arXiv.2212.00271},
	abstract = {Information is often stored in a distributed and proprietary form, and agents who own information are often self-interested and require incentives to reveal their information. Suitable mechanisms are required to elicit and aggregate such distributed information for decision making. In this paper, we use simulations to investigate the use of decision markets as mechanisms in a multi-agent learning system to aggregate distributed information for decision-making in a contextual bandit problem. The system utilises strictly proper decision scoring rules to assess the accuracy of probabilistic reports from agents, which allows agents to learn to solve the contextual bandit problem jointly. Our simulations show that our multi-agent system with distributed information can be trained as efficiently as a centralised counterpart with a single agent that receives all information. Moreover, we use our system to investigate scenarios with deterministic decision scoring rules which are not incentive compatible. We observe the emergence of more complex dynamics with manipulative behaviour, which agrees with existing theoretical analyses.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Wang, Wenlong and Pfeiffer, Thomas},
	month = dec,
	year = {2022},
	note = {arXiv:2212.00271 [cs]},
	keywords = {Computer Science - Multiagent Systems, Emergent, Mars 2025, Multi-agent},
}

@inproceedings{foster_complexity_2023,
	title = {On the {Complexity} of {Multi}-{Agent} {Decision} {Making}: {From} {Learning} in {Games} to {Partial} {Monitoring}},
	shorttitle = {On the {Complexity} of {Multi}-{Agent} {Decision} {Making}},
	url = {https://proceedings.mlr.press/v195/foster23a.html},
	abstract = {A central problem in the theory of multi-agent reinforcement learning (MARL) is to understand what structural conditions and algorithmic principles lead to sample-efficient learning guarantees, and how these considerations change as we move from few to many agents. We study this question in a general framework for interactive decision making with multiple agents, encompassing Markov games with function approximation and normal-form games with bandit feedback. We focus on equilibrium computation, in which a centralized learning algorithm aims to compute an equilibrium by controlling multiple agents that interact with an (unknown) environment. Our main contributions are:• We provide upper and lower bounds on the optimal sample complexity for multi-agent decision making based on a multi-agent generalization of the Decision-Estimation Coefficient, a complexity measure introduced by Foster et al. (2021) in the single-agent counterpart to our setting. Compared to the best results for the single-agent setting, our upper and lower bounds have additional gaps. We show that no “reasonable” complexity measure can close these gaps, highlighting a striking separation between single and multiple agents.• We show that characterizing the statistical complexity for multi-agent decision making is equivalent to characterizing the statistical complexity of single-agent decision making, but with hidden (unobserved) rewards, a framework that subsumes variants of the partial monitoring problem. As a consequence of this connection, we characterize the statistical complexity for hidden-reward interactive decision making to the best extent possible.Building on this development, we provide several new structural results, including 1) conditions under which the statistical complexity of multi-agent decision making can be reduced to that of single-agent, and 2) conditions under which the so-called curse of multiple agents can be avoided.},
	language = {en},
	urldate = {2025-03-10},
	booktitle = {Proceedings of {Thirty} {Sixth} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Foster, Dean and Foster, Dylan J. and Golowich, Noah and Rakhlin, Alexander},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	keywords = {Game Theory, Mars 2025, Multi-agent},
	pages = {2678--2792},
}

@inproceedings{brown_learning_2021,
	title = {Learning in {Multi}-{Player} {Stochastic} {Games}},
	url = {https://proceedings.mlr.press/v161/brown21a.html},
	abstract = {We consider the problem of simultaneous learning in stochastic games with many players in the finite-horizon setting. While the typical target solution for a stochastic game is a Nash equilibrium, this is intractable with many players. We instead focus on variants of  correlated equilibria, such as those studied for extensive-form games. We begin with a hardness result for the adversarial MDP problem: even for a horizon of 3, obtaining sublinear regret against the best non-stationary policy is NP-hard when both rewards and transitions are adversarial. This implies that convergence to even the weakest natural solution concept—normal-form coarse correlated equilibrium—is not possible via black-box reduction to a no-regret algorithm even in stochastic games with constant horizon (unless \$NP{\textbackslash}subseteqBPP\$). Instead, we turn to a different target: algorithms which  generate an equilibrium when they are used by all players. Our main result is algorithm which generates an  extensive-form correlated equilibrium, whose runtime is exponential in the horizon but polynomial in all other parameters. We give a similar algorithm which is polynomial in all parameters for “fast-mixing” stochastic games. We also show a method for efficiently reaching normal-form coarse correlated equilibria in “single-controller” stochastic games which follows the traditional no-regret approach. When shared randomness is available, the two generative algorithms can be extended to give simultaneous regret bounds and converge in the traditional sense.},
	language = {en},
	urldate = {2025-03-10},
	booktitle = {Proceedings of the {Thirty}-{Seventh} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Brown, William},
	month = dec,
	year = {2021},
	note = {ISSN: 2640-3498},
	keywords = {Game Theory, Mars 2025, Multi-player},
	pages = {1927--1937},
}

@inproceedings{sessa_contextual_2020,
	title = {Contextual {Games}: {Multi}-{Agent} {Learning} with {Side} {Information}},
	volume = {33},
	shorttitle = {Contextual {Games}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/f9afa97535cf7c8789a1c50a2cd83787-Abstract.html},
	abstract = {We formulate the novel class of contextual games, a type of repeated games driven by contextual information at each round. By means of kernel-based regularity assumptions, we model the correlation between different contexts and game outcomes and propose a novel online (meta) algorithm that exploits such correlations to minimize the contextual regret of individual players. We define game-theoretic notions of contextual Coarse Correlated Equilibria (c-CCE) and optimal contextual welfare for this new class of games and show that c-CCEs and optimal welfare can be approached whenever players' contextual regrets vanish. Finally, we empirically validate our results in a traffic routing experiment, where our algorithm leads to better performance and higher welfare compared to baselines that do not exploit the available contextual information or the correlations present in the game.},
	urldate = {2025-03-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sessa, Pier Giuseppe and Bogunovic, Ilija and Krause, Andreas and Kamgarpour, Maryam},
	year = {2020},
	pages = {21912--21922},
}

@article{guo_fair_2024,
	title = {Fair {Probabilistic} {Multi}-{Armed} {Bandit} {With} {Applications} to {Network} {Optimization}},
	volume = {2},
	issn = {2831-316X},
	url = {https://ieeexplore.ieee.org/document/10579843/?arnumber=10579843},
	doi = {10.1109/TMLCN.2024.3421170},
	abstract = {Online learning, particularly Multi-Armed Bandit (MAB) algorithms, has been extensively adopted in various real-world networking applications. In certain applications, such as fair heterogeneous networks coexistence, multiple links (individual arms) are selected in each round, and the throughputs (rewards) of these arms depend on the chosen set of links. Additionally, ensuring fairness among individual arms is a critical objective. However, existing MAB algorithms are unsuitable for these applications due to different models and assumptions. In this paper, we introduce a new fair probabilistic MAB (FP-MAB) problem aimed at either maximizing the minimum reward for all arms or maximizing the total reward while imposing a fairness constraint that guarantees a minimum selection fraction for each arm. In FP-MAB, the learning agent probabilistically selects a meta-arm, which is associated with one or multiple individual arms in each decision round. To address the FP-MAB problem, we propose two algorithms: Fair Probabilistic Explore-Then-Commit (FP-ETC) and Fair Probabilistic Optimism In the Face of Uncertainty (FP-OFU). We also introduce a novel concept of regret in the context of the max-min fairness objective. We analyze the performance of FP-ETC and FP-OFU in terms of the upper bound of average regret and average constraint violation. Simulation results demonstrate that FP-ETC and FP-OFU achieve lower regrets (or higher objective values) under the same fairness requirements compared to existing MAB algorithms.},
	urldate = {2025-03-10},
	journal = {IEEE Transactions on Machine Learning in Communications and Networking},
	author = {Guo, Zhiwu and Zhang, Chicheng and Li, Ming and Krunz, Marwan},
	year = {2024},
	note = {Conference Name: IEEE Transactions on Machine Learning in Communications and Networking},
	keywords = {Energy harvesting, Machine learning, Mars 2025, Minimax techniques, Multi-player, Probabilistic logic, Probabilistic multi-armed bandit, Throughput, Wireless communication, Wireless sensor networks, explore-then-commit, fairness constraint, max-min fairness, online learning, optimism in the face of uncertainty},
	pages = {994--1016},
}

@inproceedings{shao_risk-aware_2024,
	address = {Athens Greece},
	title = {Risk-{Aware} {Multi}-{Agent} {Multi}-{Armed} {Bandits}},
	isbn = {979-8-4007-0521-2},
	url = {https://dl.acm.org/doi/10.1145/3641512.3686368},
	doi = {10.1145/3641512.3686368},
	language = {en},
	urldate = {2025-03-10},
	booktitle = {Proceedings of the {Twenty}-fifth {International} {Symposium} on {Theory}, {Algorithmic} {Foundations}, and {Protocol} {Design} for {Mobile} {Networks} and {Mobile} {Computing}},
	publisher = {ACM},
	author = {Shao, Qi and Ye, Jiancheng and Lui, John C. S.},
	month = oct,
	year = {2024},
	pages = {61--70},
}

@inproceedings{chakraborty_coordinated_2017,
	address = {Melbourne, Australia},
	title = {Coordinated {Versus} {Decentralized} {Exploration} {In} {Multi}-{Agent} {Multi}-{Armed} {Bandits}},
	isbn = {978-0-9992411-0-3},
	url = {https://www.ijcai.org/proceedings/2017/24},
	doi = {10.24963/ijcai.2017/24},
	abstract = {In this paper, we introduce a multi-agent multiarmed bandit-based model for ad hoc teamwork with expensive communication. The goal of the team is to maximize the total reward gained from pulling arms of a bandit over a number of epochs. In each epoch, each agent decides whether to pull an arm, or to broadcast the reward it obtained in the previous epoch to the team and forgo pulling an arm. These decisions must be made only on the basis of the agent’s private information and the public information broadcast prior to that epoch. We ﬁrst benchmark the achievable utility by analyzing an idealized version of this problem where a central authority has complete knowledge of rewards acquired from all arms in all epochs and uses a multiplicative weights update algorithm for allocating arms to agents. We then introduce an algorithm for the decentralized setting that uses a value-ofinformation based communication strategy and an exploration-exploitation strategy based on the centralized algorithm, and show experimentally that it converges rapidly to the performance of the centralized method.},
	language = {en},
	urldate = {2025-03-10},
	booktitle = {Proceedings of the {Twenty}-{Sixth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Chakraborty, Mithun and Chua, Kai Yee Phoebe and Das, Sanmay and Juba, Brendan},
	month = aug,
	year = {2017},
	keywords = {Mars 2025, Multi-agent, Multi-player},
	pages = {164--170},
}

@misc{taywade_modelling_2022,
	title = {Modelling {Cournot} {Games} as {Multi}-agent {Multi}-armed {Bandits}},
	url = {http://arxiv.org/abs/2201.01182},
	doi = {10.48550/arXiv.2201.01182},
	abstract = {We investigate the use of a multi-agent multi-armed bandit (MA-MAB) setting for modeling repeated Cournot oligopoly games, where the firms acting as agents choose from the set of arms representing production quantity (a discrete value). Agents interact with separate and independent bandit problems. In this formulation, each agent makes sequential choices among arms to maximize its own reward. Agents do not have any information about the environment; they can only see their own rewards after taking an action. However, the market demand is a stationary function of total industry output, and random entry or exit from the market is not allowed. Given these assumptions, we found that an \${\textbackslash}epsilon\$-greedy approach offers a more viable learning mechanism than other traditional MAB approaches, as it does not require any additional knowledge of the system to operate. We also propose two novel approaches that take advantage of the ordered action space: \${\textbackslash}epsilon\$-greedy+HL and \${\textbackslash}epsilon\$-greedy+EL. These new approaches help firms to focus on more profitable actions by eliminating less profitable choices and hence are designed to optimize the exploration. We use computer simulations to study the emergence of various equilibria in the outcomes and do the empirical analysis of joint cumulative regrets.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Taywade, Kshitija and Harrison, Brent and Bagh, Adib},
	month = jan,
	year = {2022},
	note = {arXiv:2201.01182 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Economics - Econometrics, Game Theory, Mars 2025, Multi-agent, Nash equilibrium},
}

@article{bistritz_one_2021,
	title = {One for {All} and {All} for {One}: {Distributed} {Learning} of {Fair} {Allocations} {With} {Multi}-{Player} {Bandits}},
	volume = {2},
	issn = {2641-8770},
	shorttitle = {One for {All} and {All} for {One}},
	url = {https://ieeexplore.ieee.org/document/9404291/?arnumber=9404291},
	doi = {10.1109/JSAIT.2021.3073065},
	abstract = {Consider N cooperative but non-communicating players where each plays one out of M arms for T turns. Players have different utilities for each arm, represented as an N×M matrix. These utilities are unknown to the players. In each turn, players select an arm and receive a noisy observation of their utility for it. However, if any other players selected the same arm in that turn, all colliding players will receive zero utility due to the conflict. No communication between the players is possible. We propose two distributed algorithms which learn fair matchings between players and arms while minimizing the regret. We show that our first algorithm learns a max-min fairness matching with near- O(logT) regret (up to a loglogT factor). However, if one has a known target Quality of Service (QoS) (which may vary between players) then we show that our second algorithm learns a matching where all players obtain an expected reward of at least their QoS with constant regret, given that such a matching exists. In particular, if the max-min value is known, a max-min fairness matching can be learned with O(1) regret.},
	number = {2},
	urldate = {2025-03-10},
	journal = {IEEE Journal on Selected Areas in Information Theory},
	author = {Bistritz, Ilai and Baharav, Tavor Z. and Leshem, Amir and Bambos, Nicholas},
	month = jun,
	year = {2021},
	note = {Conference Name: IEEE Journal on Selected Areas in Information Theory},
	keywords = {Games, Information theory, Mars 2025, Multi-player, Multi-player bandits, Protocols, Quality of service, Resource management, Task analysis, Throughput, distributed learning, fairness, online learning, resource allocation},
	pages = {584--598},
}

@misc{xu_regret_2023,
	title = {Regret {Lower} {Bounds} in {Multi}-agent {Multi}-armed {Bandit}},
	url = {http://arxiv.org/abs/2308.08046},
	doi = {10.48550/arXiv.2308.08046},
	abstract = {Multi-armed Bandit motivates methods with provable upper bounds on regret and also the counterpart lower bounds have been extensively studied in this context. Recently, Multi-agent Multi-armed Bandit has gained significant traction in various domains, where individual clients face bandit problems in a distributed manner and the objective is the overall system performance, typically measured by regret. While efficient algorithms with regret upper bounds have emerged, limited attention has been given to the corresponding regret lower bounds, except for a recent lower bound for adversarial settings, which, however, has a gap with let known upper bounds. To this end, we herein provide the first comprehensive study on regret lower bounds across different settings and establish their tightness. Specifically, when the graphs exhibit good connectivity properties and the rewards are stochastically distributed, we demonstrate a lower bound of order \$O({\textbackslash}log T)\$ for instance-dependent bounds and \${\textbackslash}sqrt\{T\}\$ for mean-gap independent bounds which are tight. Assuming adversarial rewards, we establish a lower bound \$O(T{\textasciicircum}\{{\textbackslash}frac\{2\}\{3\}\})\$ for connected graphs, thereby bridging the gap between the lower and upper bound in the prior work. We also show a linear regret lower bound when the graph is disconnected. While previous works have explored these settings with upper bounds, we provide a thorough study on tight lower bounds.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Xu, Mengfan and Klabjan, Diego},
	month = aug,
	year = {2023},
	note = {arXiv:2308.08046 [cs]},
	keywords = {Computer Science - Machine Learning, Mars 2025, Multi-player, Statistics - Machine Learning},
}

@inproceedings{bistritz_distributed_2018,
	title = {Distributed {Multi}-{Player} {Bandits} - a {Game} of {Thrones} {Approach}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/c2964caac096f26db222cb325aa267cb-Abstract.html},
	abstract = {We consider a multi-armed bandit game where N players compete for K arms for T turns. Each player has different expected rewards for the arms, and the instantaneous rewards are independent and identically distributed. Performance is measured using the expected sum of regrets, compared to the optimal assignment of arms to players. We assume that each player only knows her actions and the reward she received each turn. Players cannot observe the actions of other players, and no communication between players is possible. We present a distributed algorithm and prove that it achieves an expected sum of regrets of near-O{\textbackslash}left({\textbackslash}log{\textasciicircum}\{2\}T{\textbackslash}right). This is the first algorithm to achieve a poly-logarithmic regret in this fully distributed scenario. All other works have assumed that either all players have the same vector of expected rewards or that communication between players is possible.},
	urldate = {2025-03-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bistritz, Ilai and Leshem, Amir},
	year = {2018},
	keywords = {Mars 2025, Multi-player},
}

@inproceedings{bistritz_cooperative_2020,
	title = {Cooperative {Multi}-player {Bandit} {Optimization}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/15ae3b9d6286f1b2a489ea4f3f4abaed-Abstract.html},
	abstract = {Consider a team of cooperative players that take actions in a networked-environment. At each turn, each player chooses an action and receives a reward that is an unknown function of all the players' actions. The goal of the team of players is to learn to play together the action profile that maximizes the sum of their rewards. However, players cannot observe the actions or rewards of other players, and can only get this information by communicating with their neighbors. We design a distributed learning algorithm that overcomes the informational bias players have towards maximizing the rewards of nearby players they got more information about. We assume twice continuously differentiable reward functions and constrained convex and compact action sets. Our communication graph is a random time-varying graph that follows an ergodic Markov chain. We prove that even if at every turn players take actions based only on the small random subset of the players' rewards that they know, our algorithm converges with probability 1 to the set of stationary points of (projected) gradient ascent on the sum of rewards function. Hence, if the sum of rewards is concave, then the algorithm converges with probability 1 to the optimal action profile.},
	urldate = {2025-03-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bistritz, Ilai and Bambos, Nicholas},
	year = {2020},
	keywords = {Congestion game, Equilibrium, MAB, Mars 2025, Multi-player, Public good game},
	pages = {2016--2027},
}

@book{sirakov_proceedings_2019,
	title = {Proceedings {Of} {The} {International} {Congress} {Of} {Mathematicians} 2018 ({Icm} 2018) ({In} 4 {Volumes})},
	isbn = {978-981-327-289-7},
	abstract = {The Proceedings of the ICM publishes the talks, by invited speakers, at the conference organized by the International Mathematical Union every 4 years. It covers several areas of Mathematics and it includes the Fields Medal and Nevanlinna, Gauss and Leelavati Prizes and the Chern Medal laudatios.},
	language = {en},
	publisher = {World Scientific},
	author = {Sirakov, Boyan and Souza, Paulo Ney De and Viana, Marcelo},
	month = feb,
	year = {2019},
	note = {Google-Books-ID: iO60DwAAQBAJ},
	keywords = {Mars 2025, Mathematics / Applied, Mathematics / General, Mathematics / Research, Multi-player, RPS},
}

@misc{bistritz_my_2020,
	title = {My {Fair} {Bandit}: {Distributed} {Learning} of {Max}-{Min} {Fairness} with {Multi}-player {Bandits}},
	shorttitle = {My {Fair} {Bandit}},
	url = {http://arxiv.org/abs/2002.09808},
	doi = {10.48550/arXiv.2002.09808},
	abstract = {Consider N cooperative but non-communicating players where each plays one out of M arms for T turns. Players have different utilities for each arm, representable as an NxM matrix. These utilities are unknown to the players. In each turn players select an arm and receive a noisy observation of their utility for it. However, if any other players selected the same arm that turn, all colliding players will all receive zero utility due to the conflict. No other communication or coordination between the players is possible. Our goal is to design a distributed algorithm that learns the matching between players and arms that achieves max-min fairness while minimizing the regret. We present an algorithm and prove that it is regret optimal up to a \${\textbackslash}log{\textbackslash}log T\$ factor. This is the first max-min fairness multi-player bandit algorithm with (near) order optimal regret.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Bistritz, Ilai and Baharav, Tavor Z. and Leshem, Amir and Bambos, Nicholas},
	month = aug,
	year = {2020},
	note = {arXiv:2002.09808 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Multiagent Systems, Feedback loop, MAB, Mars 2025, Multi-player},
}

@misc{sarkar_bandit_2023,
	title = {Bandit based centralized matching in two-sided markets for peer to peer lending},
	url = {http://arxiv.org/abs/2105.02589},
	doi = {10.48550/arXiv.2105.02589},
	abstract = {Sequential fundraising in two sided online platforms enable peer to peer lending by sequentially bringing potential contributors, each of whose decisions impact other contributors in the market. However, understanding the dynamics of sequential contributions in online platforms for peer lending has been an open ended research question. The centralized investment mechanism in these platforms makes it difficult to understand the implicit competition that borrowers face from a single lender at any point in time. Matching markets are a model of pairing agents where the preferences of agents from both sides in terms of their preferred pairing for transactions can allow to decentralize the market. We study investment designs in two sided platforms using matching markets when the investors or lenders also face restrictions on the investments based on borrower preferences. This situation creates an implicit competition among the lenders in addition to the existing borrower competition, especially when the lenders are uncertain about their standing in the market and thereby the probability of their investments being accepted or the borrower loan requests for projects reaching the reserve price. We devise a technique based on sequential decision making that allows the lenders to adjust their choices based on the dynamics of uncertainty from competition over time. We simulate two sided market matchings in a sequential decision framework and show the dynamics of the lender regret amassed compared to the optimal borrower-lender matching and find that the lender regret depends on the initial preferences set by the lenders which could affect their learning over decision making steps.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Sarkar, Soumajyoti},
	month = aug,
	year = {2023},
	note = {arXiv:2105.02589 [cs]},
	keywords = {Computer Science - Machine Learning, MAB, Mars 2025, Pricing},
}

@article{mell_doctor_nodate,
	title = {{DOCTOR} {OF} {PHILOSOPHY} {COMPUTER} {SCIENCE}},
	language = {en},
	author = {Mell, Johnathan T},
	keywords = {Mars 2025},
}

@misc{agarwal_online_2024,
	title = {Online {Recommendations} for {Agents} with {Discounted} {Adaptive} {Preferences}},
	url = {http://arxiv.org/abs/2302.06014},
	doi = {10.48550/arXiv.2302.06014},
	abstract = {We consider a bandit recommendations problem in which an agent's preferences (representing selection probabilities over recommended items) evolve as a function of past selections, according to an unknown \${\textbackslash}textit\{preference model\}\$. In each round, we show a menu of \$k\$ items (out of \$n\$ total) to the agent, who then chooses a single item, and we aim to minimize regret with respect to some \${\textbackslash}textit\{target set\}\$ (a subset of the item simplex) for adversarial losses over the agent's choices. Extending the setting from Agarwal and Brown (2022), where uniform-memory agents were considered, here we allow for non-uniform memory in which a discount factor is applied to the agent's memory vector at each subsequent round. In the "long-term memory" regime (when the effective memory horizon scales with \$T\$ sublinearly), we show that efficient sublinear regret is obtainable with respect to the set of \${\textbackslash}textit\{everywhere instantaneously realizable distributions\}\$ (the "EIRD set", as formulated in prior work) for any \${\textbackslash}textit\{smooth\}\$ preference model. Further, for preferences which are bounded above and below by linear functions of memory weight (we call these "scale-bounded" preferences) we give an algorithm which obtains efficient sublinear regret with respect to nearly the \${\textbackslash}textit\{entire\}\$ item simplex. We show an NP-hardness result for expanding to targets beyond EIRD in general. In the "short-term memory" regime (when the memory horizon is constant), we show that scale-bounded preferences again enable efficient sublinear regret for nearly the entire simplex even without smoothness if losses do not change too frequently, yet we show an information-theoretic barrier for competing against the EIRD set under arbitrary smooth preference models even when losses are constant.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Agarwal, Arpit and Brown, William},
	month = feb,
	year = {2024},
	note = {arXiv:2302.06014 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Information Retrieval, Computer Science - Machine Learning, EXP3, MAB, Multi-agent},
}

@article{toyokawa_conformist_2022,
	title = {Conformist social learning leads to self-organised prevention against adverse bias in risky decision making},
	volume = {11},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.75308},
	doi = {10.7554/eLife.75308},
	abstract = {Given the ubiquity of potentially adverse behavioural bias owing to myopic trial-and-error learning, it seems paradoxical that improvements in decision-making performance through conformist social learning, a process widely considered to be bias amplification, still prevail in animal collective behaviour. Here we show, through model analyses and large-scale interactive behavioural experiments with 585 human subjects, that conformist influence can indeed promote favourable risk taking in repeated experience-based decision making, even though many individuals are systematically biased towards adverse risk aversion. Although strong positive feedback conferred by copying the majority’s behaviour could result in unfavourable informational cascades, our differential equation model of collective behavioural dynamics identified a key role for increasing exploration by negative feedback arising when a weak minority influence undermines the inherent behavioural bias. This ‘collective behavioural rescue’, emerging through coordination of positive and negative feedback, highlights a benefit of collective learning in a broader range of environmental conditions than previously assumed and resolves the ostensible paradox of adaptive collective behavioural flexibility under conformist influences.},
	urldate = {2025-03-10},
	journal = {eLife},
	author = {Toyokawa, Wataru and Gaissmaier, Wolfgang},
	editor = {Liljeholm, Mimi and Frank, Michael J},
	month = may,
	year = {2022},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {Feedback loop, MAB, Mars 2025, collective behaviour, conformity, hot stove effect, reinforcement learning, risky decision making, social learning},
	pages = {e75308},
}

@article{sliwinski_competitive_2025,
	title = {Competitive {Pricing} {Using} {Model}-{Based} {Bandits}},
	issn = {0927-7099, 1572-9974},
	url = {https://link.springer.com/10.1007/s10614-024-10816-w},
	doi = {10.1007/s10614-024-10816-w},
	abstract = {The use of learning algorithms for automatic price adjustments in markets is on the rise. However, these algorithms often assume that reward distributions for actions are uncorrelated and stationary, a condition that does not hold in competitive pricing environments. In this paper, we introduce a pricing environment, find conditions under which a unique Nash equilibrium exists and verify the assumptions numerically. Then, we propose a bandit algorithm that approximates the structure of the environment and extend it to accommodate non-stationary settings. We perform numerical tests in both stationary and competitive pricing environments, analysing the potential benefits and drawbacks of incorporating the structure of the environment within learning algorithms. While modelling the stationary environment improves the algorithm’s performance in a stationary setting, it does not offer an advantage in pricing competitions between non-stationary learning agents.},
	language = {en},
	urldate = {2025-03-10},
	journal = {Computational Economics},
	author = {Sliwinski, Lukasz and Treetanthiploet, Tanut and Siska, David and Szpruch, Lukasz},
	month = feb,
	year = {2025},
	keywords = {Game Theory, Multi-agent, Pricing},
}

@article{klugl_modelling_2023,
	title = {Modelling {Agent} {Decision} {Making} in {Agent}-based {Simulation} - {Analysis} {Using} an {Economic} {Technology} {Uptake} {Model}},
	abstract = {Agent-based Simulation Modelling focuses on the agents’ decision making in their individual context. The decision making details may substantially affect the simulation outcome, and therefore need to be carefully designed.},
	language = {en},
	author = {Klügl, Franziska},
	year = {2023},
	keywords = {Evolutionnary game, Mars 2025, Multi-agent},
}

@misc{rahman_improving_2021,
	title = {Improving {Spectral} {Efficiency} of {Wireless} {Networks} through {Democratic} {Spectrum} {Sharing}},
	url = {http://arxiv.org/abs/2111.10570},
	doi = {10.48550/arXiv.2111.10570},
	abstract = {Wireless devices need spectrum to communicate. With the increase in the number of devices competing for the same spectrum, it has become nearly impossible to support the throughput requirements of all the devices through current spectrum sharing methods. In this work, we look at the problem of spectrum resource contention fundamentally, taking inspiration from the principles of globalization. We develop a distributed algorithm whereby the wireless nodes democratically share the spectrum resources and improve their spectral efficiency and throughput without additional power or spectrum resources. We validate the performance of our proposed democratic spectrum sharing (DSS) algorithm over real-world Wi-Fi networks and on synthetically generated networks with varying design parameters. Compared to the greedy approach, DSS achieves significant gains in throughput ({\textasciitilde}60\%), area spectral efficiency (\${\textbackslash}sim\$50{\textbackslash}\%) and fairness in datarate distribution ({\textasciitilde}20\%). Due to the distributed nature of the proposed algorithm, we can apply it to wireless networks of any size and density.},
	urldate = {2025-03-10},
	publisher = {arXiv},
	author = {Rahman, Aniq Ur and Kishk, Mustafa A. and Alouini, Mohamed-Slim},
	month = nov,
	year = {2021},
	note = {arXiv:2111.10570 [cs]},
	keywords = {Computer Science - Multiagent Systems, Computer Science - Networking and Internet Architecture, Computer Science - Systems and Control, Electrical Engineering and Systems Science - Systems and Control, Spectrum sharing},
}

@article{rossi_closed_2022,
	title = {The {Closed} {Loop} {Between} {Opinion} {Formation} and {Personalized} {Recommendations}},
	volume = {9},
	issn = {2325-5870},
	url = {https://ieeexplore.ieee.org/document/9516926/?arnumber=9516926},
	doi = {10.1109/TCNS.2021.3105616},
	abstract = {In online platforms, recommender systems are responsible for directing users to relevant content. In order to enhance the users’ engagement, recommender systems adapt their output to the reactions of the users, who are, in turn, affected by the recommended content. In this article, we study a tractable analytical model of a user that interacts with an online news aggregator, with the purpose of making explicit the feedback loop between the evolution of the user’s opinion and the personalized recommendation of content. More specifically, we assume that the user is endowed with a scalar opinion about a certain issue and receives news about it from a news aggregator: her opinion is influenced by all received pieces of news, which are characterized by a binary position on the issue at hand. The user is affected by a confirmation bias, that is, a preference for news that confirms her current opinion. The news aggregator recommends items with the goal of maximizing the number of user’s clicks (as a measure of her engagement): in order to fulfill its goal, the recommender has to compromise between exploring the user’s preferences and exploiting what it has learned so far. After defining suitable metrics for the effectiveness of the recommender systems (such as the click-through rate) and for its impact on the opinion, we perform both extensive numerical simulations and a mathematical analysis of the model. We find that personalized recommendations markedly affect the evolution of opinions and favor the emergence of more extreme ones: the intensity of these effects is inherently related to the effectiveness of the recommender. We also show that by tuning the amount of randomness in the recommendation algorithm, one can seek a balance between the effectiveness of the recommendation system and its impact on the opinions.},
	number = {3},
	urldate = {2025-03-10},
	journal = {IEEE Transactions on Control of Network Systems},
	author = {Rossi, Wilbert Samuel and Polderman, Jan Willem and Frasca, Paolo},
	month = sep,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Control of Network Systems},
	keywords = {Analytical models, Art, Control systems, Data models, Epsillon, Feedback loop, Mars 2025, Mathematical model, Numerical models, Recommender systems, networked control systems},
	pages = {1092--1103},
}

@article{lu_photon-atom_2025,
	title = {Photon-{Atom} {Hybrid} {Decision}-{Framework} with {Concurrent} {Exploration} {Acceleration}},
	copyright = {https://doi.org/10.15223/policy-029},
	issn = {2330-4022, 2330-4022},
	url = {https://pubs.acs.org/doi/10.1021/acsphotonics.4c02467},
	doi = {10.1021/acsphotonics.4c02467},
	language = {en},
	urldate = {2025-03-10},
	journal = {ACS Photonics},
	author = {Lu, Feng and Dou, Jian-Peng and Tang, Hao and Xu, Xiao-Yun and Zhang, Chao-Ni and Zhou, Wen-Hao and Sun, Hong and Jin, Xian-Min},
	month = feb,
	year = {2025},
	keywords = {Exploration, Mars 2025, Multi-agent},
	pages = {acsphotonics.4c02467},
}

@misc{krishnamurthy_dynamics_2022,
	title = {Dynamics of {Social} {Networks}: {Multi}-agent {Information} {Fusion}, {Anticipatory} {Decision} {Making} and {Polling}},
	shorttitle = {Dynamics of {Social} {Networks}},
	url = {http://arxiv.org/abs/2212.13323},
	doi = {10.48550/arXiv.2212.13323},
	abstract = {This paper surveys mathematical models, structural results and algorithms in controlled sensing with social learning in social networks. Part 1, namely Bayesian Social Learning with Controlled Sensing addresses the following questions: How does risk averse behavior in social learning affect quickest change detection? How can information fusion be priced? How is the convergence rate of state estimation affected by social learning? The aim is to develop and extend structural results in stochastic control and Bayesian estimation to answer these questions. Such structural results yield fundamental bounds on the optimal performance, give insight into what parameters affect the optimal policies, and yield computationally efficient algorithms. Part 2, namely, Multi-agent Information Fusion with Behavioral Economics Constraints generalizes Part 1. The agents exhibit sophisticated decision making in a behavioral economics sense; namely the agents make anticipatory decisions (thus the decision strategies are time inconsistent and interpreted as subgame Bayesian Nash equilibria). Part 3, namely \{{\textbackslash}em Interactive Sensing in Large Networks\}, addresses the following questions: How to track the degree distribution of an infinite random graph with dynamics (via a stochastic approximation on a Hilbert space)? How can the infected degree distribution of a Markov modulated power law network and its mean field dynamics be tracked via Bayesian filtering given incomplete information obtained by sampling the network? We also briefly discuss how the glass ceiling effect emerges in social networks. Part 4, namely {\textbackslash}emph\{Efficient Network Polling\} deals with polling in large scale social networks. In such networks, only a fraction of nodes can be polled to determine their decisions. Which nodes should be polled to achieve a statistically accurate estimates?},
	urldate = {2025-03-09},
	publisher = {arXiv},
	author = {Krishnamurthy, Vikram},
	month = dec,
	year = {2022},
	note = {arXiv:2212.13323 [eess]},
	keywords = {Electrical Engineering and Systems Science - Signal Processing, Mars 2025, Multi-agent},
}

@article{puranik_long-term_2024,
	title = {Long-{Term} {Fairness} in {Sequential} {Multi}-{Agent} {Selection} with {Positive} {Reinforcement}},
	volume = {5},
	issn = {2641-8770},
	url = {http://arxiv.org/abs/2407.07350},
	doi = {10.1109/JSAIT.2024.3416078},
	abstract = {While much of the rapidly growing literature on fair decision-making focuses on metrics for one-shot decisions, recent work has raised the intriguing possibility of designing sequential decision-making to positively impact long-term social fairness. In selection processes such as college admissions or hiring, biasing slightly towards applicants from under-represented groups is hypothesized to provide positive feedback that increases the pool of under-represented applicants in future selection rounds, thus enhancing fairness in the long term. In this paper, we examine this hypothesis and its consequences in a setting in which multiple agents are selecting from a common pool of applicants. We propose the Multi-agent Fair-Greedy policy, that balances greedy score maximization and fairness. Under this policy, we prove that the resource pool and the admissions converge to a long-term fairness target set by the agents when the score distributions across the groups in the population are identical. We provide empirical evidence of existence of equilibria under non-identical score distributions through synthetic and adapted real-world datasets. We then sound a cautionary note for more complex applicant pool evolution models, under which uncoordinated behavior by the agents can cause negative reinforcement, leading to a reduction in the fraction of under-represented applicants. Our results indicate that, while positive reinforcement is a promising mechanism for long-term fairness, policies must be designed carefully to be robust to variations in the evolution model, with a number of open issues that remain to be explored by algorithm designers, social scientists, and policymakers.},
	urldate = {2025-03-09},
	journal = {IEEE Journal on Selected Areas in Information Theory},
	author = {Puranik, Bhagyashree and Guldogan, Ozgur and Madhow, Upamanyu and Pedarsani, Ramtin},
	year = {2024},
	note = {arXiv:2407.07350 [stat]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Feedback loop, Mars 2025, Multi-agent, Statistics - Machine Learning},
	pages = {424--441},
}

@inproceedings{puranik_dynamic_2022,
	address = {Oxford United Kingdom},
	title = {A {Dynamic} {Decision}-{Making} {Framework} {Promoting} {Long}-{Term} {Fairness}},
	isbn = {978-1-4503-9247-1},
	url = {https://dl.acm.org/doi/10.1145/3514094.3534127},
	doi = {10.1145/3514094.3534127},
	language = {en},
	urldate = {2025-03-09},
	booktitle = {Proceedings of the 2022 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Puranik, Bhagyashree and Madhow, Upamanyu and Pedarsani, Ramtin},
	month = jul,
	year = {2022},
	pages = {547--556},
}

@inproceedings{tang_bandit_2021,
	title = {Bandit {Learning} with {Delayed} {Impact} of {Actions}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/e17184bcb70dcf3942c54e0b537ffc6d-Abstract.html},
	urldate = {2025-03-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tang, Wei and Ho, Chien-Ju and Liu, Yang},
	year = {2021},
	keywords = {Bandit, Feedback loop, Mars 2025},
	pages = {26804--26817},
}

@article{shao_balanced_2024,
	title = {Balanced and {Incentivized} {Learning} with {Limited} {Shared} {Information} in {Multi}-agent {Multi}-armed {Bandit}},
	abstract = {Multi-agent multi-armed bandit (MAMAB) is a classic collaborative learning model and has gained much attention in recent years. However, existing studies do not consider the case where an agent may refuse to share all her information with others, e.g., when some of the data contains personal privacy. In this paper, we propose a novel limited shared information multi-agent multi-armed bandit (LSI-MAMAB) model in which each agent only shares the information that she is willing to share, and propose the BalancedETC algorithm to help multiple agents collaborate efficiently with limited shared information. Our analysis shows that Balanced-ETC is asymptotically optimal, and its average regret (on each agent) approaches a constant when there are sufficient agents involved. Moreover, to encourage agents to participate in this collaborative learning, an incentive mechanism is proposed to make sure each agent can benefit from the collaboration system. Finally, we present experimental results to validate our theoretical results.},
	language = {en},
	journal = {New Zealand},
	author = {Shao, Junning},
	year = {2024},
}

@misc{boursier_sic-mmab_2019,
	title = {{SIC}-{MMAB}: {Synchronisation} {Involves} {Communication} in {Multiplayer} {Multi}-{Armed} {Bandits}},
	shorttitle = {{SIC}-{MMAB}},
	url = {http://arxiv.org/abs/1809.08151},
	doi = {10.48550/arXiv.1809.08151},
	abstract = {Motivated by cognitive radio networks, we consider the stochastic multiplayer multi-armed bandit problem, where several players pull arms simultaneously and collisions occur if one of them is pulled by several players at the same stage. We present a decentralized algorithm that achieves the same performance as a centralized one, contradicting the existing lower bounds for that problem. This is possible by "hacking" the standard model by constructing a communication protocol between players that deliberately enforces collisions, allowing them to share their information at a negligible cost. This motivates the introduction of a more appropriate dynamic setting without sensing, where similar communication protocols are no longer possible. However, we show that the logarithmic growth of the regret is still achievable for this model with a new algorithm.},
	urldate = {2025-03-05},
	publisher = {arXiv},
	author = {Boursier, Etienne and Perchet, Vianney},
	month = nov,
	year = {2019},
	note = {arXiv:1809.08151 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{giannou_convergence_2022,
	title = {On the convergence of policy gradient methods to {Nash} equilibria in general stochastic games},
	url = {http://arxiv.org/abs/2210.08857},
	doi = {10.48550/arXiv.2210.08857},
	abstract = {Learning in stochastic games is a notoriously difficult problem because, in addition to each other's strategic decisions, the players must also contend with the fact that the game itself evolves over time, possibly in a very complicated manner. Because of this, the convergence properties of popular learning algorithms - like policy gradient and its variants - are poorly understood, except in specific classes of games (such as potential or two-player, zero-sum games). In view of this, we examine the long-run behavior of policy gradient methods with respect to Nash equilibrium policies that are second-order stationary (SOS) in a sense similar to the type of sufficiency conditions used in optimization. Our first result is that SOS policies are locally attracting with high probability, and we show that policy gradient trajectories with gradient estimates provided by the REINFORCE algorithm achieve an \${\textbackslash}mathcal\{O\}(1/{\textbackslash}sqrt\{n\})\$ distance-squared convergence rate if the method's step-size is chosen appropriately. Subsequently, specializing to the class of deterministic Nash policies, we show that this rate can be improved dramatically and, in fact, policy gradient methods converge within a finite number of iterations in that case.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Giannou, Angeliki and Lotidis, Kyriakos and Mertikopoulos, Panayotis and Vlatakis-Gkaragkounis, Emmanouil-Vasileios},
	month = oct,
	year = {2022},
	note = {arXiv:2210.08857 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@misc{hsieh_riemannian_2023,
	title = {Riemannian stochastic optimization methods avoid strict saddle points},
	url = {http://arxiv.org/abs/2311.02374},
	doi = {10.48550/arXiv.2311.02374},
	abstract = {Many modern machine learning applications - from online principal component analysis to covariance matrix identification and dictionary learning - can be formulated as minimization problems on Riemannian manifolds, and are typically solved with a Riemannian stochastic gradient method (or some variant thereof). However, in many cases of interest, the resulting minimization problem is not geodesically convex, so the convergence of the chosen solver to a desirable solution - i.e., a local minimizer - is by no means guaranteed. In this paper, we study precisely this question, that is, whether stochastic Riemannian optimization algorithms are guaranteed to avoid saddle points with probability 1. For generality, we study a family of retraction-based methods which, in addition to having a potentially much lower per-iteration cost relative to Riemannian gradient descent, include other widely used algorithms, such as natural policy gradient methods and mirror descent in ordinary convex spaces. In this general setting, we show that, under mild assumptions for the ambient manifold and the oracle providing gradient information, the policies under study avoid strict saddle points / submanifolds with probability 1, from any initial condition. This result provides an important sanity check for the use of gradient methods on manifolds as it shows that, almost always, the limit state of a stochastic Riemannian algorithm can only be a local minimizer.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Hsieh, Ya-Ping and Karimi, Mohammad Reza and Krause, Andreas and Mertikopoulos, Panayotis},
	month = nov,
	year = {2023},
	note = {arXiv:2311.02374 [math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@misc{boone_equivalence_2023,
	title = {The equivalence of dynamic and strategic stability under regularized learning in games},
	url = {http://arxiv.org/abs/2311.02407},
	doi = {10.48550/arXiv.2311.02407},
	abstract = {In this paper, we examine the long-run behavior of regularized, no-regret learning in finite games. A well-known result in the field states that the empirical frequencies of no-regret play converge to the game's set of coarse correlated equilibria; however, our understanding of how the players' actual strategies evolve over time is much more limited - and, in many cases, non-existent. This issue is exacerbated further by a series of recent results showing that only strict Nash equilibria are stable and attracting under regularized learning, thus making the relation between learning and pointwise solution concepts particularly elusive. In lieu of this, we take a more general approach and instead seek to characterize the {\textbackslash}emph\{setwise\} rationality properties of the players' day-to-day play. To that end, we focus on one of the most stringent criteria of setwise strategic stability, namely that any unilateral deviation from the set in question incurs a cost to the deviator - a property known as closedness under better replies (club). In so doing, we obtain a far-reaching equivalence between strategic and dynamic stability: a product of pure strategies is closed under better replies if and only if its span is stable and attracting under regularized learning. In addition, we estimate the rate of convergence to such sets, and we show that methods based on entropic regularization (like the exponential weights algorithm) converge at a geometric rate, while projection-based methods converge within a finite number of iterations, even with bandit, payoff-based feedback.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Boone, Victor and Mertikopoulos, Panayotis},
	month = nov,
	year = {2023},
	note = {arXiv:2311.02407 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@misc{falniowski_discrete-time_2024,
	title = {On the discrete-time origins of the replicator dynamics: {From} convergence to instability and chaos},
	shorttitle = {On the discrete-time origins of the replicator dynamics},
	url = {http://arxiv.org/abs/2402.09824},
	doi = {10.48550/arXiv.2402.09824},
	abstract = {We consider three distinct discrete-time models of learning and evolution in games: a biological model based on intra-species selective pressure, the dynamics induced by pairwise proportional imitation, and the exponential / multiplicative weights (EW) algorithm for online learning. Even though these models share the same continuous-time limit - the replicator dynamics - we show that second-order effects play a crucial role and may lead to drastically different behaviors in each model, even in very simple, symmetric \$2{\textbackslash}times2\$ games. Specifically, we study the resulting discrete-time dynamics in a class of parametrized congestion games, and we show that (i) in the biological model of intra-species competition, the dynamics remain convergent for any parameter value; (ii) the dynamics of pairwise proportional imitation exhibit an entire range of behaviors for larger time steps and different equilibrium configurations (stability, instability, and even Li-Yorke chaos); while (iii) in the EW algorithm, increasing the time step (almost) inevitably leads to chaos (again, in the formal, Li-Yorke sense). This divergence of behaviors comes in stark contrast to the globally convergent behavior of the replicator dynamics, and serves to delineate the extent to which the replicator dynamics provide a useful predictor for the long-run behavior of their discrete-time origins.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Falniowski, Fryderyk and Mertikopoulos, Panayotis},
	month = feb,
	year = {2024},
	note = {arXiv:2402.09824 [math]},
	keywords = {Computer Science - Computer Science and Game Theory, Mathematics - Dynamical Systems},
}

@misc{legacci_geometric_2024,
	title = {A geometric decomposition of finite games: {Convergence} vs. recurrence under exponential weights},
	shorttitle = {A geometric decomposition of finite games},
	url = {http://arxiv.org/abs/2405.07224},
	doi = {10.48550/arXiv.2405.07224},
	abstract = {In view of the complexity of the dynamics of learning in games, we seek to decompose a game into simpler components where the dynamics' long-run behavior is well understood. A natural starting point for this is Helmholtz's theorem, which decomposes a vector field into a potential and an incompressible component. However, the geometry of game dynamics - and, in particular, the dynamics of exponential / multiplicative weights (EW) schemes - is not compatible with the Euclidean underpinnings of Helmholtz's theorem. This leads us to consider a specific Riemannian framework based on the so-called Shahshahani metric, and introduce the class of incompressible games, for which we establish the following results: First, in addition to being volume-preserving, the continuous-time EW dynamics in incompressible games admit a constant of motion and are Poincar{\textbackslash}'e recurrent - i.e., almost every trajectory of play comes arbitrarily close to its starting point infinitely often. Second, we establish a deep connection with a well-known decomposition of games into a potential and harmonic component (where the players' objectives are aligned and anti-aligned respectively): a game is incompressible if and only if it is harmonic, implying in turn that the EW dynamics lead to Poincar{\textbackslash}'e recurrence in harmonic games.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Legacci, Davide and Mertikopoulos, Panayotis and Pradelski, Bary},
	month = may,
	year = {2024},
	note = {arXiv:2405.07224 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@misc{lytras_tamed_2024,
	title = {Tamed {Langevin} sampling under weaker conditions},
	url = {http://arxiv.org/abs/2405.17693},
	doi = {10.48550/arXiv.2405.17693},
	abstract = {Motivated by applications to deep learning which often fail standard Lipschitz smoothness requirements, we examine the problem of sampling from distributions that are not log-concave and are only weakly dissipative, with log-gradients allowed to grow superlinearly at infinity. In terms of structure, we only assume that the target distribution satisfies either a log-Sobolev or a Poincar{\textbackslash}'e inequality and a local Lipschitz smoothness assumption with modulus growing possibly polynomially at infinity. This set of assumptions greatly exceeds the operational limits of the "vanilla" unadjusted Langevin algorithm (ULA), making sampling from such distributions a highly involved affair. To account for this, we introduce a taming scheme which is tailored to the growth and decay properties of the target distribution, and we provide explicit non-asymptotic guarantees for the proposed sampler in terms of the Kullback-Leibler (KL) divergence, total variation, and Wasserstein distance to the target distribution.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Lytras, Iosif and Mertikopoulos, Panayotis},
	month = may,
	year = {2024},
	note = {arXiv:2405.17693 [stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Numerical Analysis, Mathematics - Numerical Analysis, Mathematics - Optimization and Control, Mathematics - Probability, Statistics - Machine Learning},
}

@misc{mertikopoulos_nested_2024,
	title = {Nested replicator dynamics, nested logit choice, and similarity-based learning},
	url = {http://arxiv.org/abs/2407.17815},
	doi = {10.48550/arXiv.2407.17815},
	abstract = {We consider a model of learning and evolution in games whose action sets are endowed with a partition-based similarity structure intended to capture exogenous similarities between strategies. In this model, revising agents have a higher probability of comparing their current strategy with other strategies that they deem similar, and they switch to the observed strategy with probability proportional to its payoff excess. Because of this implicit bias toward similar strategies, the resulting dynamics - which we call the nested replicator dynamics - do not satisfy any of the standard monotonicity postulates for imitative game dynamics; nonetheless, we show that they retain the main long-run rationality properties of the replicator dynamics, albeit at quantitatively different rates. We also show that the induced dynamics can be viewed as a stimulus-response model in the spirit of Erev \& Roth (1998), with choice probabilities given by the nested logit choice rule of Ben-Akiva (1973) and McFadden (1978). This result generalizes an existing relation between the replicator dynamics and the exponential weights algorithm in online learning, and provides an additional layer of interpretation to our analysis and results.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Mertikopoulos, Panayotis and Sandholm, William H.},
	month = jul,
	year = {2024},
	note = {arXiv:2407.17815 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning},
}

@article{azizian_rate_2024,
	title = {The rate of convergence of {Bregman} proximal methods: {Local} geometry vs. regularity vs. sharpness},
	volume = {34},
	issn = {1052-6234, 1095-7189},
	shorttitle = {The rate of convergence of {Bregman} proximal methods},
	url = {http://arxiv.org/abs/2211.08043},
	doi = {10.1137/23M1580218},
	abstract = {We examine the last-iterate convergence rate of Bregman proximal methods - from mirror descent to mirror-prox and its optimistic variants - as a function of the local geometry induced by the prox-mapping defining the method. For generality, we focus on local solutions of constrained, non-monotone variational inequalities, and we show that the convergence rate of a given method depends sharply on its associated Legendre exponent, a notion that measures the growth rate of the underlying Bregman function (Euclidean, entropic, or other) near a solution. In particular, we show that boundary solutions exhibit a stark separation of regimes between methods with a zero and non-zero Legendre exponent: the former converge at a linear rate, while the latter converge, in general, sublinearly. This dichotomy becomes even more pronounced in linearly constrained problems where methods with entropic regularization achieve a linear convergence rate along sharp directions, compared to convergence in a finite number of steps under Euclidean regularization.},
	number = {3},
	urldate = {2025-02-20},
	journal = {SIAM Journal on Optimization},
	author = {Azizian, Waïss and Iutzeler, Franck and Malick, Jérôme and Mertikopoulos, Panayotis},
	month = sep,
	year = {2024},
	note = {arXiv:2211.08043 [math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
	pages = {2440--2471},
}

@misc{noauthor_definition_nodate,
	title = {Définition de projet},
	language = {en},
	urldate = {2025-02-19},
}

@article{tuyls_evolutionary_2005,
	title = {Evolutionary game theory and multi-agent reinforcement learning},
	volume = {20},
	copyright = {https://www.cambridge.org/core/terms},
	issn = {0269-8889, 1469-8005},
	url = {https://www.cambridge.org/core/product/identifier/S026988890500041X/type/journal_article},
	doi = {10.1017/S026988890500041X},
	abstract = {In this paper we survey the basics of reinforcement learning and (evolutionary) game theory, applied to the ﬁeld of multi-agent systems. This paper contains three parts. We start with an overview on the fundamentals of reinforcement learning. Next we summarize the most important aspects of evolutionary game theory. Finally, we discuss the state-of-the-art of multi-agent reinforcement learning and the mathematical connection with evolutionary game theory.},
	language = {en},
	number = {1},
	urldate = {2025-02-19},
	journal = {The Knowledge Engineering Review},
	author = {Tuyls, Karl and Nowé, Ann},
	month = mar,
	year = {2005},
	keywords = {Battle of sexes, Evolutionnary game, Game Theory, MARL, Nash equilibrium, Prisoner's Dilema, evolutionary stable strategies},
	pages = {63--90},
}

@book{wiering_reinforcement_2012,
	address = {Berlin, Heidelberg},
	series = {Adaptation, {Learning}, and {Optimization}},
	title = {Reinforcement {Learning}: {State}-of-the-{Art}},
	volume = {12},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-642-27644-6 978-3-642-27645-3},
	shorttitle = {Reinforcement {Learning}},
	url = {https://link.springer.com/10.1007/978-3-642-27645-3},
	language = {en},
	urldate = {2025-02-05},
	publisher = {Springer Berlin Heidelberg},
	editor = {Wiering, Marco and Van Otterlo, Martijn},
	year = {2012},
	doi = {10.1007/978-3-642-27645-3},
	keywords = {Février, Game Theory, Livre, RL},
}

@misc{baker_emergent_2020,
	title = {Emergent {Tool} {Use} {From} {Multi}-{Agent} {Autocurricula}},
	url = {http://arxiv.org/abs/1909.07528},
	doi = {10.48550/arXiv.1909.07528},
	abstract = {Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we ﬁnd that agents create a selfsupervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We ﬁnd clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and ﬁne-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-speciﬁc intelligence tests.},
	language = {en},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Baker, Bowen and Kanitscheider, Ingmar and Markov, Todor and Wu, Yi and Powell, Glenn and McGrew, Bob and Mordatch, Igor},
	month = feb,
	year = {2020},
	note = {arXiv:1909.07528 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Emergent, MARL, Statistics - Machine Learning},
}

@misc{zhang_equilibrium_2024,
	title = {Equilibrium {Selection} for {Multi}-agent {Reinforcement} {Learning}: {A} {Unified} {Framework}},
	shorttitle = {Equilibrium {Selection} for {Multi}-agent {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2406.08844},
	doi = {10.48550/arXiv.2406.08844},
	abstract = {While there are numerous works in multi-agent reinforcement learning (MARL), most of them focus on designing algorithms and proving convergence to a Nash equilibrium (NE) or other equilibrium such as coarse correlated equilibrium. However, NEs can be non-unique and their performance varies drastically. Thus, it is important to design algorithms that converge to Nash equilibrium with better rewards or social welfare. In contrast, classical game theory literature has extensively studied equilibrium selection for multi-agent learning in normal-form games, demonstrating that decentralized learning algorithms can asymptotically converge to potential-maximizing or Pareto-optimal NEs. These insights motivate this paper to investigate equilibrium selection in the MARL setting. We focus on the stochastic game model, leveraging classical equilibrium selection results from normal-form games to propose a unified framework for equilibrium selection in stochastic games. The proposed framework is highly modular and can extend various learning rules and their corresponding equilibrium selection results from normal-form games to the stochastic game setting.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Zhang, Runyu and Shamma, Jeff and Li, Na},
	month = jun,
	year = {2024},
	note = {arXiv:2406.08844 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Equilibrium, MARL, Mathematics - Optimization and Control, Theory},
}

@article{fan_test_1998,
	title = {Test of {Significance} {When} {Data} are {Curves}},
	volume = {93},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.1998.10473763},
	doi = {10.1080/01621459.1998.10473763},
	abstract = {With modern technology, massive data can easily be collected in a form of multiple sets of curves. New statistical challenge includes testing whether there is any statistically significant difference among these sets of curves. In this article we propose some new tests for comparing two groups of curves based on the adaptive Neyman test and the wavelet thresholding techniques introduced earlier by Fan. We demonstrate that these tests inherit the properties outlined by Fan and that they are simple and powerful for detecting differences between two sets of curves. We then further generalize the idea to compare multiple sets of curves, resulting in an adaptive high-dimensional analysis of variance, called HANOVA. These newly developed techniques are illustrated by using a dataset on pizza commercials where observations are curves and an analysis of cornea topography in ophthalmology where images of individuals are observed. A simulation example is also presented to illustrate the power of the adaptive Neyman test.},
	number = {443},
	urldate = {2025-02-16},
	journal = {Journal of the American Statistical Association},
	author = {Fan, Jianqing and Lin, Sheng-Kuei},
	month = sep,
	year = {1998},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1080/01621459.1998.10473763},
	pages = {1007--1021},
}

@misc{yang_overview_2021,
	title = {An {Overview} of {Multi}-{Agent} {Reinforcement} {Learning} from {Game} {Theoretical} {Perspective}},
	url = {http://arxiv.org/abs/2011.00583},
	doi = {10.48550/arXiv.2011.00583},
	abstract = {Following the remarkable success of the AlphaGO series, 2019 was a booming year that witnessed significant advances in multi-agent reinforcement learning (MARL) techniques. MARL corresponds to the learning problem in a multi-agent system in which multiple agents learn simultaneously. It is an interdisciplinary domain with a long history that includes game theory, machine learning, stochastic control, psychology, and optimisation. Although MARL has achieved considerable empirical success in solving real-world games, there is a lack of a self-contained overview in the literature that elaborates the game theoretical foundations of modern MARL methods and summarises the recent advances. In fact, the majority of existing surveys are outdated and do not fully cover the recent developments since 2010. In this work, we provide a monograph on MARL that covers both the fundamentals and the latest developments in the research frontier. The goal of our monograph is to provide a self-contained assessment of the current state-of-the-art MARL techniques from a game theoretical perspective. We expect this work to serve as a stepping stone for both new researchers who are about to enter this fast-growing domain and existing domain experts who want to obtain a panoramic view and identify new directions based on recent advances.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Yang, Yaodong and Wang, Jun},
	month = mar,
	year = {2021},
	note = {arXiv:2011.00583 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems, Feedback loop, Game Theory, MARL, Matrix game, Normal-form game, Review},
}

@article{lupu_leveraging_2019,
	title = {Leveraging {Observations} in {Bandits}: {Between} {Risks} and {Benefits}},
	volume = {33},
	copyright = {https://www.aaai.org},
	issn = {2374-3468, 2159-5399},
	shorttitle = {Leveraging {Observations} in {Bandits}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4568},
	doi = {10.1609/aaai.v33i01.33016112},
	abstract = {Imitation learning has been widely used to speed up learning in novice agents, by allowing them to leverage existing data from experts. Allowing an agent to be inﬂuenced by external observations can beneﬁt to the learning process, but it also puts the agent at risk of following sub-optimal behaviours. In this paper, we study this problem in the context of bandits. More speciﬁcally, we consider that an agent (learner) is interacting with a bandit-style decision task, but can also observe a target policy interacting with the same environment. The learner observes only the target’s actions, not the rewards obtained. We introduce a new bandit optimism modiﬁer that uses conditional optimism contingent on the actions of the target in order to guide the agent’s exploration. We analyze the effect of this modiﬁcation on the well-known Upper Conﬁdence Bound algorithm by proving that it preserves a regret upper-bound of order O(ln T ), even in the presence of a very poor target, and we derive the dependency of the expected regret on the general target policy. We provide empirical results showing both great beneﬁts as well as certain limitations inherent to observational learning in the multi-armed bandit setting. Experiments are conducted using targets satisfying theoretical assumptions with high probability, thus narrowing the gap between theory and application.},
	language = {en},
	number = {01},
	urldate = {2025-01-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Lupu, Andrei and Durand, Audrey and Precup, Doina},
	month = jul,
	year = {2019},
	keywords = {Audrey, Bandit},
	pages = {6112--6119},
}

@article{szolnoki_cyclic_2014,
	title = {Cyclic dominance in evolutionary games: {A} review},
	volume = {11},
	issn = {1742-5689, 1742-5662},
	shorttitle = {Cyclic dominance in evolutionary games},
	url = {http://arxiv.org/abs/1408.6828},
	doi = {10.1098/rsif.2014.0735},
	abstract = {Rock is wrapped by paper, paper is cut by scissors, and scissors are crushed by rock. This simple game is popular among children and adults to decide on trivial disputes that have no obvious winner, but cyclic dominance is also at the heart of predator-prey interactions, the mating strategy of side-blotched lizards, the overgrowth of marine sessile organisms, and the competition in microbial populations. Cyclical interactions also emerge spontaneously in evolutionary games entailing volunteering, reward, punishment, and in fact are common when the competing strategies are three or more regardless of the particularities of the game. Here we review recent advances on the rock-paper-scissors and related evolutionary games, focusing in particular on pattern formation, the impact of mobility, and the spontaneous emergence of cyclic dominance. We also review mean-ﬁeld and zero-dimensional rock-paper-scissors models and the application of the complex Ginzburg-Landau equation, and we highlight the importance and usefulness of statistical physics for the successful study of large-scale ecological systems. Directions for future research, related for example to dynamical effects of coevolutionary rules and invasion reversals due to multi-point interactions, are outlined as well.},
	language = {en},
	number = {100},
	urldate = {2025-01-26},
	journal = {Journal of The Royal Society Interface},
	author = {Szolnoki, Attila and Mobilia, Mauro and Jiang, Luo-Luo and Szczesny, Bartosz and Rucklidge, Alastair M. and Perc, Matjaz},
	month = nov,
	year = {2014},
	note = {arXiv:1408.6828 [physics]},
	keywords = {Closed Orbits, Complex Ginzburg-Landau Equation, Computer Science - Social and Information Networks, Condensed Matter - Statistical Mechanics, Cycle, Game Theory, Heteroclininc Cycles, Hopf Bifurcation, May-Leonard Model, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Physics - Physics and Society, Prisoner's Dilema, Quantitative Biology - Populations and Evolution, RPS, Review},
	pages = {20140735},
}

@book{robinson_topology_2006,
	address = {London New York},
	title = {The topology of the 2x2 games: a new periodic table},
	isbn = {978-0-415-33609-3},
	shorttitle = {The topology of the 2x2 games},
	abstract = {2x2 games provide the very basis of game theory and this book constitutes something approaching a 'periodic table' of the most common games - the prisoner's dilemma, coordination games, chicken and the battle of the sexes among them},
	language = {en},
	publisher = {Routledge},
	author = {Robinson, David and Goforth, David},
	year = {2006},
	keywords = {Février, Game Theory, Important, Matric game, Review},
}

@misc{lin_online_2022,
	title = {Online {Learning} in {Iterated} {Prisoner}'s {Dilemma} to {Mimic} {Human} {Behavior}},
	url = {http://arxiv.org/abs/2006.06580},
	doi = {10.48550/arXiv.2006.06580},
	abstract = {As an important psychological and social experiment, the Iterated Prisoner's Dilemma (IPD) treats the choice to cooperate or defect as an atomic action. We propose to study the behaviors of online learning algorithms in the Iterated Prisoner's Dilemma (IPD) game, where we investigate the full spectrum of reinforcement learning agents: multi-armed bandits, contextual bandits and reinforcement learning. We evaluate them based on a tournament of iterated prisoner's dilemma where multiple agents can compete in a sequential fashion. This allows us to analyze the dynamics of policies learned by multiple self-interested independent reward-driven agents, and also allows us study the capacity of these algorithms to fit the human behaviors. Results suggest that considering the current situation to make decision is the worst in this kind of social dilemma game. Multiples discoveries on online learning behaviors and clinical validations are stated, as an effort to connect artificial intelligence algorithms with human behaviors and their abnormal states in neuropsychiatric conditions.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Lin, Baihan and Bouneffouf, Djallel and Cecchi, Guillermo},
	month = aug,
	year = {2022},
	note = {arXiv:2006.06580 [cs]},
	keywords = {Bandit, Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Computer Science - Multiagent Systems, MARL, Prisoner's Dilema, Q-Learning, Quantitative Biology - Neurons and Cognition, Recommandé Audrey},
}

@misc{lotidis_accelerated_2024,
	title = {Accelerated regularized learning in finite {N}-person games},
	url = {http://arxiv.org/abs/2412.20365},
	doi = {10.48550/arXiv.2412.20365},
	abstract = {Motivated by the success of Nesterov's accelerated gradient algorithm for convex minimization problems, we examine whether it is possible to achieve similar performance gains in the context of online learning in games. To that end, we introduce a family of accelerated learning methods, which we call "follow the accelerated leader" (FTXL), and which incorporates the use of momentum within the general framework of regularized learning - and, in particular, the exponential/multiplicative weights algorithm and its variants. Drawing inspiration and techniques from the continuous-time analysis of Nesterov's algorithm, we show that FTXL converges locally to strict Nash equilibria at a superlinear rate, achieving in this way an exponential speed-up over vanilla regularized learning methods (which, by comparison, converge to strict equilibria at a geometric, linear rate). Importantly, FTXL maintains its superlinear convergence rate in a broad range of feedback structures, from deterministic, full information models to stochastic, realization-based ones, and even when run with bandit, payoff-based information, where players are only able to observe their individual realized payoffs.},
	urldate = {2025-02-12},
	publisher = {arXiv},
	author = {Lotidis, Kyriakos and Giannou, Angeliki and Mertikopoulos, Panayotis and Bambos, Nicholas},
	month = dec,
	year = {2024},
	note = {arXiv:2412.20365 [cs]},
	keywords = {Bandit, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Congestion game, Mathematics - Optimization and Control},
}

@misc{mertikopoulos_unified_2023,
	title = {A unified stochastic approximation framework for learning in games},
	url = {http://arxiv.org/abs/2206.03922},
	doi = {10.48550/arXiv.2206.03922},
	abstract = {We develop a flexible stochastic approximation framework for analyzing the long-run behavior of learning in games (both continuous and finite). The proposed analysis template incorporates a wide array of popular learning algorithms, including gradient-based methods, the exponential / multiplicative weights algorithm for learning in finite games, optimistic and bandit variants of the above, etc. In addition to providing an integrated view of these algorithms, our framework further allows us to obtain several new convergence results, both asymptotic and in finite time, in both continuous and finite games. Specifically, we provide a range of criteria for identifying classes of Nash equilibria and sets of action profiles that are attracting with high probability, and we also introduce the notion of coherence, a game-theoretic property that includes strict and sharp equilibria, and which leads to convergence in finite time. Importantly, our analysis applies to both oracle-based and bandit, payoff-based methods – that is, when players only observe their realized payoffs.},
	language = {en},
	urldate = {2025-02-12},
	publisher = {arXiv},
	author = {Mertikopoulos, Panayotis and Hsieh, Ya-Ping and Cevher, Volkan},
	month = jul,
	year = {2023},
	note = {arXiv:2206.03922 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@article{lefebvre_shallow_nodate,
	title = {On {Shallow} {Planning} {Under} {Partial} {Observability}},
	abstract = {Formulating a real-world problem under the Reinforcement Learning framework involves non-trivial design choices, such as selecting a discount factor for the learning objective (discounted cumulative rewards), which articulates the planning horizon of the agent. This work investigates the impact of the discount factor on the bias-variance trade-off given structural parameters of the underlying Markov Decision Process. Our results support the idea that a shorter planning horizon might be beneficial, especially under partial observability.},
	language = {en},
	author = {Lefebvre, Randy and Durand, Audrey},
}

@inproceedings{heliou_learning_2017,
	title = {Learning with {Bandit} {Feedback} in {Potential} {Games}},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/39ae2ed11b14a4ccb41d35e9d1ba5d11-Abstract.html},
	abstract = {This paper examines the equilibrium convergence properties of no-regret learning with exponential weights in potential games. To establish convergence with minimal information requirements on the players' side, we focus on two frameworks: the semi-bandit case (where players have access to a noisy estimate of their payoff vectors, including strategies they did not play), and the bandit case (where players are only able to observe their in-game, realized payoffs). In the semi-bandit case, we show that the induced sequence of play converges almost surely to a Nash equilibrium at a quasi-exponential rate. In the bandit case, the same result holds for approximate Nash equilibria if we introduce a constant exploration factor that guarantees that action choice probabilities never become arbitrarily small. In particular, if the algorithm is run with a suitably decreasing exploration factor, the sequence of play converges to a bona fide Nash equilibrium with probability 1.},
	urldate = {2025-02-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Heliou, Amélie and Cohen, Johanne and Mertikopoulos, Panayotis},
	year = {2017},
	keywords = {Bandit, EXP3, Epsillon, Feedback loop, Important},
}

@misc{zeng_survey_2023,
	title = {A {Survey} on {Causal} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2302.05209},
	doi = {10.48550/arXiv.2302.05209},
	abstract = {While Reinforcement Learning (RL) achieves tremendous success in sequential decision-making problems of many domains, it still faces key challenges of data inefficiency and the lack of interpretability. Interestingly, many researchers have leveraged insights from the causality literature recently, bringing forth flourishing works to unify the merits of causality and address well the challenges from RL. As such, it is of great necessity and significance to collate these Causal Reinforcement Learning (CRL) works, offer a review of CRL methods, and investigate the potential functionality from causality toward RL. In particular, we divide existing CRL approaches into two categories according to whether their causality-based information is given in advance or not. We further analyze each category in terms of the formalization of different models, ranging from the Markov Decision Process (MDP), Partially Observed Markov Decision Process (POMDP), Multi-Arm Bandits (MAB), and Dynamic Treatment Regime (DTR). Moreover, we summarize the evaluation matrices and open sources while we discuss emerging applications, along with promising prospects for the future development of CRL.},
	urldate = {2025-02-05},
	publisher = {arXiv},
	author = {Zeng, Yan and Cai, Ruichu and Sun, Fuchun and Huang, Libo and Hao, Zhifeng},
	month = jun,
	year = {2023},
	note = {arXiv:2302.05209 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{asmar_efficient_2024,
	title = {Efficient {Multiagent} {Planning} via {Shared} {Action} {Suggestions}},
	url = {http://arxiv.org/abs/2412.11430},
	doi = {10.48550/arXiv.2412.11430},
	abstract = {Decentralized partially observable Markov decision processes with communication (Dec-POMDP-Com) provide a framework for multiagent decision making under uncertainty, but the NEXP-complete complexity renders solutions intractable in general. While sharing actions and observations can reduce the complexity to PSPACE-complete, we propose an approach that bridges POMDPs and Dec-POMDPs by communicating only suggested joint actions, eliminating the need to share observations while maintaining performance comparable to fully centralized planning and execution. Our algorithm estimates joint beliefs using shared actions to prune infeasible beliefs. Each agent maintains possible belief sets for other agents, pruning them based on suggested actions to form an estimated joint belief usable with any centralized policy. This approach requires solving a POMDP for each agent, reducing computational complexity while preserving performance. We demonstrate its effectiveness on several Dec-POMDP benchmarks showing performance comparable to centralized methods when shared actions enable effective belief pruning. This action-based communication framework offers a natural avenue for integrating human-agent cooperation, opening new directions for scalable multiagent planning under uncertainty, with applications in both autonomous systems and human-agent teams.},
	urldate = {2025-02-05},
	publisher = {arXiv},
	author = {Asmar, Dylan M. and Kochenderfer, Mykel J.},
	month = dec,
	year = {2024},
	note = {arXiv:2412.11430 [cs]},
	keywords = {Computer Science - Multiagent Systems},
}

@misc{bernstein_complexity_2013,
	title = {The {Complexity} of {Decentralized} {Control} of {Markov} {Decision} {Processes}},
	url = {http://arxiv.org/abs/1301.3836},
	doi = {10.48550/arXiv.1301.3836},
	abstract = {Planning for distributed agents with partial state information is considered from a decision- theoretic perspective. We describe generalizations of both the MDP and POMDP models that allow for decentralized control. For even a small number of agents, the finite-horizon problems corresponding to both of our models are complete for nondeterministic exponential time. These complexity results illustrate a fundamental difference between centralized and decentralized control of Markov processes. In contrast to the MDP and POMDP problems, the problems we consider provably do not admit polynomial-time algorithms and most likely require doubly exponential time to solve in the worst case. We have thus provided mathematical evidence corresponding to the intuition that decentralized planning problems cannot easily be reduced to centralized problems and solved exactly using established techniques.},
	urldate = {2025-02-05},
	publisher = {arXiv},
	author = {Bernstein, Daniel S. and Zilberstein, Shlomo and Immerman, Neil},
	month = jan,
	year = {2013},
	note = {arXiv:1301.3836 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{liu_distributed_2013,
	title = {Distributed {Output}-{Feedback} {Control} of {Nonlinear} {Multi}-{Agent} {Systems}},
	volume = {58},
	issn = {1558-2523},
	url = {https://ieeexplore.ieee.org/document/6497512/?arnumber=6497512},
	doi = {10.1109/TAC.2013.2257616},
	abstract = {This technical note presents a cyclic-small-gain approach to distributed output-feedback control of nonlinear multi-agent systems. Through novel distributed observer and control law designs, the closed-loop multi-agent system is transformed into a large-scale system composed of input-to-output stable (IOS) subsystems, the IOS gains of which can be appropriately designed. By guaranteeing the IOS of the closed-loop multi-agent system with the recently developed cyclic-small-gain theorem, the outputs of the controlled agents can be driven to within an arbitrarily small neighborhood of the desired agreement value under bounded external disturbances. Moreover, if the system is disturbance-free, then asymptotic convergence can be achieved. Interestingly, the closed-loop distributed system is also robust to bounded time-delays of exchanged information.},
	number = {11},
	urldate = {2025-02-05},
	journal = {IEEE Transactions on Automatic Control},
	author = {Liu, Tengfei and Jiang, Zhong-Ping},
	month = nov,
	year = {2013},
	note = {Conference Name: IEEE Transactions on Automatic Control},
	keywords = {Cyclic-small-gain method, Decentralized control, Multi-agent systems, Nonlinear systems, Observers, Robustness, Topology, distributed control, nonlinear systems, output agreement},
	pages = {2912--2917},
}

@article{seuken_memory-bounded_nodate,
	title = {Memory-{Bounded} {Dynamic} {Programming} for {DEC}-{POMDPs}},
	abstract = {Decentralized decision making under uncertainty has been shown to be intractable when each agent has different partial information about the domain. Thus, improving the applicability and scalability of planning algorithms is an important challenge. We present the ﬁrst memory-bounded dynamic programming algorithm for ﬁnite-horizon decentralized POMDPs. A set of heuristics is used to identify relevant points of the inﬁnitely large belief space. Using these belief points, the algorithm successively selects the best joint policies for each horizon. The algorithm is extremely efﬁcient, having linear time and space complexity with respect to the horizon length. Experimental results show that it can handle horizons that are multiple orders of magnitude larger than what was previously possible, while achieving the same or better solution quality. These results signiﬁcantly increase the applicability of decentralized decision-making techniques.},
	language = {en},
	author = {Seuken, Sven},
}

@article{canese_multi-agent_2021,
	title = {Multi-{Agent} {Reinforcement} {Learning}: {A} {Review} of {Challenges} and {Applications}},
	volume = {11},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2076-3417},
	shorttitle = {Multi-{Agent} {Reinforcement} {Learning}},
	url = {https://www.mdpi.com/2076-3417/11/11/4948},
	doi = {10.3390/app11114948},
	abstract = {In this review, we present an analysis of the most used multi-agent reinforcement learning algorithms. Starting with the single-agent reinforcement learning algorithms, we focus on the most critical issues that must be taken into account in their extension to multi-agent scenarios. The analyzed algorithms were grouped according to their features. We present a detailed taxonomy of the main multi-agent approaches proposed in the literature, focusing on their related mathematical models. For each algorithm, we describe the possible application ﬁelds, while pointing out its pros and cons. The described multi-agent algorithms are compared in terms of the most important characteristics for multi-agent reinforcement learning applications—namely, nonstationarity, scalability, and observability. We also describe the most common benchmark environments used to evaluate the performances of the considered methods.},
	language = {en},
	number = {11},
	urldate = {2025-02-05},
	journal = {Applied Sciences},
	author = {Canese, Lorenzo and Cardarilli, Gian Carlo and Di Nunzio, Luca and Fazzolari, Rocco and Giardino, Daniele and Re, Marco and Spanò, Sergio},
	month = may,
	year = {2021},
	pages = {4948},
}

@inproceedings{chapelle_empirical_2011,
	title = {An {Empirical} {Evaluation} of {Thompson} {Sampling}},
	volume = {24},
	url = {https://papers.nips.cc/paper_files/paper/2011/hash/e53a0a2978c28872a4505bdb51db06dc-Abstract.html},
	abstract = {Thompson sampling is one of oldest heuristic to address the exploration / exploitation trade-off, but it is surprisingly not very popular in the literature. We present here some empirical results using Thompson sampling on simulated and real data, and show that it is highly competitive. And since this heuristic is very easy to implement, we argue that it should be part of the standard baselines to compare against.},
	urldate = {2025-02-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chapelle, Olivier and Li, Lihong},
	year = {2011},
}

@book{sutton_reinforcement_2018,
	address = {Cambridge, Massachusetts},
	edition = {Second edition},
	series = {Adaptive computation and machine learning series},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	language = {en},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {2018},
	keywords = {Reinforcement learning},
}

@article{noauthor_3-exploration_exploitation_nodate,
	title = {3-exploration\_exploitation},
	language = {fr},
}

@misc{marzo_large_2024,
	title = {Large {Language} {Model} agents can coordinate beyond human scale},
	url = {http://arxiv.org/abs/2409.02822},
	doi = {10.48550/arXiv.2409.02822},
	abstract = {Large Language Models (LLMs) are increasingly deployed as interacting agents, forming ``LLM societies''. Understanding whether these societies can self-organize and coordinate on norms without external influence is crucial to understand their risks and opportunities. Here we explore their opinion dynamics finding that it is governed by a majority force coefficient such that LLM societies can spontaneously reach consensus only up to a critical group size. This critical size grows exponentially with the language understanding capabilities of the models, exceeding the typical size of informal human groups for advanced LLMs. These results reveal emerging self-organization properties in LLM societies and provide insights for designing collaborative AI systems where coordination is either a goal or a risk.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Marzo, Giordano De and Castellano, Claudio and Garcia, David},
	month = dec,
	year = {2024},
	note = {arXiv:2409.02822 [physics]},
	keywords = {Feedback loop, LLM, Multi-agent, Negotiation, Physics - Physics and Society},
}

@misc{yongacoglu_paths_2024,
	title = {Paths to {Equilibrium} in {Games}},
	url = {http://arxiv.org/abs/2403.18079},
	doi = {10.48550/arXiv.2403.18079},
	abstract = {In multi-agent reinforcement learning (MARL) and game theory, agents repeatedly interact and revise their strategies as new data arrives, producing a sequence of strategy profiles. This paper studies sequences of strategies satisfying a pairwise constraint inspired by policy updating in reinforcement learning, where an agent who is best responding in one period does not switch its strategy in the next period. This constraint merely requires that optimizing agents do not switch strategies, but does not constrain the non-optimizing agents in any way, and thus allows for exploration. Sequences with this property are called satisficing paths, and arise naturally in many MARL algorithms. A fundamental question about strategic dynamics is such: for a given game and initial strategy profile, is it always possible to construct a satisficing path that terminates at an equilibrium? The resolution of this question has implications about the capabilities or limitations of a class of MARL algorithms. We answer this question in the affirmative for normal-form games. Our analysis reveals a counterintuitive insight that reward deteriorating strategic updates are key to driving play to equilibrium along a satisficing path.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Yongacoglu, Bora and Arslan, Gürdal and Pavel, Lacra and Yüksel, Serdar},
	month = oct,
	year = {2024},
	note = {arXiv:2403.18079 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Equilibrium, MARL, Theory},
}

@article{dekel_1_nodate,
	title = {1 {Recap}: {Diﬀerence} between {Experts} and {Bandits}},
	language = {en},
	author = {Dekel, Ofer and Mandel, Travis},
	keywords = {EXP3, O{\textasciicircum}2/3},
}

@inproceedings{tainaka_physics_2001,
	address = {Berlin, Heidelberg},
	title = {Physics and {Ecology} of {Rock}-{Paper}-{Scissors} {Game}},
	isbn = {978-3-540-45579-0},
	doi = {10.1007/3-540-45579-5_25},
	abstract = {From physical and ecological aspects, we reviewan interacting particle system which follows a rule of the Rock-Paper-Scissors (RPS) game. This rule symbolically represents a food chain in ecosystems. It also represents nonequilibrium systems which have a feedback mechanism.We describe the spatial pattern dynamics in lattice RPS system: the time dependence of each species is not fully understood, especially on two-dimensional lattice. Moreover, we modify and apply RPS rule to voter and biological systems. Computer simulation for both voter model and ecosystems exhibits counter-intuitive results in phase transition. Such results can be seen in many cyclic systems, and they may be related to the unpredictability in nonequilibrium systems.},
	language = {en},
	booktitle = {Computers and {Games}},
	publisher = {Springer},
	author = {Tainaka, Kei-ichi},
	editor = {Marsland, Tony and Frank, Ian},
	year = {2001},
	keywords = {Dimensional Lattice, Feedback loop, Interact Particle System, Press Perturbation, Prey Density, RPS, Voter Model},
	pages = {384--395},
}

@article{semmann_volunteering_2003,
	title = {Volunteering leads to rock–paper–scissors dynamics in a public goods game},
	volume = {425},
	copyright = {2003 Macmillan Magazines Ltd.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature01986},
	doi = {10.1038/nature01986},
	abstract = {Collective efforts are a trademark of both insect and human societies1. They are achieved through relatedness in the former2 and unknown mechanisms in the latter. The problem of achieving cooperation among non-kin has been described as the ‘tragedy of the commons’, prophesying the inescapable collapse of many human enterprises3,4. In public goods experiments, initial cooperation usually drops quickly to almost zero5. It can be maintained by the opportunity to punish defectors6 or the need to maintain good reputation7. Both schemes require that defectors are identified. Theorists propose that a simple but effective mechanism operates under full anonymity. With optional participation in the public goods game, ‘loners’ (players who do not join the group), defectors and cooperators will coexist through rock–paper–scissors dynamics8,9. Here we show experimentally that volunteering generates these dynamics in public goods games and that manipulating initial conditions can produce each predicted direction. If, by manipulating displayed decisions, it is pretended that defectors have the highest frequency, loners soon become most frequent, as do cooperators after loners and defectors after cooperators. On average, cooperation is perpetuated at a substantial level.},
	language = {en},
	number = {6956},
	urldate = {2025-01-27},
	journal = {Nature},
	author = {Semmann, Dirk and Krambeck, Hans-Jürgen and Milinski, Manfred},
	month = sep,
	year = {2003},
	note = {Publisher: Nature Publishing Group},
	keywords = {Application, Cycle, Humanities and Social Sciences, Public Good, RPS, Science, multidisciplinary},
	pages = {390--393},
}

@misc{fujimoto_learning_2023,
	title = {Learning in {Multi}-{Memory} {Games} {Triggers} {Complex} {Dynamics} {Diverging} from {Nash} {Equilibrium}},
	url = {http://arxiv.org/abs/2302.01073},
	doi = {10.48550/arXiv.2302.01073},
	abstract = {Repeated games consider a situation where multiple agents are motivated by their independent rewards throughout learning. In general, the dynamics of their learning become complex. Especially when their rewards compete with each other like zero-sum games, the dynamics often do not converge to their optimum, i.e., the Nash equilibrium. To tackle such complexity, many studies have understood various learning algorithms as dynamical systems and discovered qualitative insights among the algorithms. However, such studies have yet to handle multi-memory games (where agents can memorize actions they played in the past and choose their actions based on their memories), even though memorization plays a pivotal role in artificial intelligence and interpersonal relationship. This study extends two major learning algorithms in games, i.e., replicator dynamics and gradient ascent, into multi-memory games. Then, we prove their dynamics are identical. Furthermore, theoretically and experimentally, we clarify that the learning dynamics diverge from the Nash equilibrium in multi-memory zero-sum games and reach heteroclinic cycles (sojourn longer around the boundary of the strategy space), providing a fundamental advance in learning in games.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Fujimoto, Yuma and Ariu, Kaito and Abe, Kenshi},
	month = may,
	year = {2023},
	note = {arXiv:2302.01073 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Multiagent Systems, Mathematics - Optimization and Control, Nonlinear Sciences - Chaotic Dynamics},
}

@misc{fujimoto_memory_2024,
	title = {Memory {Asymmetry} {Creates} {Heteroclinic} {Orbits} to {Nash} {Equilibrium} in {Learning} in {Zero}-{Sum} {Games}},
	url = {http://arxiv.org/abs/2305.13619},
	doi = {10.48550/arXiv.2305.13619},
	abstract = {Learning in games considers how multiple agents maximize their own rewards through repeated games. Memory, an ability that an agent changes his/her action depending on the history of actions in previous games, is often introduced into learning to explore more clever strategies and discuss the decision-making of real agents like humans. However, such games with memory are hard to analyze because they exhibit complex phenomena like chaotic dynamics or divergence from Nash equilibrium. In particular, how asymmetry in memory capacities between agents affects learning in games is still unclear. In response, this study formulates a gradient ascent algorithm in games with asymmetry memory capacities. To obtain theoretical insights into learning dynamics, we first consider a simple case of zero-sum games. We observe complex behavior, where learning dynamics draw a heteroclinic connection from unstable fixed points to stable ones. Despite this complexity, we analyze learning dynamics and prove local convergence to these stable fixed points, i.e., the Nash equilibria. We identify the mechanism driving this convergence: an agent with a longer memory learns to exploit the other, which in turn endows the other's utility function with strict concavity. We further numerically observe such convergence in various initial strategies, action numbers, and memory lengths. This study reveals a novel phenomenon due to memory asymmetry, providing fundamental strides in learning in games and new insights into computing equilibria.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Fujimoto, Yuma and Ariu, Kaito and Abe, Kenshi},
	month = feb,
	year = {2024},
	note = {arXiv:2305.13619 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Multiagent Systems, Mathematics - Optimization and Control, Nonlinear Sciences - Chaotic Dynamics},
}

@misc{noauthor_literature_nodate,
	title = {The {Literature} {Review}: {A} {Few} {Tips} {On} {Conducting} {It} {\textbar} {Writing} {Advice}},
	shorttitle = {The {Literature} {Review}},
	url = {https://advice.writing.utoronto.ca/types-of-writing/literature-review/},
	language = {en-US},
	urldate = {2025-01-27},
}
