<?xml version='1.0' encoding='utf-8'?>
<map version="0.9.0"><node TEXT="Bibliographie"><node TEXT="Année: 1998"><node TEXT="Auteur: Cassandra, Anthony R."><node TEXT="Planning and acting in partially observable stochastic domains" /></node><node TEXT="Auteur: Fan, Jianqing"><node TEXT="Test of {Significance} {When} {Data} are {Curves}" /></node><node TEXT="Auteur: Kaelbling, Leslie Pack"><node TEXT="Planning and acting in partially observable stochastic domains" /></node><node TEXT="Auteur: Lin, Sheng-Kuei"><node TEXT="Test of {Significance} {When} {Data} are {Curves}" /></node><node TEXT="Auteur: Littman, Michael L."><node TEXT="Planning and acting in partially observable stochastic domains" /></node></node><node TEXT="Année: 2001"><node TEXT="Auteur: Tainaka, Kei-ichi"><node TEXT="Physics and {Ecology} of {Rock}-{Paper}-{Scissors} {Game}" /></node></node><node TEXT="Année: 2003"><node TEXT="Auteur: Krambeck, Hans-Jürgen"><node TEXT="Volunteering leads to rock–paper–scissors dynamics in a public goods game" /></node><node TEXT="Auteur: Milinski, Manfred"><node TEXT="Volunteering leads to rock–paper–scissors dynamics in a public goods game" /></node><node TEXT="Auteur: Semmann, Dirk"><node TEXT="Volunteering leads to rock–paper–scissors dynamics in a public goods game" /></node></node><node TEXT="Année: 2005"><node TEXT="Auteur: Nowé, Ann"><node TEXT="Evolutionary game theory and multi-agent reinforcement learning" /></node><node TEXT="Auteur: Tuyls, Karl"><node TEXT="Evolutionary game theory and multi-agent reinforcement learning" /></node></node><node TEXT="Année: 2006"><node TEXT="Auteur: Goforth, David"><node TEXT="The topology of the 2x2 games: a new periodic table" /></node><node TEXT="Auteur: Robinson, David"><node TEXT="The topology of the 2x2 games: a new periodic table" /></node></node><node TEXT="Année: 2007"><node TEXT="Auteur: Fath, Gabor"><node TEXT="Evolutionary games on graphs" /></node><node TEXT="Auteur: Szabo, Gyorgy"><node TEXT="Evolutionary games on graphs" /></node></node><node TEXT="Année: 2011"><node TEXT="Auteur: Chapelle, Olivier"><node TEXT="An {Empirical} {Evaluation} of {Thompson} {Sampling}" /></node><node TEXT="Auteur: Li, Lihong"><node TEXT="An {Empirical} {Evaluation} of {Thompson} {Sampling}" /></node></node><node TEXT="Année: 2012"><node TEXT="Auteur: Akrour, Riad"><node TEXT="{APRIL}: {Active} {Preference}-learning based {Reinforcement} {Learning}" /></node><node TEXT="Auteur: AuteurInconnu"><node TEXT="Reinforcement {Learning}: {State}-of-the-{Art}" /></node><node TEXT="Auteur: Dyson, Freeman J."><node TEXT="Iterated {Prisoner}’s {Dilemma} contains strategies that dominate any evolutionary opponent" /></node><node TEXT="Auteur: Laurent, Guillaume J."><node TEXT="Independent reinforcement learners in cooperative {Markov} games: a survey regarding coordination problems" /></node><node TEXT="Auteur: Le Fort-Piat, Nadine"><node TEXT="Independent reinforcement learners in cooperative {Markov} games: a survey regarding coordination problems" /></node><node TEXT="Auteur: Matignon, Laetitia"><node TEXT="Independent reinforcement learners in cooperative {Markov} games: a survey regarding coordination problems" /></node><node TEXT="Auteur: Press, William H."><node TEXT="Iterated {Prisoner}’s {Dilemma} contains strategies that dominate any evolutionary opponent" /></node><node TEXT="Auteur: Schoenauer, Marc"><node TEXT="{APRIL}: {Active} {Preference}-learning based {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Sebag, Michèle"><node TEXT="{APRIL}: {Active} {Preference}-learning based {Reinforcement} {Learning}" /></node></node><node TEXT="Année: 2013"><node TEXT="Auteur: Bernstein, Daniel S."><node TEXT="The {Complexity} of {Decentralized} {Control} of {Markov} {Decision} {Processes}" /></node><node TEXT="Auteur: Immerman, Neil"><node TEXT="The {Complexity} of {Decentralized} {Control} of {Markov} {Decision} {Processes}" /></node><node TEXT="Auteur: Jiang, Zhong-Ping"><node TEXT="Distributed {Output}-{Feedback} {Control} of {Nonlinear} {Multi}-{Agent} {Systems}" /></node><node TEXT="Auteur: Liu, Tengfei"><node TEXT="Distributed {Output}-{Feedback} {Control} of {Nonlinear} {Multi}-{Agent} {Systems}" /></node><node TEXT="Auteur: Zilberstein, Shlomo"><node TEXT="The {Complexity} of {Decentralized} {Control} of {Markov} {Decision} {Processes}" /></node></node><node TEXT="Année: 2014"><node TEXT="Auteur: Jiang, Luo-Luo"><node TEXT="Cyclic dominance in evolutionary games: {A} review" /></node><node TEXT="Auteur: Mobilia, Mauro"><node TEXT="Cyclic dominance in evolutionary games: {A} review" /></node><node TEXT="Auteur: Perc, Matjaz"><node TEXT="Cyclic dominance in evolutionary games: {A} review" /></node><node TEXT="Auteur: Rucklidge, Alastair M."><node TEXT="Cyclic dominance in evolutionary games: {A} review" /></node><node TEXT="Auteur: Szczesny, Bartosz"><node TEXT="Cyclic dominance in evolutionary games: {A} review" /></node><node TEXT="Auteur: Szolnoki, Attila"><node TEXT="Cyclic dominance in evolutionary games: {A} review" /></node></node><node TEXT="Année: 2015"><node TEXT="Auteur: Bloembergen, Daan"><node TEXT="Evolutionary {Dynamics} of {Multi}-{Agent} {Learning}: {A} {Survey}" /></node><node TEXT="Auteur: Hennes, Daniel"><node TEXT="Evolutionary {Dynamics} of {Multi}-{Agent} {Learning}: {A} {Survey}" /></node><node TEXT="Auteur: Kaisers, Michael"><node TEXT="Evolutionary {Dynamics} of {Multi}-{Agent} {Learning}: {A} {Survey}" /></node><node TEXT="Auteur: Tuyls, Karl"><node TEXT="Evolutionary {Dynamics} of {Multi}-{Agent} {Learning}: {A} {Survey}" /></node></node><node TEXT="Année: 2016"><node TEXT="Auteur: Assael, Yannis M."><node TEXT="Learning to {Communicate} with {Deep} {Multi}-{Agent} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Foerster, Jakob N."><node TEXT="Learning to {Communicate} with {Deep} {Multi}-{Agent} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Freitas, Nando de"><node TEXT="Learning to {Communicate} with {Deep} {Multi}-{Agent} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Whiteson, Shimon"><node TEXT="Learning to {Communicate} with {Deep} {Multi}-{Agent} {Reinforcement} {Learning}" /></node></node><node TEXT="Année: 2017"><node TEXT="Auteur: Bachman, Philip"><node TEXT="Learning {Algorithms} for {Active} {Learning}" /></node><node TEXT="Auteur: Batra, Dhruv"><node TEXT="Deal or {No} {Deal}? {End}-to-{End} {Learning} of {Negotiation} {Dialogues}" /></node><node TEXT="Auteur: Chakraborty, Mithun"><node TEXT="Coordinated {Versus} {Decentralized} {Exploration} {In} {Multi}-{Agent} {Multi}-{Armed} {Bandits}" /></node><node TEXT="Auteur: Chua, Kai Yee Phoebe"><node TEXT="Coordinated {Versus} {Decentralized} {Exploration} {In} {Multi}-{Agent} {Multi}-{Armed} {Bandits}" /></node><node TEXT="Auteur: Cohen, Johanne"><node TEXT="Learning with {Bandit} {Feedback} in {Potential} {Games}" /></node><node TEXT="Auteur: Das, Sanmay"><node TEXT="Coordinated {Versus} {Decentralized} {Exploration} {In} {Multi}-{Agent} {Multi}-{Armed} {Bandits}" /></node><node TEXT="Auteur: Dauphin, Yann"><node TEXT="Deal or {No} {Deal}? {End}-to-{End} {Learning} of {Negotiation} {Dialogues}" /></node><node TEXT="Auteur: Doshi-Velez, Finale"><node TEXT="Right for the {Right} {Reasons}: {Training} {Differentiable} {Models} by {Constraining} their {Explanations}" /></node><node TEXT="Auteur: Heliou, Amélie"><node TEXT="Learning with {Bandit} {Feedback} in {Potential} {Games}" /></node><node TEXT="Auteur: Hughes, Michael C."><node TEXT="Right for the {Right} {Reasons}: {Training} {Differentiable} {Models} by {Constraining} their {Explanations}" /></node><node TEXT="Auteur: Juba, Brendan"><node TEXT="Coordinated {Versus} {Decentralized} {Exploration} {In} {Multi}-{Agent} {Multi}-{Armed} {Bandits}" /></node><node TEXT="Auteur: Lewis, Mike"><node TEXT="Deal or {No} {Deal}? {End}-to-{End} {Learning} of {Negotiation} {Dialogues}" /></node><node TEXT="Auteur: Mertikopoulos, Panayotis"><node TEXT="Learning with {Bandit} {Feedback} in {Potential} {Games}" /></node><node TEXT="Auteur: Parikh, Devi"><node TEXT="Deal or {No} {Deal}? {End}-to-{End} {Learning} of {Negotiation} {Dialogues}" /></node><node TEXT="Auteur: Ross, Andrew Slavin"><node TEXT="Right for the {Right} {Reasons}: {Training} {Differentiable} {Models} by {Constraining} their {Explanations}" /></node><node TEXT="Auteur: Sordoni, Alessandro"><node TEXT="Learning {Algorithms} for {Active} {Learning}" /></node><node TEXT="Auteur: Trischler, Adam"><node TEXT="Learning {Algorithms} for {Active} {Learning}" /></node><node TEXT="Auteur: Yarats, Denis"><node TEXT="Deal or {No} {Deal}? {End}-to-{End} {Learning} of {Negotiation} {Dialogues}" /></node></node><node TEXT="Année: 2018"><node TEXT="Auteur: Barto, Andrew G."><node TEXT="Reinforcement learning: an introduction" /></node><node TEXT="Auteur: Bilodeau, Anthony"><node TEXT="A machine learning approach for online automated optimization of super-resolution optical microscopy" /></node><node TEXT="Auteur: Bistritz, Ilai"><node TEXT="Distributed {Multi}-{Player} {Bandits} - a {Game} of {Thrones} {Approach}" /></node><node TEXT="Auteur: De Koninck, Paul"><node TEXT="A machine learning approach for online automated optimization of super-resolution optical microscopy" /></node><node TEXT="Auteur: Durand, Audrey"><node TEXT="A machine learning approach for online automated optimization of super-resolution optical microscopy" /></node><node TEXT="Auteur: Gagné, Christian"><node TEXT="A machine learning approach for online automated optimization of super-resolution optical microscopy" /></node><node TEXT="Auteur: Gardner, Marc-André"><node TEXT="A machine learning approach for online automated optimization of super-resolution optical microscopy" /></node><node TEXT="Auteur: Lavoie-Cardinal, Flavie"><node TEXT="A machine learning approach for online automated optimization of super-resolution optical microscopy" /></node><node TEXT="Auteur: Leshem, Amir"><node TEXT="Distributed {Multi}-{Player} {Bandits} - a {Game} of {Thrones} {Approach}" /></node><node TEXT="Auteur: Robitaille, Louis-Émile"><node TEXT="A machine learning approach for online automated optimization of super-resolution optical microscopy" /></node><node TEXT="Auteur: Sutton, Richard S."><node TEXT="Reinforcement learning: an introduction" /></node><node TEXT="Auteur: Wiesner, Theresa"><node TEXT="A machine learning approach for online automated optimization of super-resolution optical microscopy" /></node></node><node TEXT="Année: 2019"><node TEXT="Auteur: Baarslag, Tim"><node TEXT="A {Survey} of {Learning} in {Multiagent} {Environments}: {Dealing} with {Non}-{Stationarity}" /></node><node TEXT="Auteur: Boursier, Etienne"><node TEXT="{SIC}-{MMAB}: {Synchronisation} {Involves} {Communication} in {Multiplayer} {Multi}-{Armed} {Bandits}" /></node><node TEXT="Auteur: Cote, Enrique Munoz de"><node TEXT="A {Survey} of {Learning} in {Multiagent} {Environments}: {Dealing} with {Non}-{Stationarity}" /></node><node TEXT="Auteur: Durand, Audrey"><node TEXT="Leveraging {Observations} in {Bandits}: {Between} {Risks} and {Benefits}" /></node><node TEXT="Auteur: Graepel, Thore"><node TEXT="Autocurricula and the {Emergence} of {Innovation} from {Social} {Interaction}: {A} {Manifesto} for {Multi}-{Agent} {Intelligence} {Research}" /></node><node TEXT="Auteur: Hernandez-Leal, Pablo"><node TEXT="A {Survey} and {Critique} of {Multiagent} {Deep} {Reinforcement} {Learning}" /><node TEXT="A {Survey} of {Learning} in {Multiagent} {Environments}: {Dealing} with {Non}-{Stationarity}" /></node><node TEXT="Auteur: Hughes, Edward"><node TEXT="Autocurricula and the {Emergence} of {Innovation} from {Social} {Interaction}: {A} {Manifesto} for {Multi}-{Agent} {Intelligence} {Research}" /></node><node TEXT="Auteur: Kaisers, Michael"><node TEXT="A {Survey} of {Learning} in {Multiagent} {Environments}: {Dealing} with {Non}-{Stationarity}" /></node><node TEXT="Auteur: Kartal, Bilal"><node TEXT="A {Survey} and {Critique} of {Multiagent} {Deep} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Lanctot, Marc"><node TEXT="Autocurricula and the {Emergence} of {Innovation} from {Social} {Interaction}: {A} {Manifesto} for {Multi}-{Agent} {Intelligence} {Research}" /></node><node TEXT="Auteur: Leibo, Joel Z."><node TEXT="Autocurricula and the {Emergence} of {Innovation} from {Social} {Interaction}: {A} {Manifesto} for {Multi}-{Agent} {Intelligence} {Research}" /></node><node TEXT="Auteur: Lupu, Andrei"><node TEXT="Leveraging {Observations} in {Bandits}: {Between} {Risks} and {Benefits}" /></node><node TEXT="Auteur: Perchet, Vianney"><node TEXT="{SIC}-{MMAB}: {Synchronisation} {Involves} {Communication} in {Multiplayer} {Multi}-{Armed} {Bandits}" /></node><node TEXT="Auteur: Precup, Doina"><node TEXT="Leveraging {Observations} in {Bandits}: {Between} {Risks} and {Benefits}" /></node><node TEXT="Auteur: Sirakov, Boyan"><node TEXT="Proceedings {Of} {The} {International} {Congress} {Of} {Mathematicians} 2018 ({Icm} 2018) ({In} 4 {Volumes})" /></node><node TEXT="Auteur: Souza, Paulo Ney De"><node TEXT="Proceedings {Of} {The} {International} {Congress} {Of} {Mathematicians} 2018 ({Icm} 2018) ({In} 4 {Volumes})" /></node><node TEXT="Auteur: Taylor, Matthew E."><node TEXT="A {Survey} and {Critique} of {Multiagent} {Deep} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Viana, Marcelo"><node TEXT="Proceedings {Of} {The} {International} {Congress} {Of} {Mathematicians} 2018 ({Icm} 2018) ({In} 4 {Volumes})" /></node></node><node TEXT="Année: 2020"><node TEXT="Auteur: Abdollahpouri, Himan"><node TEXT="Feedback {Loop} and {Bias} {Amplification} in {Recommender} {Systems}" /></node><node TEXT="Auteur: Baharav, Tavor Z."><node TEXT="My {Fair} {Bandit}: {Distributed} {Learning} of {Max}-{Min} {Fairness} with {Multi}-player {Bandits}" /></node><node TEXT="Auteur: Baker, Bowen"><node TEXT="Emergent {Tool} {Use} {From} {Multi}-{Agent} {Autocurricula}" /></node><node TEXT="Auteur: Bambos, Nicholas"><node TEXT="Cooperative {Multi}-player {Bandit} {Optimization}" /><node TEXT="My {Fair} {Bandit}: {Distributed} {Learning} of {Max}-{Min} {Fairness} with {Multi}-player {Bandits}" /></node><node TEXT="Auteur: Bistritz, Ilai"><node TEXT="Cooperative {Multi}-player {Bandit} {Optimization}" /><node TEXT="My {Fair} {Bandit}: {Distributed} {Learning} of {Max}-{Min} {Fairness} with {Multi}-player {Bandits}" /></node><node TEXT="Auteur: Bogunovic, Ilija"><node TEXT="Contextual {Games}: {Multi}-{Agent} {Learning} with {Side} {Information}" /></node><node TEXT="Auteur: Burdick, Joel W."><node TEXT="Dueling {Posterior} {Sampling} for {Preference}-{Based} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Burke, Robin"><node TEXT="Feedback {Loop} and {Bias} {Amplification} in {Recommender} {Systems}" /></node><node TEXT="Auteur: Chouldechova, Alexandra"><node TEXT="Counterfactual {Risk} {Assessments}, {Evaluation}, and {Fairness}" /></node><node TEXT="Auteur: Coston, Amanda"><node TEXT="Counterfactual {Risk} {Assessments}, {Evaluation}, and {Fairness}" /></node><node TEXT="Auteur: Kamgarpour, Maryam"><node TEXT="Contextual {Games}: {Multi}-{Agent} {Learning} with {Side} {Information}" /></node><node TEXT="Auteur: Kanitscheider, Ingmar"><node TEXT="Emergent {Tool} {Use} {From} {Multi}-{Agent} {Autocurricula}" /></node><node TEXT="Auteur: Kennedy, Edward H."><node TEXT="Counterfactual {Risk} {Assessments}, {Evaluation}, and {Fairness}" /></node><node TEXT="Auteur: Krause, Andreas"><node TEXT="Contextual {Games}: {Multi}-{Agent} {Learning} with {Side} {Information}" /></node><node TEXT="Auteur: Lattimore, Tor"><node TEXT="Bandit {Algorithms}" /></node><node TEXT="Auteur: Leshem, Amir"><node TEXT="My {Fair} {Bandit}: {Distributed} {Learning} of {Max}-{Min} {Fairness} with {Multi}-player {Bandits}" /></node><node TEXT="Auteur: Mansoury, Masoud"><node TEXT="Feedback {Loop} and {Bias} {Amplification} in {Recommender} {Systems}" /></node><node TEXT="Auteur: Markov, Todor"><node TEXT="Emergent {Tool} {Use} {From} {Multi}-{Agent} {Autocurricula}" /></node><node TEXT="Auteur: McGrew, Bob"><node TEXT="Emergent {Tool} {Use} {From} {Multi}-{Agent} {Autocurricula}" /></node><node TEXT="Auteur: Mishler, Alan"><node TEXT="Counterfactual {Risk} {Assessments}, {Evaluation}, and {Fairness}" /></node><node TEXT="Auteur: Mobasher, Bamshad"><node TEXT="Feedback {Loop} and {Bias} {Amplification} in {Recommender} {Systems}" /></node><node TEXT="Auteur: Mordatch, Igor"><node TEXT="Emergent {Tool} {Use} {From} {Multi}-{Agent} {Autocurricula}" /></node><node TEXT="Auteur: Novoseller, Ellen R."><node TEXT="Dueling {Posterior} {Sampling} for {Preference}-{Based} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Pechenizkiy, Mykola"><node TEXT="Feedback {Loop} and {Bias} {Amplification} in {Recommender} {Systems}" /></node><node TEXT="Auteur: Powell, Glenn"><node TEXT="Emergent {Tool} {Use} {From} {Multi}-{Agent} {Autocurricula}" /></node><node TEXT="Auteur: Sessa, Pier Giuseppe"><node TEXT="Contextual {Games}: {Multi}-{Agent} {Learning} with {Side} {Information}" /></node><node TEXT="Auteur: Sui, Yanan"><node TEXT="Dueling {Posterior} {Sampling} for {Preference}-{Based} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Szepesvári, Csaba"><node TEXT="Bandit {Algorithms}" /></node><node TEXT="Auteur: Wei, Yibing"><node TEXT="Dueling {Posterior} {Sampling} for {Preference}-{Based} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Wu, Yi"><node TEXT="Emergent {Tool} {Use} {From} {Multi}-{Agent} {Autocurricula}" /></node><node TEXT="Auteur: Yue, Yisong"><node TEXT="Dueling {Posterior} {Sampling} for {Preference}-{Based} {Reinforcement} {Learning}" /></node></node><node TEXT="Année: 2021"><node TEXT="Auteur: Alouini, Mohamed-Slim"><node TEXT="Improving {Spectral} {Efficiency} of {Wireless} {Networks} through {Democratic} {Spectrum} {Sharing}" /></node><node TEXT="Auteur: Baharav, Tavor Z."><node TEXT="One for {All} and {All} for {One}: {Distributed} {Learning} of {Fair} {Allocations} {With} {Multi}-{Player} {Bandits}" /></node><node TEXT="Auteur: Bambos, Nicholas"><node TEXT="One for {All} and {All} for {One}: {Distributed} {Learning} of {Fair} {Allocations} {With} {Multi}-{Player} {Bandits}" /></node><node TEXT="Auteur: Bistritz, Ilai"><node TEXT="One for {All} and {All} for {One}: {Distributed} {Learning} of {Fair} {Allocations} {With} {Multi}-{Player} {Bandits}" /></node><node TEXT="Auteur: Brown, William"><node TEXT="Learning in {Multi}-{Player} {Stochastic} {Games}" /></node><node TEXT="Auteur: Canese, Lorenzo"><node TEXT="Multi-{Agent} {Reinforcement} {Learning}: {A} {Review} of {Challenges} and {Applications}" /></node><node TEXT="Auteur: Cardarilli, Gian Carlo"><node TEXT="Multi-{Agent} {Reinforcement} {Learning}: {A} {Review} of {Challenges} and {Applications}" /></node><node TEXT="Auteur: Chen, Songcan"><node TEXT="Improving {Model} {Robustness} by {Adaptively} {Correcting} {Perturbation} {Levels} with {Active} {Queries}" /></node><node TEXT="Auteur: Di Nunzio, Luca"><node TEXT="Multi-{Agent} {Reinforcement} {Learning}: {A} {Review} of {Challenges} and {Applications}" /></node><node TEXT="Auteur: Farquhar, Sebastian"><node TEXT="On {Statistical} {Bias} {In} {Active} {Learning}: {How} and {When} {To} {Fix} {It}" /></node><node TEXT="Auteur: Fazzolari, Rocco"><node TEXT="Multi-{Agent} {Reinforcement} {Learning}: {A} {Review} of {Challenges} and {Applications}" /></node><node TEXT="Auteur: Gal, Yarin"><node TEXT="On {Statistical} {Bias} {In} {Active} {Learning}: {How} and {When} {To} {Fix} {It}" /></node><node TEXT="Auteur: Giardino, Daniele"><node TEXT="Multi-{Agent} {Reinforcement} {Learning}: {A} {Review} of {Challenges} and {Applications}" /></node><node TEXT="Auteur: Ho, Chien-Ju"><node TEXT="Bandit {Learning} with {Delayed} {Impact} of {Actions}" /></node><node TEXT="Auteur: Huang, Sheng-Jun"><node TEXT="Improving {Model} {Robustness} by {Adaptively} {Correcting} {Perturbation} {Levels} with {Active} {Queries}" /></node><node TEXT="Auteur: Kishk, Mustafa A."><node TEXT="Improving {Spectral} {Efficiency} of {Wireless} {Networks} through {Democratic} {Spectrum} {Sharing}" /></node><node TEXT="Auteur: Leshem, Amir"><node TEXT="One for {All} and {All} for {One}: {Distributed} {Learning} of {Fair} {Allocations} {With} {Multi}-{Player} {Bandits}" /></node><node TEXT="Auteur: Liu, Yang"><node TEXT="Bandit {Learning} with {Delayed} {Impact} of {Actions}" /></node><node TEXT="Auteur: Mehr, Negar"><node TEXT="Maximum-{Entropy} {Multi}-{Agent} {Dynamic} {Games}: {Forward} and {Inverse} {Solutions}" /></node><node TEXT="Auteur: Ning, Kun-Peng"><node TEXT="Improving {Model} {Robustness} by {Adaptively} {Correcting} {Perturbation} {Levels} with {Active} {Queries}" /></node><node TEXT="Auteur: Rahman, Aniq Ur"><node TEXT="Improving {Spectral} {Efficiency} of {Wireless} {Networks} through {Democratic} {Spectrum} {Sharing}" /></node><node TEXT="Auteur: Rainforth, Tom"><node TEXT="On {Statistical} {Bias} {In} {Active} {Learning}: {How} and {When} {To} {Fix} {It}" /></node><node TEXT="Auteur: Re, Marco"><node TEXT="Multi-{Agent} {Reinforcement} {Learning}: {A} {Review} of {Challenges} and {Applications}" /></node><node TEXT="Auteur: Schwager, Mac"><node TEXT="Maximum-{Entropy} {Multi}-{Agent} {Dynamic} {Games}: {Forward} and {Inverse} {Solutions}" /></node><node TEXT="Auteur: Spanò, Sergio"><node TEXT="Multi-{Agent} {Reinforcement} {Learning}: {A} {Review} of {Challenges} and {Applications}" /></node><node TEXT="Auteur: Tang, Wei"><node TEXT="Bandit {Learning} with {Delayed} {Impact} of {Actions}" /></node><node TEXT="Auteur: Tao, Lue"><node TEXT="Improving {Model} {Robustness} by {Adaptively} {Correcting} {Perturbation} {Levels} with {Active} {Queries}" /></node><node TEXT="Auteur: Wang, Jun"><node TEXT="An {Overview} of {Multi}-{Agent} {Reinforcement} {Learning} from {Game} {Theoretical} {Perspective}" /></node><node TEXT="Auteur: Wang, Mingyu"><node TEXT="Maximum-{Entropy} {Multi}-{Agent} {Dynamic} {Games}: {Forward} and {Inverse} {Solutions}" /></node><node TEXT="Auteur: Yang, Yaodong"><node TEXT="An {Overview} of {Multi}-{Agent} {Reinforcement} {Learning} from {Game} {Theoretical} {Perspective}" /></node></node><node TEXT="Année: 2022"><node TEXT="Auteur: Antonakopoulos, Kimon"><node TEXT="A universal black-box optimization method with almost dimension-free convergence rate guarantees" /></node><node TEXT="Auteur: AuteurInconnu"><node TEXT="A {Stochastic} {Variant} of {Replicator} {Dynamics} in {Zero}-{Sum} {Games} and {Its} {Invariant} {Measures}" /></node><node TEXT="Auteur: Bagh, Adib"><node TEXT="Modelling {Cournot} {Games} as {Multi}-agent {Multi}-armed {Bandits}" /></node><node TEXT="Auteur: Bambos, Nicholas"><node TEXT="Learning in {Games} with {Quantized} {Payoff} {Observations}" /></node><node TEXT="Auteur: Bouneffouf, Djallel"><node TEXT="Online {Learning} in {Iterated} {Prisoner}'s {Dilemma} to {Mimic} {Human} {Behavior}" /></node><node TEXT="Auteur: Brown, Gavin"><node TEXT="Performative {Prediction} in a {Stateful} {World}" /></node><node TEXT="Auteur: Bryan, Chris"><node TEXT="{ConceptExplainer}: {Interactive} {Explanation} for {Deep} {Neural} {Networks} from a {Concept} {Perspective}" /></node><node TEXT="Auteur: Cecchi, Guillermo"><node TEXT="Online {Learning} in {Iterated} {Prisoner}'s {Dilemma} to {Mimic} {Human} {Behavior}" /></node><node TEXT="Auteur: Cevher, Vokan"><node TEXT="A universal black-box optimization method with almost dimension-free convergence rate guarantees" /></node><node TEXT="Auteur: Costantini, Marina"><node TEXT="Pick your {Neighbor}: {Local} {Gauss}-{Southwell} {Rule} for {Fast} {Asynchronous} {Decentralized} {Optimization}" /></node><node TEXT="Auteur: Frasca, Paolo"><node TEXT="The {Closed} {Loop} {Between} {Opinion} {Formation} and {Personalized} {Recommendations}" /></node><node TEXT="Auteur: Gaissmaier, Wolfgang"><node TEXT="Conformist social learning leads to self-organised prevention against adverse bias in risky decision making" /></node><node TEXT="Auteur: Giannou, Angeliki"><node TEXT="On the convergence of policy gradient methods to {Nash} equilibria in general stochastic games" /></node><node TEXT="Auteur: Harrison, Brent"><node TEXT="Modelling {Cournot} {Games} as {Multi}-agent {Multi}-armed {Bandits}" /></node><node TEXT="Auteur: Hod, Shlomi"><node TEXT="Performative {Prediction} in a {Stateful} {World}" /></node><node TEXT="Auteur: Huang, Jinbin"><node TEXT="{ConceptExplainer}: {Interactive} {Explanation} for {Deep} {Neural} {Networks} from a {Concept} {Perspective}" /></node><node TEXT="Auteur: Jordan, Michael I."><node TEXT="Explicit {Second}-{Order} {Min}-{Max} {Optimization} {Methods} with {Optimal} {Convergence} {Guarantee}" /></node><node TEXT="Auteur: Kalemaj, Iden"><node TEXT="Performative {Prediction} in a {Stateful} {World}" /></node><node TEXT="Auteur: Krishnamurthy, Vikram"><node TEXT="Dynamics of {Social} {Networks}: {Multi}-agent {Information} {Fusion}, {Anticipatory} {Decision} {Making} and {Polling}" /></node><node TEXT="Auteur: Kwon, Bum Chul"><node TEXT="{ConceptExplainer}: {Interactive} {Explanation} for {Deep} {Neural} {Networks} from a {Concept} {Perspective}" /></node><node TEXT="Auteur: Levy, Kfir Y."><node TEXT="A universal black-box optimization method with almost dimension-free convergence rate guarantees" /></node><node TEXT="Auteur: Liakopoulos, Nikolaos"><node TEXT="Pick your {Neighbor}: {Local} {Gauss}-{Southwell} {Rule} for {Fast} {Asynchronous} {Decentralized} {Optimization}" /></node><node TEXT="Auteur: Lin, Baihan"><node TEXT="Online {Learning} in {Iterated} {Prisoner}'s {Dilemma} to {Mimic} {Human} {Behavior}" /></node><node TEXT="Auteur: Lin, Tianyi"><node TEXT="Explicit {Second}-{Order} {Min}-{Max} {Optimization} {Methods} with {Optimal} {Convergence} {Guarantee}" /></node><node TEXT="Auteur: Lotidis, Kyriakos"><node TEXT="Learning in {Games} with {Quantized} {Payoff} {Observations}" /><node TEXT="On the convergence of policy gradient methods to {Nash} equilibria in general stochastic games" /></node><node TEXT="Auteur: Madhow, Upamanyu"><node TEXT="A {Dynamic} {Decision}-{Making} {Framework} {Promoting} {Long}-{Term} {Fairness}" /></node><node TEXT="Auteur: Martin, Matthieu"><node TEXT="Nested bandits" /></node><node TEXT="Auteur: Mertikopoulos, Panayotis"><node TEXT="A universal black-box optimization method with almost dimension-free convergence rate guarantees" /><node TEXT="Pick your {Neighbor}: {Local} {Gauss}-{Southwell} {Rule} for {Fast} {Asynchronous} {Decentralized} {Optimization}" /><node TEXT="Learning in {Games} with {Quantized} {Payoff} {Observations}" /><node TEXT="Nested bandits" /><node TEXT="Explicit {Second}-{Order} {Min}-{Max} {Optimization} {Methods} with {Optimal} {Convergence} {Guarantee}" /><node TEXT="Survival of dominated strategies under imitation dynamics" /><node TEXT="On the convergence of policy gradient methods to {Nash} equilibria in general stochastic games" /></node><node TEXT="Auteur: Mishra, Aditi"><node TEXT="{ConceptExplainer}: {Interactive} {Explanation} for {Deep} {Neural} {Networks} from a {Concept} {Perspective}" /></node><node TEXT="Auteur: Pedarsani, Ramtin"><node TEXT="A {Dynamic} {Decision}-{Making} {Framework} {Promoting} {Long}-{Term} {Fairness}" /></node><node TEXT="Auteur: Pfeiffer, Thomas"><node TEXT="Decision {Market} {Based} {Learning} {For} {Multi}-agent {Contextual} {Bandit} {Problems}" /></node><node TEXT="Auteur: Polderman, Jan Willem"><node TEXT="The {Closed} {Loop} {Between} {Opinion} {Formation} and {Personalized} {Recommendations}" /></node><node TEXT="Auteur: Puranik, Bhagyashree"><node TEXT="A {Dynamic} {Decision}-{Making} {Framework} {Promoting} {Long}-{Term} {Fairness}" /></node><node TEXT="Auteur: Rahier, Thibaud"><node TEXT="Nested bandits" /></node><node TEXT="Auteur: Rossi, Wilbert Samuel"><node TEXT="The {Closed} {Loop} {Between} {Opinion} {Formation} and {Personalized} {Recommendations}" /></node><node TEXT="Auteur: Spyropoulos, Thrasyvoulos"><node TEXT="Pick your {Neighbor}: {Local} {Gauss}-{Southwell} {Rule} for {Fast} {Asynchronous} {Decentralized} {Optimization}" /></node><node TEXT="Auteur: Taywade, Kshitija"><node TEXT="Modelling {Cournot} {Games} as {Multi}-agent {Multi}-armed {Bandits}" /></node><node TEXT="Auteur: Toyokawa, Wataru"><node TEXT="Conformist social learning leads to self-organised prevention against adverse bias in risky decision making" /></node><node TEXT="Auteur: Viossat, Yannick"><node TEXT="Survival of dominated strategies under imitation dynamics" /></node><node TEXT="Auteur: Vlatakis-Gkaragkounis, Emmanouil-Vasileios"><node TEXT="On the convergence of policy gradient methods to {Nash} equilibria in general stochastic games" /></node><node TEXT="Auteur: Vu, Dong Quan"><node TEXT="A universal black-box optimization method with almost dimension-free convergence rate guarantees" /></node><node TEXT="Auteur: Wang, Wenlong"><node TEXT="Decision {Market} {Based} {Learning} {For} {Multi}-agent {Contextual} {Bandit} {Problems}" /></node><node TEXT="Auteur: Zenati, Houssam"><node TEXT="Nested bandits" /></node></node><node TEXT="Année: 2023"><node TEXT="Auteur: Abe, Kenshi"><node TEXT="Learning in {Multi}-{Memory} {Games} {Triggers} {Complex} {Dynamics} {Diverging} from {Nash} {Equilibrium}" /></node><node TEXT="Auteur: Abràmoff, Michael D."><node TEXT="Considerations for addressing bias in artificial intelligence for health equity" /></node><node TEXT="Auteur: Algorithmic Interpretation Working Group of the Collaborative Community for Ophthalmic Imaging Foundation, Washington, D.C.}"><node TEXT="Considerations for addressing bias in artificial intelligence for health equity" /></node><node TEXT="Auteur: Amodei, Dario"><node TEXT="Deep reinforcement learning from human preferences" /></node><node TEXT="Auteur: Anthony, Thomas"><node TEXT="Population-based {Evaluation} in {Repeated} {Rock}-{Paper}-{Scissors} as a {Benchmark} for {Multiagent} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Antonakopoulos, Kimon"><node TEXT="No-{Regret} {Learning} in {Games} with {Noisy} {Feedback}: {Faster} {Rates} and {Adaptivity} via {Learning} {Rate} {Separation}" /></node><node TEXT="Auteur: Ariu, Kaito"><node TEXT="Learning in {Multi}-{Memory} {Games} {Triggers} {Complex} {Dynamics} {Diverging} from {Nash} {Equilibrium}" /></node><node TEXT="Auteur: Azar, Mohammad Gheshlaghi"><node TEXT="A {General} {Theoretical} {Paradigm} to {Understand} {Learning} from {Human} {Preferences}" /></node><node TEXT="Auteur: Bai, Xingjian"><node TEXT="Goodhart's {Law} in {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Bambos, Nicholas"><node TEXT="Payoff-based learning with matrix multiplicative weights in quantum games" /><node TEXT="Learning in quantum games" /></node><node TEXT="Auteur: Bansal, Poonam"><node TEXT="Convergence to {Nash} {Equilibrium}: {A} {Comparative} {Study} of {Rock}-{Paper}-{Scissors} {Algorithms}" /></node><node TEXT="Auteur: Baumann, Joachim"><node TEXT="A {Classification} of {Feedback} {Loops} and {Their} {Relation} to {Biases} in {Automated} {Decision}-{Making} {Systems}" /></node><node TEXT="Auteur: Bengio, Emmanuel"><node TEXT="Biological {Sequence} {Design} with {GFlowNets}" /></node><node TEXT="Auteur: Bengio, Yoshua"><node TEXT="Biological {Sequence} {Design} with {GFlowNets}" /></node><node TEXT="Auteur: Blanchet, Jose"><node TEXT="Payoff-based learning with matrix multiplicative weights in quantum games" /></node><node TEXT="Auteur: Bolognani, Saverio"><node TEXT="A {Classification} of {Feedback} {Loops} and {Their} {Relation} to {Biases} in {Automated} {Decision}-{Making} {Systems}" /></node><node TEXT="Auteur: Boone, Victor"><node TEXT="The equivalence of dynamic and strategic stability under regularized learning in games" /></node><node TEXT="Auteur: Brown, Tom B."><node TEXT="Deep reinforcement learning from human preferences" /></node><node TEXT="Auteur: Burch, Neil"><node TEXT="Population-based {Evaluation} in {Repeated} {Rock}-{Paper}-{Scissors} as a {Benchmark} for {Multiagent} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Cai, Ruichu"><node TEXT="A {Survey} on {Causal} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Calandriello, Daniele"><node TEXT="A {General} {Theoretical} {Paradigm} to {Understand} {Learning} from {Human} {Preferences}" /></node><node TEXT="Auteur: Cevher, Volkan"><node TEXT="No-{Regret} {Learning} in {Games} with {Noisy} {Feedback}: {Faster} {Rates} and {Adaptivity} via {Learning} {Rate} {Separation}" /><node TEXT="A unified stochastic approximation framework for learning in games" /></node><node TEXT="Auteur: Char, Danton"><node TEXT="Considerations for addressing bias in artificial intelligence for health equity" /></node><node TEXT="Auteur: Christiano, Paul"><node TEXT="Deep reinforcement learning from human preferences" /></node><node TEXT="Auteur: Das, Payel"><node TEXT="Biological {Sequence} {Design} with {GFlowNets}" /></node><node TEXT="Auteur: De Pasquale, Giulia"><node TEXT="A {Classification} of {Feedback} {Loops} and {Their} {Relation} to {Biases} in {Automated} {Decision}-{Making} {Systems}" /></node><node TEXT="Auteur: Dossou, Bonaventure F. P."><node TEXT="Biological {Sequence} {Design} with {GFlowNets}" /></node><node TEXT="Auteur: Ekbote, Chanakya"><node TEXT="Biological {Sequence} {Design} with {GFlowNets}" /></node><node TEXT="Auteur: Elokda, Ezzat"><node TEXT="A {Classification} of {Feedback} {Loops} and {Their} {Relation} to {Biases} in {Automated} {Decision}-{Making} {Systems}" /></node><node TEXT="Auteur: Eydelman, Malvina B."><node TEXT="Considerations for addressing bias in artificial intelligence for health equity" /></node><node TEXT="Auteur: Foster, Dean"><node TEXT="On the {Complexity} of {Multi}-{Agent} {Decision} {Making}: {From} {Learning} in {Games} to {Partial} {Monitoring}" /></node><node TEXT="Auteur: Foster, Dylan J."><node TEXT="On the {Complexity} of {Multi}-{Agent} {Decision} {Making}: {From} {Learning} in {Games} to {Partial} {Monitoring}" /></node><node TEXT="Auteur: Fu, Jie"><node TEXT="Biological {Sequence} {Design} with {GFlowNets}" /></node><node TEXT="Auteur: Fujimoto, Yuma"><node TEXT="Learning in {Multi}-{Memory} {Games} {Triggers} {Complex} {Dynamics} {Diverging} from {Nash} {Equilibrium}" /></node><node TEXT="Auteur: Garcia, Alex-Hernandez"><node TEXT="Biological {Sequence} {Design} with {GFlowNets}" /></node><node TEXT="Auteur: Golowich, Noah"><node TEXT="On the {Complexity} of {Multi}-{Agent} {Decision} {Making}: {From} {Learning} in {Games} to {Partial} {Monitoring}" /></node><node TEXT="Auteur: Grefenstette, Edward"><node TEXT="General intelligence requires rethinking exploration" /></node><node TEXT="Auteur: Griffin, Charlie"><node TEXT="Goodhart's {Law} in {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Guo, Daniel"><node TEXT="A {General} {Theoretical} {Paradigm} to {Understand} {Learning} from {Human} {Preferences}" /></node><node TEXT="Auteur: Hannák, Anikó"><node TEXT="A {Classification} of {Feedback} {Loops} and {Their} {Relation} to {Biases} in {Automated} {Decision}-{Making} {Systems}" /></node><node TEXT="Auteur: Hao, Zhifeng"><node TEXT="A {Survey} on {Causal} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Hayman, Oliver"><node TEXT="Goodhart's {Law} in {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Hennes, Daniel"><node TEXT="Population-based {Evaluation} in {Repeated} {Rock}-{Paper}-{Scissors} as a {Benchmark} for {Multiagent} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Hsieh, Ya-Ping"><node TEXT="Riemannian stochastic optimization methods avoid strict saddle points" /><node TEXT="A unified stochastic approximation framework for learning in games" /></node><node TEXT="Auteur: Hsieh, Yu-Guan"><node TEXT="No-{Regret} {Learning} in {Games} with {Noisy} {Feedback}: {Faster} {Rates} and {Adaptivity} via {Learning} {Rate} {Separation}" /></node><node TEXT="Auteur: Huang, Libo"><node TEXT="A {Survey} on {Causal} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Jain, Moksh"><node TEXT="Biological {Sequence} {Design} with {GFlowNets}" /></node><node TEXT="Auteur: Jiang, Minqi"><node TEXT="General intelligence requires rethinking exploration" /></node><node TEXT="Auteur: Jordan, Michael I."><node TEXT="A {Quadratic} {Speedup} in {Finding} {Nash} {Equilibria} of {Quantum} {Zero}-{Sum} {Games}" /></node><node TEXT="Auteur: Kahng, Minsuk"><node TEXT="{VLSlice}: {Interactive} {Vision}-and-{Language} {Slice} {Discovery}" /></node><node TEXT="Auteur: Karimi, Mohammad Reza"><node TEXT="Riemannian stochastic optimization methods avoid strict saddle points" /></node><node TEXT="Auteur: Karwowski, Jacek"><node TEXT="Goodhart's {Law} in {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Kiendlhofer, Klaus"><node TEXT="Goodhart's {Law} in {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Kilgour, Micheal"><node TEXT="Biological {Sequence} {Design} with {GFlowNets}" /></node><node TEXT="Auteur: Klabjan, Diego"><node TEXT="Regret {Lower} {Bounds} in {Multi}-agent {Multi}-armed {Bandit}" /></node><node TEXT="Auteur: Klügl, Franziska"><node TEXT="Modelling {Agent} {Decision} {Making} in {Agent}-based {Simulation} - {Analysis} {Using} an {Economic} {Technology} {Uptake} {Model}" /></node><node TEXT="Auteur: Krause, Andreas"><node TEXT="Riemannian stochastic optimization methods avoid strict saddle points" /></node><node TEXT="Auteur: Lan, Tian"><node TEXT="{MAC}-{PO}: {Multi}-{Agent} {Experience} {Replay} via {Collective} {Priority} {Optimization}" /></node><node TEXT="Auteur: Lanctot, Marc"><node TEXT="Population-based {Evaluation} in {Repeated} {Rock}-{Paper}-{Scissors} as a {Benchmark} for {Multiagent} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Lee, Jonathan"><node TEXT="Dueling {RL}: {Reinforcement} {Learning} with {Trajectory} {Preferences}" /></node><node TEXT="Auteur: Lee, Stefan"><node TEXT="{VLSlice}: {Interactive} {Vision}-and-{Language} {Slice} {Discovery}" /></node><node TEXT="Auteur: Legg, Shane"><node TEXT="Deep reinforcement learning from human preferences" /></node><node TEXT="Auteur: Leike, Jan"><node TEXT="Deep reinforcement learning from human preferences" /></node><node TEXT="Auteur: Lotidis, Kyriakos"><node TEXT="Payoff-based learning with matrix multiplicative weights in quantum games" /><node TEXT="Learning in quantum games" /></node><node TEXT="Auteur: Loyo-Berrios, Nilsa"><node TEXT="Considerations for addressing bias in artificial intelligence for health equity" /></node><node TEXT="Auteur: Maisel, William H."><node TEXT="Considerations for addressing bias in artificial intelligence for health equity" /></node><node TEXT="Auteur: Martic, Miljan"><node TEXT="Deep reinforcement learning from human preferences" /></node><node TEXT="Auteur: Mei, Yongsheng"><node TEXT="{MAC}-{PO}: {Multi}-{Agent} {Experience} {Replay} via {Collective} {Priority} {Optimization}" /></node><node TEXT="Auteur: Mertikopoulos, Panayotis"><node TEXT="No-{Regret} {Learning} in {Games} with {Noisy} {Feedback}: {Faster} {Rates} and {Adaptivity} via {Learning} {Rate} {Separation}" /><node TEXT="Payoff-based learning with matrix multiplicative weights in quantum games" /><node TEXT="Learning in quantum games" /><node TEXT="Exploiting hidden structures in non-convex games for convergence to {Nash} equilibrium" /><node TEXT="A {Quadratic} {Speedup} in {Finding} {Nash} {Equilibria} of {Quantum} {Zero}-{Sum} {Games}" /><node TEXT="Riemannian stochastic optimization methods avoid strict saddle points" /><node TEXT="The equivalence of dynamic and strategic stability under regularized learning in games" /><node TEXT="A unified stochastic approximation framework for learning in games" /></node><node TEXT="Auteur: Munos, Rémi"><node TEXT="A {General} {Theoretical} {Paradigm} to {Understand} {Learning} from {Human} {Preferences}" /></node><node TEXT="Auteur: Obermeyer, Ziad"><node TEXT="Considerations for addressing bias in artificial intelligence for health equity" /></node><node TEXT="Auteur: Pacchiano, Aldo"><node TEXT="Dueling {RL}: {Reinforcement} {Learning} with {Trajectory} {Preferences}" /></node><node TEXT="Auteur: Pagan, Nicolò"><node TEXT="A {Classification} of {Feedback} {Loops} and {Their} {Relation} to {Biases} in {Automated} {Decision}-{Making} {Systems}" /></node><node TEXT="Auteur: Perolat, Julien"><node TEXT="Population-based {Evaluation} in {Repeated} {Rock}-{Paper}-{Scissors} as a {Benchmark} for {Multiagent} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Piliouras, Georgios"><node TEXT="Exploiting hidden structures in non-convex games for convergence to {Nash} equilibrium" /><node TEXT="A {Quadratic} {Speedup} in {Finding} {Nash} {Equilibria} of {Quantum} {Zero}-{Sum} {Games}" /></node><node TEXT="Auteur: Piot, Bilal"><node TEXT="A {General} {Theoretical} {Paradigm} to {Understand} {Learning} from {Human} {Preferences}" /></node><node TEXT="Auteur: Rakhlin, Alexander"><node TEXT="On the {Complexity} of {Multi}-{Agent} {Decision} {Making}: {From} {Learning} in {Games} to {Partial} {Monitoring}" /></node><node TEXT="Auteur: Rani, Ritu"><node TEXT="Convergence to {Nash} {Equilibrium}: {A} {Comparative} {Study} of {Rock}-{Paper}-{Scissors} {Algorithms}" /></node><node TEXT="Auteur: Rector-Brooks, Jarrid"><node TEXT="Biological {Sequence} {Design} with {GFlowNets}" /></node><node TEXT="Auteur: Reddy, M Deekshitha"><node TEXT="Convergence to {Nash} {Equilibrium}: {A} {Comparative} {Study} of {Rock}-{Paper}-{Scissors} {Algorithms}" /></node><node TEXT="Auteur: Rocktäschel, Tim"><node TEXT="General intelligence requires rethinking exploration" /></node><node TEXT="Auteur: Rowland, Mark"><node TEXT="A {General} {Theoretical} {Paradigm} to {Understand} {Learning} from {Human} {Preferences}" /></node><node TEXT="Auteur: Saha, Aadirupa"><node TEXT="Dueling {RL}: {Reinforcement} {Learning} with {Trajectory} {Preferences}" /></node><node TEXT="Auteur: Sakos, Iosif"><node TEXT="Exploiting hidden structures in non-convex games for convergence to {Nash} equilibrium" /></node><node TEXT="Auteur: Sarkar, Soumajyoti"><node TEXT="Bandit based centralized matching in two-sided markets for peer to peer lending" /></node><node TEXT="Auteur: Schultz, John"><node TEXT="Population-based {Evaluation} in {Repeated} {Rock}-{Paper}-{Scissors} as a {Benchmark} for {Multiagent} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Simine, Lena"><node TEXT="Biological {Sequence} {Design} with {GFlowNets}" /></node><node TEXT="Auteur: Skalse, Joar"><node TEXT="Goodhart's {Law} in {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Slyman, Eric"><node TEXT="{VLSlice}: {Interactive} {Vision}-and-{Language} {Slice} {Discovery}" /></node><node TEXT="Auteur: Smith, Max Olan"><node TEXT="Population-based {Evaluation} in {Repeated} {Rock}-{Paper}-{Scissors} as a {Benchmark} for {Multiagent} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Sun, Fuchun"><node TEXT="A {Survey} on {Causal} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Tarver, Michelle E."><node TEXT="Considerations for addressing bias in artificial intelligence for health equity" /></node><node TEXT="Auteur: Trujillo, Sylvia"><node TEXT="Considerations for addressing bias in artificial intelligence for health equity" /></node><node TEXT="Auteur: Vadali, Geetika"><node TEXT="Convergence to {Nash} {Equilibrium}: {A} {Comparative} {Study} of {Rock}-{Paper}-{Scissors} {Algorithms}" /></node><node TEXT="Auteur: Valko, Michal"><node TEXT="A {General} {Theoretical} {Paradigm} to {Understand} {Learning} from {Human} {Preferences}" /></node><node TEXT="Auteur: Vasconcelos, Francisca"><node TEXT="A {Quadratic} {Speedup} in {Finding} {Nash} {Equilibria} of {Quantum} {Zero}-{Sum} {Games}" /></node><node TEXT="Auteur: Venkataramani, Guru"><node TEXT="{MAC}-{PO}: {Multi}-{Agent} {Experience} {Replay} via {Collective} {Priority} {Optimization}" /></node><node TEXT="Auteur: Vlatakis-Gkaragkounis, Emmanouil-Vasileios"><node TEXT="Exploiting hidden structures in non-convex games for convergence to {Nash} equilibrium" /><node TEXT="A {Quadratic} {Speedup} in {Finding} {Nash} {Equilibria} of {Quantum} {Zero}-{Sum} {Games}" /></node><node TEXT="Auteur: Wei, Peng"><node TEXT="{MAC}-{PO}: {Multi}-{Agent} {Experience} {Replay} via {Collective} {Priority} {Optimization}" /></node><node TEXT="Auteur: Xu, Mengfan"><node TEXT="Regret {Lower} {Bounds} in {Multi}-agent {Multi}-armed {Bandit}" /></node><node TEXT="Auteur: Zeng, Yan"><node TEXT="A {Survey} on {Causal} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Zhang, Dinghuai"><node TEXT="Biological {Sequence} {Design} with {GFlowNets}" /></node><node TEXT="Auteur: Zhang, Tianyu"><node TEXT="Biological {Sequence} {Design} with {GFlowNets}" /></node><node TEXT="Auteur: Zhou, Hanhan"><node TEXT="{MAC}-{PO}: {Multi}-{Agent} {Experience} {Replay} via {Collective} {Priority} {Optimization}" /></node><node TEXT="Auteur: {Foundational Principles of Ophthalmic Imaging"><node TEXT="Considerations for addressing bias in artificial intelligence for health equity" /></node></node><node TEXT="Année: 2024"><node TEXT="Auteur: Abe, Kenshi"><node TEXT="Memory {Asymmetry} {Creates} {Heteroclinic} {Orbits} to {Nash} {Equilibrium} in {Learning} in {Zero}-{Sum} {Games}" /></node><node TEXT="Auteur: Agarwal, Arpit"><node TEXT="Online {Recommendations} for {Agents} with {Discounted} {Adaptive} {Preferences}" /></node><node TEXT="Auteur: Aghajohari, Milad"><node TEXT="{LOQA}: {LEARNING} {WITH} {OPPONENT} {Q}-{LEARNING} {AWARENESS}" /></node><node TEXT="Auteur: Ariu, Kaito"><node TEXT="Memory {Asymmetry} {Creates} {Heteroclinic} {Orbits} to {Nash} {Equilibrium} in {Learning} in {Zero}-{Sum} {Games}" /></node><node TEXT="Auteur: Arslan, Gürdal"><node TEXT="Paths to {Equilibrium} in {Games}" /></node><node TEXT="Auteur: Asmar, Dylan M."><node TEXT="Efficient {Multiagent} {Planning} via {Shared} {Action} {Suggestions}" /></node><node TEXT="Auteur: Azizian, Waïss"><node TEXT="What is the long-run distribution of stochastic gradient descent? {A} large deviations analysis" /><node TEXT="The rate of convergence of {Bregman} proximal methods: {Local} geometry vs. regularity vs. sharpness" /></node><node TEXT="Auteur: Bambos, Nicholas"><node TEXT="Accelerated regularized learning in finite {N}-person games" /></node><node TEXT="Auteur: Belonax, Tim"><node TEXT="Alignment faking in large language models" /></node><node TEXT="Auteur: Beltrame, Giovanni"><node TEXT="Evolution with {Opponent}-{Learning} {Awareness}" /></node><node TEXT="Auteur: Bishop, Colton"><node TEXT="{RLAIF} vs. {RLHF}: {Scaling} {Reinforcement} {Learning} from {Human} {Feedback} with {AI} {Feedback}" /></node><node TEXT="Auteur: Bouteiller, Yann"><node TEXT="Evolution with {Opponent}-{Learning} {Awareness}" /></node><node TEXT="Auteur: Bowman, Samuel R."><node TEXT="Alignment faking in large language models" /></node><node TEXT="Auteur: Brown, William"><node TEXT="Online {Recommendations} for {Agents} with {Discounted} {Adaptive} {Preferences}" /></node><node TEXT="Auteur: Carbune, Victor"><node TEXT="{RLAIF} vs. {RLHF}: {Scaling} {Reinforcement} {Learning} from {Human} {Feedback} with {AI} {Feedback}" /></node><node TEXT="Auteur: Castellano, Claudio"><node TEXT="Large {Language} {Model} agents can coordinate beyond human scale" /></node><node TEXT="Auteur: Chen, Jack"><node TEXT="Alignment faking in large language models" /></node><node TEXT="Auteur: Cooijmans, Tim"><node TEXT="{LOQA}: {LEARNING} {WITH} {OPPONENT} {Q}-{LEARNING} {AWARENESS}" /></node><node TEXT="Auteur: Courville, Aaron"><node TEXT="{LOQA}: {LEARNING} {WITH} {OPPONENT} {Q}-{LEARNING} {AWARENESS}" /></node><node TEXT="Auteur: Denison, Carson"><node TEXT="Alignment faking in large language models" /></node><node TEXT="Auteur: Douglas, Connor"><node TEXT="Naive {Algorithmic} {Collusion}: {When} {Do} {Bandit} {Learners} {Cooperate} and {When} {Do} {They} {Compete}?" /></node><node TEXT="Auteur: Duque, Juan Agustin"><node TEXT="{LOQA}: {LEARNING} {WITH} {OPPONENT} {Q}-{LEARNING} {AWARENESS}" /></node><node TEXT="Auteur: Duvenaud, David"><node TEXT="Alignment faking in large language models" /></node><node TEXT="Auteur: Everitt, Tom"><node TEXT="Robust agents learn causal world models" /></node><node TEXT="Auteur: Falniowski, Fryderyk"><node TEXT="On the discrete-time origins of the replicator dynamics: {From} convergence to instability and chaos" /></node><node TEXT="Auteur: Ferret, Johan"><node TEXT="{RLAIF} vs. {RLHF}: {Scaling} {Reinforcement} {Learning} from {Human} {Feedback} with {AI} {Feedback}" /></node><node TEXT="Auteur: Fujimoto, Yuma"><node TEXT="Memory {Asymmetry} {Creates} {Heteroclinic} {Orbits} to {Nash} {Equilibrium} in {Learning} in {Zero}-{Sum} {Games}" /></node><node TEXT="Auteur: Garcia, David"><node TEXT="Large {Language} {Model} agents can coordinate beyond human scale" /></node><node TEXT="Auteur: Giannou, Angeliki"><node TEXT="Accelerated regularized learning in finite {N}-person games" /></node><node TEXT="Auteur: Greenblatt, Ryan"><node TEXT="Alignment faking in large language models" /></node><node TEXT="Auteur: Guldogan, Ozgur"><node TEXT="Long-{Term} {Fairness} in {Sequential} {Multi}-{Agent} {Selection} with {Positive} {Reinforcement}" /></node><node TEXT="Auteur: Guo, Zhiwu"><node TEXT="Fair {Probabilistic} {Multi}-{Armed} {Bandit} {With} {Applications} to {Network} {Optimization}" /></node><node TEXT="Auteur: Gürsoy, Kemal"><node TEXT="Multi-armed bandit games" /></node><node TEXT="Auteur: Hall, Ethan"><node TEXT="{RLAIF} vs. {RLHF}: {Scaling} {Reinforcement} {Learning} from {Human} {Feedback} with {AI} {Feedback}" /></node><node TEXT="Auteur: Hubinger, Evan"><node TEXT="Alignment faking in large language models" /></node><node TEXT="Auteur: Iutzeler, Franck"><node TEXT="What is the long-run distribution of stochastic gradient descent? {A} large deviations analysis" /><node TEXT="The rate of convergence of {Bregman} proximal methods: {Local} geometry vs. regularity vs. sharpness" /></node><node TEXT="Auteur: Kaplan, Jared"><node TEXT="Alignment faking in large language models" /></node><node TEXT="Auteur: Khan, Akbir"><node TEXT="Alignment faking in large language models" /></node><node TEXT="Auteur: Kochenderfer, Mykel J."><node TEXT="Efficient {Multiagent} {Planning} via {Shared} {Action} {Suggestions}" /></node><node TEXT="Auteur: Krunz, Marwan"><node TEXT="Fair {Probabilistic} {Multi}-{Armed} {Bandit} {With} {Applications} to {Network} {Optimization}" /></node><node TEXT="Auteur: Lee, Harrison"><node TEXT="{RLAIF} vs. {RLHF}: {Scaling} {Reinforcement} {Learning} from {Human} {Feedback} with {AI} {Feedback}" /></node><node TEXT="Auteur: Legacci, Davide"><node TEXT="No-regret learning in harmonic games: {Extrapolation} in the face of conflicting interests" /><node TEXT="A geometric decomposition of finite games: {Convergence} vs. recurrence under exponential weights" /></node><node TEXT="Auteur: Leshem, Amir"><node TEXT="Fair {Multi}-{Agent} {Bandits}" /></node><node TEXT="Auteur: Li, Ming"><node TEXT="Fair {Probabilistic} {Multi}-{Armed} {Bandit} {With} {Applications} to {Network} {Optimization}" /></node><node TEXT="Auteur: Li, Na"><node TEXT="Equilibrium {Selection} for {Multi}-agent {Reinforcement} {Learning}: {A} {Unified} {Framework}" /></node><node TEXT="Auteur: Lotidis, Kyriakos"><node TEXT="Accelerated regularized learning in finite {N}-person games" /></node><node TEXT="Auteur: Lu, Kellie"><node TEXT="{RLAIF} vs. {RLHF}: {Scaling} {Reinforcement} {Learning} from {Human} {Feedback} with {AI} {Feedback}" /></node><node TEXT="Auteur: Lui, John C. S."><node TEXT="Risk-{Aware} {Multi}-{Agent} {Multi}-{Armed} {Bandits}" /></node><node TEXT="Auteur: Lytras, Iosif"><node TEXT="Tamed {Langevin} sampling under weaker conditions" /></node><node TEXT="Auteur: MacDiarmid, Monte"><node TEXT="Alignment faking in large language models" /></node><node TEXT="Auteur: Madhow, Upamanyu"><node TEXT="Long-{Term} {Fairness} in {Sequential} {Multi}-{Agent} {Selection} with {Positive} {Reinforcement}" /></node><node TEXT="Auteur: Malick, Jérôme"><node TEXT="What is the long-run distribution of stochastic gradient descent? {A} large deviations analysis" /><node TEXT="The rate of convergence of {Bregman} proximal methods: {Local} geometry vs. regularity vs. sharpness" /></node><node TEXT="Auteur: Mansoor, Hassan"><node TEXT="{RLAIF} vs. {RLHF}: {Scaling} {Reinforcement} {Learning} from {Human} {Feedback} with {AI} {Feedback}" /></node><node TEXT="Auteur: Marks, Sam"><node TEXT="Alignment faking in large language models" /></node><node TEXT="Auteur: Marzo, Giordano De"><node TEXT="Large {Language} {Model} agents can coordinate beyond human scale" /></node><node TEXT="Auteur: Mertikopoulos, Panayotis"><node TEXT="No-regret learning in harmonic games: {Extrapolation} in the face of conflicting interests" /><node TEXT="What is the long-run distribution of stochastic gradient descent? {A} large deviations analysis" /><node TEXT="On the discrete-time origins of the replicator dynamics: {From} convergence to instability and chaos" /><node TEXT="A geometric decomposition of finite games: {Convergence} vs. recurrence under exponential weights" /><node TEXT="Tamed {Langevin} sampling under weaker conditions" /><node TEXT="Nested replicator dynamics, nested logit choice, and similarity-based learning" /><node TEXT="The rate of convergence of {Bregman} proximal methods: {Local} geometry vs. regularity vs. sharpness" /><node TEXT="Accelerated regularized learning in finite {N}-person games" /></node><node TEXT="Auteur: Mesnard, Thomas"><node TEXT="{RLAIF} vs. {RLHF}: {Scaling} {Reinforcement} {Learning} from {Human} {Feedback} with {AI} {Feedback}" /></node><node TEXT="Auteur: Michael, Julian"><node TEXT="Alignment faking in large language models" /></node><node TEXT="Auteur: Mindermann, Sören"><node TEXT="Alignment faking in large language models" /></node><node TEXT="Auteur: Papadimitriou, Christos H."><node TEXT="No-regret learning in harmonic games: {Extrapolation} in the face of conflicting interests" /></node><node TEXT="Auteur: Pavel, Lacra"><node TEXT="Paths to {Equilibrium} in {Games}" /></node><node TEXT="Auteur: Pedarsani, Ramtin"><node TEXT="Long-{Term} {Fairness} in {Sequential} {Multi}-{Agent} {Selection} with {Positive} {Reinforcement}" /></node><node TEXT="Auteur: Perez, Ethan"><node TEXT="Alignment faking in large language models" /></node><node TEXT="Auteur: Petrini, Linda"><node TEXT="Alignment faking in large language models" /></node><node TEXT="Auteur: Phatale, Samrat"><node TEXT="{RLAIF} vs. {RLHF}: {Scaling} {Reinforcement} {Learning} from {Human} {Feedback} with {AI} {Feedback}" /></node><node TEXT="Auteur: Piliouras, Georgios"><node TEXT="No-regret learning in harmonic games: {Extrapolation} in the face of conflicting interests" /></node><node TEXT="Auteur: Pradelski, Bary"><node TEXT="A geometric decomposition of finite games: {Convergence} vs. recurrence under exponential weights" /></node><node TEXT="Auteur: Pradelski, Bary S. R."><node TEXT="No-regret learning in harmonic games: {Extrapolation} in the face of conflicting interests" /></node><node TEXT="Auteur: Prakash, Sushant"><node TEXT="{RLAIF} vs. {RLHF}: {Scaling} {Reinforcement} {Learning} from {Human} {Feedback} with {AI} {Feedback}" /></node><node TEXT="Auteur: Provost, Foster"><node TEXT="Naive {Algorithmic} {Collusion}: {When} {Do} {Bandit} {Learners} {Cooperate} and {When} {Do} {They} {Compete}?" /></node><node TEXT="Auteur: Puranik, Bhagyashree"><node TEXT="Long-{Term} {Fairness} in {Sequential} {Multi}-{Agent} {Selection} with {Positive} {Reinforcement}" /></node><node TEXT="Auteur: Rastogi, Abhinav"><node TEXT="{RLAIF} vs. {RLHF}: {Scaling} {Reinforcement} {Learning} from {Human} {Feedback} with {AI} {Feedback}" /></node><node TEXT="Auteur: Richens, Jonathan"><node TEXT="Robust agents learn causal world models" /></node><node TEXT="Auteur: Roger, Fabien"><node TEXT="Alignment faking in large language models" /></node><node TEXT="Auteur: Sandholm, William H."><node TEXT="Nested replicator dynamics, nested logit choice, and similarity-based learning" /></node><node TEXT="Auteur: Shamma, Jeff"><node TEXT="Equilibrium {Selection} for {Multi}-agent {Reinforcement} {Learning}: {A} {Unified} {Framework}" /></node><node TEXT="Auteur: Shao, Junning"><node TEXT="Balanced and {Incentivized} {Learning} with {Limited} {Shared} {Information} in {Multi}-agent {Multi}-armed {Bandit}" /></node><node TEXT="Auteur: Shao, Qi"><node TEXT="Risk-{Aware} {Multi}-{Agent} {Multi}-{Armed} {Bandits}" /></node><node TEXT="Auteur: Shlegeris, Buck"><node TEXT="Alignment faking in large language models" /></node><node TEXT="Auteur: Soma, Karthik"><node TEXT="Evolution with {Opponent}-{Learning} {Awareness}" /></node><node TEXT="Auteur: Sundararajan, Arun"><node TEXT="Naive {Algorithmic} {Collusion}: {When} {Do} {Bandit} {Learners} {Cooperate} and {When} {Do} {They} {Compete}?" /></node><node TEXT="Auteur: Treutlein, Johannes"><node TEXT="Alignment faking in large language models" /></node><node TEXT="Auteur: Uesato, Jonathan"><node TEXT="Alignment faking in large language models" /></node><node TEXT="Auteur: Wright, Benjamin"><node TEXT="Alignment faking in large language models" /></node><node TEXT="Auteur: Ye, Jiancheng"><node TEXT="Risk-{Aware} {Multi}-{Agent} {Multi}-{Armed} {Bandits}" /></node><node TEXT="Auteur: Yongacoglu, Bora"><node TEXT="Paths to {Equilibrium} in {Games}" /></node><node TEXT="Auteur: Yüksel, Serdar"><node TEXT="Paths to {Equilibrium} in {Games}" /></node><node TEXT="Auteur: Zhang, Chicheng"><node TEXT="Fair {Probabilistic} {Multi}-{Armed} {Bandit} {With} {Applications} to {Network} {Optimization}" /></node><node TEXT="Auteur: Zhang, Runyu"><node TEXT="Equilibrium {Selection} for {Multi}-agent {Reinforcement} {Learning}: {A} {Unified} {Framework}" /></node></node><node TEXT="Année: 2025"><node TEXT="Auteur: Dou, Jian-Peng"><node TEXT="Photon-{Atom} {Hybrid} {Decision}-{Framework} with {Concurrent} {Exploration} {Acceleration}" /></node><node TEXT="Auteur: Guan, Xinyu"><node TEXT="{rStar}-{Math}: {Small} {LLMs} {Can} {Master} {Math} {Reasoning} with {Self}-{Evolved} {Deep} {Thinking}" /></node><node TEXT="Auteur: Jin, Xian-Min"><node TEXT="Photon-{Atom} {Hybrid} {Decision}-{Framework} with {Concurrent} {Exploration} {Acceleration}" /></node><node TEXT="Auteur: Liu, Yifei"><node TEXT="{rStar}-{Math}: {Small} {LLMs} {Can} {Master} {Math} {Reasoning} with {Self}-{Evolved} {Deep} {Thinking}" /></node><node TEXT="Auteur: Lu, Feng"><node TEXT="Photon-{Atom} {Hybrid} {Decision}-{Framework} with {Concurrent} {Exploration} {Acceleration}" /></node><node TEXT="Auteur: Shang, Ning"><node TEXT="{rStar}-{Math}: {Small} {LLMs} {Can} {Master} {Math} {Reasoning} with {Self}-{Evolved} {Deep} {Thinking}" /></node><node TEXT="Auteur: Siska, David"><node TEXT="Competitive {Pricing} {Using} {Model}-{Based} {Bandits}" /></node><node TEXT="Auteur: Sliwinski, Lukasz"><node TEXT="Competitive {Pricing} {Using} {Model}-{Based} {Bandits}" /></node><node TEXT="Auteur: Sun, Hong"><node TEXT="Photon-{Atom} {Hybrid} {Decision}-{Framework} with {Concurrent} {Exploration} {Acceleration}" /></node><node TEXT="Auteur: Sun, Youran"><node TEXT="{rStar}-{Math}: {Small} {LLMs} {Can} {Master} {Math} {Reasoning} with {Self}-{Evolved} {Deep} {Thinking}" /></node><node TEXT="Auteur: Szpruch, Lukasz"><node TEXT="Competitive {Pricing} {Using} {Model}-{Based} {Bandits}" /></node><node TEXT="Auteur: Tang, Hao"><node TEXT="Photon-{Atom} {Hybrid} {Decision}-{Framework} with {Concurrent} {Exploration} {Acceleration}" /></node><node TEXT="Auteur: Treetanthiploet, Tanut"><node TEXT="Competitive {Pricing} {Using} {Model}-{Based} {Bandits}" /></node><node TEXT="Auteur: Xu, Xiao-Yun"><node TEXT="Photon-{Atom} {Hybrid} {Decision}-{Framework} with {Concurrent} {Exploration} {Acceleration}" /></node><node TEXT="Auteur: Yang, Fan"><node TEXT="{rStar}-{Math}: {Small} {LLMs} {Can} {Master} {Math} {Reasoning} with {Self}-{Evolved} {Deep} {Thinking}" /></node><node TEXT="Auteur: Yang, Mao"><node TEXT="{rStar}-{Math}: {Small} {LLMs} {Can} {Master} {Math} {Reasoning} with {Self}-{Evolved} {Deep} {Thinking}" /></node><node TEXT="Auteur: Zhang, Chao-Ni"><node TEXT="Photon-{Atom} {Hybrid} {Decision}-{Framework} with {Concurrent} {Exploration} {Acceleration}" /></node><node TEXT="Auteur: Zhang, Li Lyna"><node TEXT="{rStar}-{Math}: {Small} {LLMs} {Can} {Master} {Math} {Reasoning} with {Self}-{Evolved} {Deep} {Thinking}" /></node><node TEXT="Auteur: Zhou, Wen-Hao"><node TEXT="Photon-{Atom} {Hybrid} {Decision}-{Framework} with {Concurrent} {Exploration} {Acceleration}" /></node><node TEXT="Auteur: Zhu, Yi"><node TEXT="{rStar}-{Math}: {Small} {LLMs} {Can} {Master} {Math} {Reasoning} with {Self}-{Evolved} {Deep} {Thinking}" /></node></node><node TEXT="Année: SansAnnée"><node TEXT="Auteur: Alatur, Pragnya"><node TEXT="Multi-{Player} {Bandits}: {The} {Adversarial} {Case}" /><node TEXT="Multi-{Player} {Bandits}: {The} {Adversarial} {Case}" /></node><node TEXT="Auteur: Anderson, Sean"><node TEXT="Learning with contextual information in non-stationary environments" /></node><node TEXT="Auteur: Aschenbrenner, Leopold"><node TEXT="Situational {Awareness}" /></node><node TEXT="Auteur: AuteurInconnu"><node TEXT="3-dynamic\_programming" /><node TEXT="Définition de projet" /><node TEXT="3-exploration\_exploitation" /><node TEXT="The {Literature} {Review}: {A} {Few} {Tips} {On} {Conducting} {It} {\textbar} {Writing} {Advice}" /></node><node TEXT="Auteur: Babes, Monica"><node TEXT="Classes of {Multiagent} {Q}-learning {Dynamics}  with -greedy {Exploration}" /></node><node TEXT="Auteur: Bachrach, Yoram"><node TEXT="Open-ended {Learning} in {Symmetric} {Zero}-sum {Games}" /></node><node TEXT="Auteur: Balduzzi, David"><node TEXT="Open-ended {Learning} in {Symmetric} {Zero}-sum {Games}" /></node><node TEXT="Auteur: Berg, Alexander C"><node TEXT="Neural {Pseudo}-{Label} {Optimism} for the {Bank} {Loan} {Problem}" /></node><node TEXT="Auteur: Besbes, Omar"><node TEXT="Stochastic {Multi}-{Armed}-{Bandit} {Problem} with {Non}-stationary {Rewards}" /></node><node TEXT="Auteur: Bhati, Rupali"><node TEXT="A {Game}-{Theoretic} {Perspective} on {Risk}-{Sensitive}{\textbackslash} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Boursier, Etienne"><node TEXT="A {Survey} on {Multi}-player {Bandits}" /></node><node TEXT="Auteur: Boutilier, Craig"><node TEXT="The {Dynamics} of {Reinforcement} {Learning} in {Cooperative} {Multiagent} {Systems}" /></node><node TEXT="Auteur: Bowling, Michael"><node TEXT="Rational and {Convergent} {Learning} in {Stochastic} {Games}" /></node><node TEXT="Auteur: Chandar, Sharath"><node TEXT="A {Game}-{Theoretic} {Perspective} on {Risk}-{Sensitive}{\textbackslash} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Chang, Yu-Han"><node TEXT="All learning is local: {Multi}-agent learning in global reward games" /></node><node TEXT="Auteur: Chou, Edward"><node TEXT="Neural {Pseudo}-{Label} {Optimism} for the {Bank} {Loan} {Problem}" /></node><node TEXT="Auteur: Claus, Caroline"><node TEXT="The {Dynamics} of {Reinforcement} {Learning} in {Cooperative} {Multiagent} {Systems}" /></node><node TEXT="Auteur: Cui, Nan"><node TEXT="Metric-{Fair} {Active} {Learning}" /></node><node TEXT="Auteur: Czarnecki, Wojciech M"><node TEXT="Open-ended {Learning} in {Symmetric} {Zero}-sum {Games}" /></node><node TEXT="Auteur: Dekel, Ofer"><node TEXT="1 {Recap}: {Diﬀerence} between {Experts} and {Bandits}" /></node><node TEXT="Auteur: Durand, Audrey"><node TEXT="Old {Dog} {Learns} {New} {Tricks}: {Randomized} {UCB} for {Bandit} {Problems}" /><node TEXT="Tracking the {Risk} of {Machine} {Learning} {Systems} with {Partial} {Monitoring}" /><node TEXT="Apprentissage par {Renforcement}" /><node TEXT="A {Game}-{Theoretic} {Perspective} on {Risk}-{Sensitive}{\textbackslash} {Reinforcement} {Learning}" /><node TEXT="On {Shallow} {Planning} {Under} {Partial} {Observability}" /></node><node TEXT="Auteur: Foerster, Jakob"><node TEXT="Neural {Pseudo}-{Label} {Optimism} for the {Bank} {Loan} {Problem}" /></node><node TEXT="Auteur: Garnelo, Marta"><node TEXT="Open-ended {Learning} in {Symmetric} {Zero}-sum {Games}" /></node><node TEXT="Auteur: Godbout, Mathieu"><node TEXT="A {Game}-{Theoretic} {Perspective} on {Risk}-{Sensitive}{\textbackslash} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Graepel, Thore"><node TEXT="Open-ended {Learning} in {Symmetric} {Zero}-sum {Games}" /></node><node TEXT="Auteur: Gur, Yonatan"><node TEXT="Stochastic {Multi}-{Armed}-{Bandit} {Problem} with {Non}-stationary {Rewards}" /></node><node TEXT="Auteur: Hardt, Moritz"><node TEXT="Strategic {Classification} is {Causal} {Modeling} in {Disguise}" /><node TEXT="Performative {Prediction}" /></node><node TEXT="Auteur: Hespanha, Joao P"><node TEXT="Learning with contextual information in non-stationary environments" /></node><node TEXT="Auteur: Heuillet, Maxime"><node TEXT="Tracking the {Risk} of {Machine} {Learning} {Systems} with {Partial} {Monitoring}" /><node TEXT="A {Game}-{Theoretic} {Perspective} on {Risk}-{Sensitive}{\textbackslash} {Reinforcement} {Learning}" /></node><node TEXT="Auteur: Ho, Chien-Ju"><node TEXT="Bandit {Learning} with {Biased} {Human} {Feedback}" /></node><node TEXT="Auteur: Ho, Tracey"><node TEXT="All learning is local: {Multi}-agent learning in global reward games" /></node><node TEXT="Auteur: Jaderberg, Max"><node TEXT="Open-ended {Learning} in {Symmetric} {Zero}-sum {Games}" /></node><node TEXT="Auteur: Kaelbling, Leslie Pack"><node TEXT="All learning is local: {Multi}-agent learning in global reward games" /></node><node TEXT="Auteur: Krause, Andreas"><node TEXT="Multi-{Player} {Bandits}: {The} {Adversarial} {Case}" /></node><node TEXT="Auteur: Kveton, Branislav"><node TEXT="Old {Dog} {Learns} {New} {Tricks}: {Randomized} {UCB} for {Bandit} {Problems}" /></node><node TEXT="Auteur: Lattimore, Tor"><node TEXT="Partial monitoring" /></node><node TEXT="Auteur: Lefebvre, Randy"><node TEXT="On {Shallow} {Planning} {Under} {Partial} {Observability}" /></node><node TEXT="Auteur: Levy, Kﬁr Y"><node TEXT="Multi-{Player} {Bandits}: {The} {Adversarial} {Case}" /></node><node TEXT="Auteur: Littman, Michael"><node TEXT="Classes of {Multiagent} {Q}-learning {Dynamics}  with -greedy {Exploration}" /></node><node TEXT="Auteur: Mandel, Travis"><node TEXT="1 {Recap}: {Diﬀerence} between {Experts} and {Bandits}" /></node><node TEXT="Auteur: Mao, Weichao"><node TEXT="{MULTI}-{AGENT} {REINFORCEMENT} {LEARNING} {FOR} {NONZERO}-{SUM} {MARKOV} {GAMES}" /></node><node TEXT="Auteur: Mehrabian, Abbas"><node TEXT="Old {Dog} {Learns} {New} {Tricks}: {Randomized} {UCB} for {Bandit} {Problems}" /></node><node TEXT="Auteur: Mell, Johnathan T"><node TEXT="{DOCTOR} {OF} {PHILOSOPHY} {COMPUTER} {SCIENCE}" /></node><node TEXT="Auteur: Mendler-Dünner, Celestine"><node TEXT="Performative {Prediction}" /></node><node TEXT="Auteur: Miller, John"><node TEXT="Strategic {Classification} is {Causal} {Modeling} in {Disguise}" /></node><node TEXT="Auteur: Milli, Smitha"><node TEXT="Strategic {Classification} is {Causal} {Modeling} in {Disguise}" /></node><node TEXT="Auteur: Pacchiano, Aldo"><node TEXT="Neural {Pseudo}-{Label} {Optimism} for the {Bank} {Loan} {Problem}" /></node><node TEXT="Auteur: Perchet, Vianney"><node TEXT="A {Survey} on {Multi}-player {Bandits}" /></node><node TEXT="Auteur: Perdomo, Juan C"><node TEXT="Performative {Prediction}" /></node><node TEXT="Auteur: Perolat, Julien"><node TEXT="Open-ended {Learning} in {Symmetric} {Zero}-sum {Games}" /></node><node TEXT="Auteur: Seuken, Sven"><node TEXT="Memory-{Bounded} {Dynamic} {Programming} for {DEC}-{POMDPs}" /></node><node TEXT="Auteur: Shen, Jie"><node TEXT="Metric-{Fair} {Active} {Learning}" /></node><node TEXT="Auteur: Singh, Shaun"><node TEXT="Neural {Pseudo}-{Label} {Optimism} for the {Bank} {Loan} {Problem}" /></node><node TEXT="Auteur: Szepesvari, Csaba"><node TEXT="Partial monitoring" /></node><node TEXT="Auteur: Tang, Wei"><node TEXT="Bandit {Learning} with {Biased} {Human} {Feedback}" /></node><node TEXT="Auteur: Vaswani, Sharan"><node TEXT="Old {Dog} {Learns} {New} {Tricks}: {Randomized} {UCB} for {Bandit} {Problems}" /></node><node TEXT="Auteur: Veloso, Manuela"><node TEXT="Rational and {Convergent} {Learning} in {Stochastic} {Games}" /></node><node TEXT="Auteur: Wang, Jing"><node TEXT="Metric-{Fair} {Active} {Learning}" /></node><node TEXT="Auteur: Wunder, Michael"><node TEXT="Classes of {Multiagent} {Q}-learning {Dynamics}  with -greedy {Exploration}" /></node><node TEXT="Auteur: Zeevi, Assaf"><node TEXT="Stochastic {Multi}-{Armed}-{Bandit} {Problem} with {Non}-stationary {Rewards}" /></node><node TEXT="Auteur: Zrnic, Tijana"><node TEXT="Prediction and {Statistical} {Inference} in {Feedback} {Loops}" /><node TEXT="Performative {Prediction}" /></node></node></node></map>